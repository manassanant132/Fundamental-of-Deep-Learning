{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+vrJdGA3SZIPgNHwMtyeX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manassanant132/02_Maching_Learing_Pipeline/blob/main/Unit7_%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%A7%E0%B8%B4%E0%B9%80%E0%B8%84%E0%B8%A3%E0%B8%B2%E0%B8%B0%E0%B8%AB%E0%B9%8C%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%AA%E0%B8%B4%E0%B8%97%E0%B8%98%E0%B8%B4%E0%B8%A0%E0%B8%B2%E0%B8%9E_Machine_Learning_Model_%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2_Learning_Curve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Curve เป็นสิ่งที่แสดงถึงประสิทธิภาพการเรียนรู้ของ Model จาก Training Dataset ซึ่งแกน x ของกราฟจะเป็น Epoch และแกน y จะเป็นประสิทธิภาพของ Model โดยประสิทธิภาพของ Model จะถูกวัดหลังจากการปรับปรุง Weight และ Bias ด้วยข้อมูล 2 ชนิด ได้แก่ \"Training Dataset ที่ Model กำลังเรียนรู้\" และ \"Validation Dataset ที่ไม่เคยถูกใช้สอน Model มาก่อน\"\n",
        "\n",
        "ประสิทธิภาพของ Model จะวัดจาก Loss และ Accuracy โดยยิ่งค่า Loss หรือ Error ของ Model น้อย แสดงว่า Model มีการเรียนรู้ที่ดี แต่สำหรับค่า Accuracy จะเป็นในทางตรงกันข้าม คือยิ่งค่า Accuracy มากแสดงว่า Model มีการเรียนรู้ที่ดี\n",
        "\n",
        "การวินิจฉัยและแก้ไขปัญหาการเรียนรู้ของ Model อย่างเช่น ปัญหา Underfitting และ Overfitting ได้ เราจะต้องมีความเข้าใจรูปแบบ Learning Curve ที่เกิดขึ้น นอกจากนี้เรายังสามารถพิจารณาจาก Learning Curve ได้ว่า Training Dataset และ Validation Dataset เป็นตัวแทนของ Data ที่เหมาะสมในกระบวนการพัฒนา Model หรือไม่\n",
        "\n",
        "ในบทความนี้ผู้อ่านจะได้ทำความเข้าใจรูปแบบของ Learning Curve ที่สำคัญได้แก่ Underfit Learning Curve, Overfit Learning Curve, Good Fit Learning Curve รวมทั้งรูปแบบของ Learning Curve ที่แสดงว่า Training Dataset และ Validation Dataset เป็นตัวแทนของ Data ที่ไม่ดีครับ"
      ],
      "metadata": {
        "id": "dbiWntxPfheV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Underfit Learning Curve\n",
        "\n",
        "Learning Curve แบบ Underfitting จะบ่งบอกว่า Model ไม่สามารถเรียนรู้ได้จาก Training Dataset\n",
        "\n",
        "โดยเราจะจำลองสถานการณ์ของ Model ที่มีปัญหาการเรียนรู้แบบ Underfit ด้วยการพัฒนา Model เพื่อทำ Sentiment Analysis จาก IMDB Dataset ซึ่งเป็นคำวิจารณ์ภาพยนตร์ต่างประเทศ ตามขั้นตอนดังต่อไปนี้"
      ],
      "metadata": {
        "id": "YpjMvbMkfj56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ก่อนอื่นเราจะ Import Library ที่จำเป็นต้องใช้ในการทดลองดังต่อไปนี้"
      ],
      "metadata": {
        "id": "RXJDUJnGfoAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fIiPx6Zle2Fn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "imdb = tf.keras.datasets.imdb\n",
        "to_categorical = tf.keras.utils.to_categorical\n",
        "sequence = tf.keras.preprocessing.sequence\n",
        "\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import numpy\n",
        "\n",
        "from sklearn.datasets import make_circles, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load IMDB Dataset"
      ],
      "metadata": {
        "id": "nUW7HdTHfwRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = 5000\n",
        "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=top_words)\n",
        "max_words = 500\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEzRwmRmfwkK",
        "outputId": "33bbd65a-8c15-4116-a2d8-4607e7a37011"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000,), (25000,), (25000,), (25000,))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.get_word_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMNfHwBUf3Kq",
        "outputId": "94fddd26-c0b6-44db-ca5a-166c34f6e042"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fawn': 34701,\n",
              " 'tsukino': 52006,\n",
              " 'nunnery': 52007,\n",
              " 'sonja': 16816,\n",
              " 'vani': 63951,\n",
              " 'woods': 1408,\n",
              " 'spiders': 16115,\n",
              " 'hanging': 2345,\n",
              " 'woody': 2289,\n",
              " 'trawling': 52008,\n",
              " \"hold's\": 52009,\n",
              " 'comically': 11307,\n",
              " 'localized': 40830,\n",
              " 'disobeying': 30568,\n",
              " \"'royale\": 52010,\n",
              " \"harpo's\": 40831,\n",
              " 'canet': 52011,\n",
              " 'aileen': 19313,\n",
              " 'acurately': 52012,\n",
              " \"diplomat's\": 52013,\n",
              " 'rickman': 25242,\n",
              " 'arranged': 6746,\n",
              " 'rumbustious': 52014,\n",
              " 'familiarness': 52015,\n",
              " \"spider'\": 52016,\n",
              " 'hahahah': 68804,\n",
              " \"wood'\": 52017,\n",
              " 'transvestism': 40833,\n",
              " \"hangin'\": 34702,\n",
              " 'bringing': 2338,\n",
              " 'seamier': 40834,\n",
              " 'wooded': 34703,\n",
              " 'bravora': 52018,\n",
              " 'grueling': 16817,\n",
              " 'wooden': 1636,\n",
              " 'wednesday': 16818,\n",
              " \"'prix\": 52019,\n",
              " 'altagracia': 34704,\n",
              " 'circuitry': 52020,\n",
              " 'crotch': 11585,\n",
              " 'busybody': 57766,\n",
              " \"tart'n'tangy\": 52021,\n",
              " 'burgade': 14129,\n",
              " 'thrace': 52023,\n",
              " \"tom's\": 11038,\n",
              " 'snuggles': 52025,\n",
              " 'francesco': 29114,\n",
              " 'complainers': 52027,\n",
              " 'templarios': 52125,\n",
              " '272': 40835,\n",
              " '273': 52028,\n",
              " 'zaniacs': 52130,\n",
              " '275': 34706,\n",
              " 'consenting': 27631,\n",
              " 'snuggled': 40836,\n",
              " 'inanimate': 15492,\n",
              " 'uality': 52030,\n",
              " 'bronte': 11926,\n",
              " 'errors': 4010,\n",
              " 'dialogs': 3230,\n",
              " \"yomada's\": 52031,\n",
              " \"madman's\": 34707,\n",
              " 'dialoge': 30585,\n",
              " 'usenet': 52033,\n",
              " 'videodrome': 40837,\n",
              " \"kid'\": 26338,\n",
              " 'pawed': 52034,\n",
              " \"'girlfriend'\": 30569,\n",
              " \"'pleasure\": 52035,\n",
              " \"'reloaded'\": 52036,\n",
              " \"kazakos'\": 40839,\n",
              " 'rocque': 52037,\n",
              " 'mailings': 52038,\n",
              " 'brainwashed': 11927,\n",
              " 'mcanally': 16819,\n",
              " \"tom''\": 52039,\n",
              " 'kurupt': 25243,\n",
              " 'affiliated': 21905,\n",
              " 'babaganoosh': 52040,\n",
              " \"noe's\": 40840,\n",
              " 'quart': 40841,\n",
              " 'kids': 359,\n",
              " 'uplifting': 5034,\n",
              " 'controversy': 7093,\n",
              " 'kida': 21906,\n",
              " 'kidd': 23379,\n",
              " \"error'\": 52041,\n",
              " 'neurologist': 52042,\n",
              " 'spotty': 18510,\n",
              " 'cobblers': 30570,\n",
              " 'projection': 9878,\n",
              " 'fastforwarding': 40842,\n",
              " 'sters': 52043,\n",
              " \"eggar's\": 52044,\n",
              " 'etherything': 52045,\n",
              " 'gateshead': 40843,\n",
              " 'airball': 34708,\n",
              " 'unsinkable': 25244,\n",
              " 'stern': 7180,\n",
              " \"cervi's\": 52046,\n",
              " 'dnd': 40844,\n",
              " 'dna': 11586,\n",
              " 'insecurity': 20598,\n",
              " \"'reboot'\": 52047,\n",
              " 'trelkovsky': 11037,\n",
              " 'jaekel': 52048,\n",
              " 'sidebars': 52049,\n",
              " \"sforza's\": 52050,\n",
              " 'distortions': 17633,\n",
              " 'mutinies': 52051,\n",
              " 'sermons': 30602,\n",
              " '7ft': 40846,\n",
              " 'boobage': 52052,\n",
              " \"o'bannon's\": 52053,\n",
              " 'populations': 23380,\n",
              " 'chulak': 52054,\n",
              " 'mesmerize': 27633,\n",
              " 'quinnell': 52055,\n",
              " 'yahoo': 10307,\n",
              " 'meteorologist': 52057,\n",
              " 'beswick': 42577,\n",
              " 'boorman': 15493,\n",
              " 'voicework': 40847,\n",
              " \"ster'\": 52058,\n",
              " 'blustering': 22922,\n",
              " 'hj': 52059,\n",
              " 'intake': 27634,\n",
              " 'morally': 5621,\n",
              " 'jumbling': 40849,\n",
              " 'bowersock': 52060,\n",
              " \"'porky's'\": 52061,\n",
              " 'gershon': 16821,\n",
              " 'ludicrosity': 40850,\n",
              " 'coprophilia': 52062,\n",
              " 'expressively': 40851,\n",
              " \"india's\": 19500,\n",
              " \"post's\": 34710,\n",
              " 'wana': 52063,\n",
              " 'wang': 5283,\n",
              " 'wand': 30571,\n",
              " 'wane': 25245,\n",
              " 'edgeways': 52321,\n",
              " 'titanium': 34711,\n",
              " 'pinta': 40852,\n",
              " 'want': 178,\n",
              " 'pinto': 30572,\n",
              " 'whoopdedoodles': 52065,\n",
              " 'tchaikovsky': 21908,\n",
              " 'travel': 2103,\n",
              " \"'victory'\": 52066,\n",
              " 'copious': 11928,\n",
              " 'gouge': 22433,\n",
              " \"chapters'\": 52067,\n",
              " 'barbra': 6702,\n",
              " 'uselessness': 30573,\n",
              " \"wan'\": 52068,\n",
              " 'assimilated': 27635,\n",
              " 'petiot': 16116,\n",
              " 'most\\x85and': 52069,\n",
              " 'dinosaurs': 3930,\n",
              " 'wrong': 352,\n",
              " 'seda': 52070,\n",
              " 'stollen': 52071,\n",
              " 'sentencing': 34712,\n",
              " 'ouroboros': 40853,\n",
              " 'assimilates': 40854,\n",
              " 'colorfully': 40855,\n",
              " 'glenne': 27636,\n",
              " 'dongen': 52072,\n",
              " 'subplots': 4760,\n",
              " 'kiloton': 52073,\n",
              " 'chandon': 23381,\n",
              " \"effect'\": 34713,\n",
              " 'snugly': 27637,\n",
              " 'kuei': 40856,\n",
              " 'welcomed': 9092,\n",
              " 'dishonor': 30071,\n",
              " 'concurrence': 52075,\n",
              " 'stoicism': 23382,\n",
              " \"guys'\": 14896,\n",
              " \"beroemd'\": 52077,\n",
              " 'butcher': 6703,\n",
              " \"melfi's\": 40857,\n",
              " 'aargh': 30623,\n",
              " 'playhouse': 20599,\n",
              " 'wickedly': 11308,\n",
              " 'fit': 1180,\n",
              " 'labratory': 52078,\n",
              " 'lifeline': 40859,\n",
              " 'screaming': 1927,\n",
              " 'fix': 4287,\n",
              " 'cineliterate': 52079,\n",
              " 'fic': 52080,\n",
              " 'fia': 52081,\n",
              " 'fig': 34714,\n",
              " 'fmvs': 52082,\n",
              " 'fie': 52083,\n",
              " 'reentered': 52084,\n",
              " 'fin': 30574,\n",
              " 'doctresses': 52085,\n",
              " 'fil': 52086,\n",
              " 'zucker': 12606,\n",
              " 'ached': 31931,\n",
              " 'counsil': 52088,\n",
              " 'paterfamilias': 52089,\n",
              " 'songwriter': 13885,\n",
              " 'shivam': 34715,\n",
              " 'hurting': 9654,\n",
              " 'effects': 299,\n",
              " 'slauther': 52090,\n",
              " \"'flame'\": 52091,\n",
              " 'sommerset': 52092,\n",
              " 'interwhined': 52093,\n",
              " 'whacking': 27638,\n",
              " 'bartok': 52094,\n",
              " 'barton': 8775,\n",
              " 'frewer': 21909,\n",
              " \"fi'\": 52095,\n",
              " 'ingrid': 6192,\n",
              " 'stribor': 30575,\n",
              " 'approporiately': 52096,\n",
              " 'wobblyhand': 52097,\n",
              " 'tantalisingly': 52098,\n",
              " 'ankylosaurus': 52099,\n",
              " 'parasites': 17634,\n",
              " 'childen': 52100,\n",
              " \"jenkins'\": 52101,\n",
              " 'metafiction': 52102,\n",
              " 'golem': 17635,\n",
              " 'indiscretion': 40860,\n",
              " \"reeves'\": 23383,\n",
              " \"inamorata's\": 57781,\n",
              " 'brittannica': 52104,\n",
              " 'adapt': 7916,\n",
              " \"russo's\": 30576,\n",
              " 'guitarists': 48246,\n",
              " 'abbott': 10553,\n",
              " 'abbots': 40861,\n",
              " 'lanisha': 17649,\n",
              " 'magickal': 40863,\n",
              " 'mattter': 52105,\n",
              " \"'willy\": 52106,\n",
              " 'pumpkins': 34716,\n",
              " 'stuntpeople': 52107,\n",
              " 'estimate': 30577,\n",
              " 'ugghhh': 40864,\n",
              " 'gameplay': 11309,\n",
              " \"wern't\": 52108,\n",
              " \"n'sync\": 40865,\n",
              " 'sickeningly': 16117,\n",
              " 'chiara': 40866,\n",
              " 'disturbed': 4011,\n",
              " 'portmanteau': 40867,\n",
              " 'ineffectively': 52109,\n",
              " \"duchonvey's\": 82143,\n",
              " \"nasty'\": 37519,\n",
              " 'purpose': 1285,\n",
              " 'lazers': 52112,\n",
              " 'lightened': 28105,\n",
              " 'kaliganj': 52113,\n",
              " 'popularism': 52114,\n",
              " \"damme's\": 18511,\n",
              " 'stylistics': 30578,\n",
              " 'mindgaming': 52115,\n",
              " 'spoilerish': 46449,\n",
              " \"'corny'\": 52117,\n",
              " 'boerner': 34718,\n",
              " 'olds': 6792,\n",
              " 'bakelite': 52118,\n",
              " 'renovated': 27639,\n",
              " 'forrester': 27640,\n",
              " \"lumiere's\": 52119,\n",
              " 'gaskets': 52024,\n",
              " 'needed': 884,\n",
              " 'smight': 34719,\n",
              " 'master': 1297,\n",
              " \"edie's\": 25905,\n",
              " 'seeber': 40868,\n",
              " 'hiya': 52120,\n",
              " 'fuzziness': 52121,\n",
              " 'genesis': 14897,\n",
              " 'rewards': 12607,\n",
              " 'enthrall': 30579,\n",
              " \"'about\": 40869,\n",
              " \"recollection's\": 52122,\n",
              " 'mutilated': 11039,\n",
              " 'fatherlands': 52123,\n",
              " \"fischer's\": 52124,\n",
              " 'positively': 5399,\n",
              " '270': 34705,\n",
              " 'ahmed': 34720,\n",
              " 'zatoichi': 9836,\n",
              " 'bannister': 13886,\n",
              " 'anniversaries': 52127,\n",
              " \"helm's\": 30580,\n",
              " \"'work'\": 52128,\n",
              " 'exclaimed': 34721,\n",
              " \"'unfunny'\": 52129,\n",
              " '274': 52029,\n",
              " 'feeling': 544,\n",
              " \"wanda's\": 52131,\n",
              " 'dolan': 33266,\n",
              " '278': 52133,\n",
              " 'peacoat': 52134,\n",
              " 'brawny': 40870,\n",
              " 'mishra': 40871,\n",
              " 'worlders': 40872,\n",
              " 'protags': 52135,\n",
              " 'skullcap': 52136,\n",
              " 'dastagir': 57596,\n",
              " 'affairs': 5622,\n",
              " 'wholesome': 7799,\n",
              " 'hymen': 52137,\n",
              " 'paramedics': 25246,\n",
              " 'unpersons': 52138,\n",
              " 'heavyarms': 52139,\n",
              " 'affaire': 52140,\n",
              " 'coulisses': 52141,\n",
              " 'hymer': 40873,\n",
              " 'kremlin': 52142,\n",
              " 'shipments': 30581,\n",
              " 'pixilated': 52143,\n",
              " \"'00s\": 30582,\n",
              " 'diminishing': 18512,\n",
              " 'cinematic': 1357,\n",
              " 'resonates': 14898,\n",
              " 'simplify': 40874,\n",
              " \"nature'\": 40875,\n",
              " 'temptresses': 40876,\n",
              " 'reverence': 16822,\n",
              " 'resonated': 19502,\n",
              " 'dailey': 34722,\n",
              " '2\\x85': 52144,\n",
              " 'treize': 27641,\n",
              " 'majo': 52145,\n",
              " 'kiya': 21910,\n",
              " 'woolnough': 52146,\n",
              " 'thanatos': 39797,\n",
              " 'sandoval': 35731,\n",
              " 'dorama': 40879,\n",
              " \"o'shaughnessy\": 52147,\n",
              " 'tech': 4988,\n",
              " 'fugitives': 32018,\n",
              " 'teck': 30583,\n",
              " \"'e'\": 76125,\n",
              " 'doesn’t': 40881,\n",
              " 'purged': 52149,\n",
              " 'saying': 657,\n",
              " \"martians'\": 41095,\n",
              " 'norliss': 23418,\n",
              " 'dickey': 27642,\n",
              " 'dicker': 52152,\n",
              " \"'sependipity\": 52153,\n",
              " 'padded': 8422,\n",
              " 'ordell': 57792,\n",
              " \"sturges'\": 40882,\n",
              " 'independentcritics': 52154,\n",
              " 'tempted': 5745,\n",
              " \"atkinson's\": 34724,\n",
              " 'hounded': 25247,\n",
              " 'apace': 52155,\n",
              " 'clicked': 15494,\n",
              " \"'humor'\": 30584,\n",
              " \"martino's\": 17177,\n",
              " \"'supporting\": 52156,\n",
              " 'warmongering': 52032,\n",
              " \"zemeckis's\": 34725,\n",
              " 'lube': 21911,\n",
              " 'shocky': 52157,\n",
              " 'plate': 7476,\n",
              " 'plata': 40883,\n",
              " 'sturgess': 40884,\n",
              " \"nerds'\": 40885,\n",
              " 'plato': 20600,\n",
              " 'plath': 34726,\n",
              " 'platt': 40886,\n",
              " 'mcnab': 52159,\n",
              " 'clumsiness': 27643,\n",
              " 'altogether': 3899,\n",
              " 'massacring': 42584,\n",
              " 'bicenntinial': 52160,\n",
              " 'skaal': 40887,\n",
              " 'droning': 14360,\n",
              " 'lds': 8776,\n",
              " 'jaguar': 21912,\n",
              " \"cale's\": 34727,\n",
              " 'nicely': 1777,\n",
              " 'mummy': 4588,\n",
              " \"lot's\": 18513,\n",
              " 'patch': 10086,\n",
              " 'kerkhof': 50202,\n",
              " \"leader's\": 52161,\n",
              " \"'movie\": 27644,\n",
              " 'uncomfirmed': 52162,\n",
              " 'heirloom': 40888,\n",
              " 'wrangle': 47360,\n",
              " 'emotion\\x85': 52163,\n",
              " \"'stargate'\": 52164,\n",
              " 'pinoy': 40889,\n",
              " 'conchatta': 40890,\n",
              " 'broeke': 41128,\n",
              " 'advisedly': 40891,\n",
              " \"barker's\": 17636,\n",
              " 'descours': 52166,\n",
              " 'lots': 772,\n",
              " 'lotr': 9259,\n",
              " 'irs': 9879,\n",
              " 'lott': 52167,\n",
              " 'xvi': 40892,\n",
              " 'irk': 34728,\n",
              " 'irl': 52168,\n",
              " 'ira': 6887,\n",
              " 'belzer': 21913,\n",
              " 'irc': 52169,\n",
              " 'ire': 27645,\n",
              " 'requisites': 40893,\n",
              " 'discipline': 7693,\n",
              " 'lyoko': 52961,\n",
              " 'extend': 11310,\n",
              " 'nature': 873,\n",
              " \"'dickie'\": 52170,\n",
              " 'optimist': 40894,\n",
              " 'lapping': 30586,\n",
              " 'superficial': 3900,\n",
              " 'vestment': 52171,\n",
              " 'extent': 2823,\n",
              " 'tendons': 52172,\n",
              " \"heller's\": 52173,\n",
              " 'quagmires': 52174,\n",
              " 'miyako': 52175,\n",
              " 'moocow': 20601,\n",
              " \"coles'\": 52176,\n",
              " 'lookit': 40895,\n",
              " 'ravenously': 52177,\n",
              " 'levitating': 40896,\n",
              " 'perfunctorily': 52178,\n",
              " 'lookin': 30587,\n",
              " \"lot'\": 40898,\n",
              " 'lookie': 52179,\n",
              " 'fearlessly': 34870,\n",
              " 'libyan': 52181,\n",
              " 'fondles': 40899,\n",
              " 'gopher': 35714,\n",
              " 'wearying': 40901,\n",
              " \"nz's\": 52182,\n",
              " 'minuses': 27646,\n",
              " 'puposelessly': 52183,\n",
              " 'shandling': 52184,\n",
              " 'decapitates': 31268,\n",
              " 'humming': 11929,\n",
              " \"'nother\": 40902,\n",
              " 'smackdown': 21914,\n",
              " 'underdone': 30588,\n",
              " 'frf': 40903,\n",
              " 'triviality': 52185,\n",
              " 'fro': 25248,\n",
              " 'bothers': 8777,\n",
              " \"'kensington\": 52186,\n",
              " 'much': 73,\n",
              " 'muco': 34730,\n",
              " 'wiseguy': 22615,\n",
              " \"richie's\": 27648,\n",
              " 'tonino': 40904,\n",
              " 'unleavened': 52187,\n",
              " 'fry': 11587,\n",
              " \"'tv'\": 40905,\n",
              " 'toning': 40906,\n",
              " 'obese': 14361,\n",
              " 'sensationalized': 30589,\n",
              " 'spiv': 40907,\n",
              " 'spit': 6259,\n",
              " 'arkin': 7364,\n",
              " 'charleton': 21915,\n",
              " 'jeon': 16823,\n",
              " 'boardroom': 21916,\n",
              " 'doubts': 4989,\n",
              " 'spin': 3084,\n",
              " 'hepo': 53083,\n",
              " 'wildcat': 27649,\n",
              " 'venoms': 10584,\n",
              " 'misconstrues': 52191,\n",
              " 'mesmerising': 18514,\n",
              " 'misconstrued': 40908,\n",
              " 'rescinds': 52192,\n",
              " 'prostrate': 52193,\n",
              " 'majid': 40909,\n",
              " 'climbed': 16479,\n",
              " 'canoeing': 34731,\n",
              " 'majin': 52195,\n",
              " 'animie': 57804,\n",
              " 'sylke': 40910,\n",
              " 'conditioned': 14899,\n",
              " 'waddell': 40911,\n",
              " '3\\x85': 52196,\n",
              " 'hyperdrive': 41188,\n",
              " 'conditioner': 34732,\n",
              " 'bricklayer': 53153,\n",
              " 'hong': 2576,\n",
              " 'memoriam': 52198,\n",
              " 'inventively': 30592,\n",
              " \"levant's\": 25249,\n",
              " 'portobello': 20638,\n",
              " 'remand': 52200,\n",
              " 'mummified': 19504,\n",
              " 'honk': 27650,\n",
              " 'spews': 19505,\n",
              " 'visitations': 40912,\n",
              " 'mummifies': 52201,\n",
              " 'cavanaugh': 25250,\n",
              " 'zeon': 23385,\n",
              " \"jungle's\": 40913,\n",
              " 'viertel': 34733,\n",
              " 'frenchmen': 27651,\n",
              " 'torpedoes': 52202,\n",
              " 'schlessinger': 52203,\n",
              " 'torpedoed': 34734,\n",
              " 'blister': 69876,\n",
              " 'cinefest': 52204,\n",
              " 'furlough': 34735,\n",
              " 'mainsequence': 52205,\n",
              " 'mentors': 40914,\n",
              " 'academic': 9094,\n",
              " 'stillness': 20602,\n",
              " 'academia': 40915,\n",
              " 'lonelier': 52206,\n",
              " 'nibby': 52207,\n",
              " \"losers'\": 52208,\n",
              " 'cineastes': 40916,\n",
              " 'corporate': 4449,\n",
              " 'massaging': 40917,\n",
              " 'bellow': 30593,\n",
              " 'absurdities': 19506,\n",
              " 'expetations': 53241,\n",
              " 'nyfiken': 40918,\n",
              " 'mehras': 75638,\n",
              " 'lasse': 52209,\n",
              " 'visability': 52210,\n",
              " 'militarily': 33946,\n",
              " \"elder'\": 52211,\n",
              " 'gainsbourg': 19023,\n",
              " 'hah': 20603,\n",
              " 'hai': 13420,\n",
              " 'haj': 34736,\n",
              " 'hak': 25251,\n",
              " 'hal': 4311,\n",
              " 'ham': 4892,\n",
              " 'duffer': 53259,\n",
              " 'haa': 52213,\n",
              " 'had': 66,\n",
              " 'advancement': 11930,\n",
              " 'hag': 16825,\n",
              " \"hand'\": 25252,\n",
              " 'hay': 13421,\n",
              " 'mcnamara': 20604,\n",
              " \"mozart's\": 52214,\n",
              " 'duffel': 30731,\n",
              " 'haq': 30594,\n",
              " 'har': 13887,\n",
              " 'has': 44,\n",
              " 'hat': 2401,\n",
              " 'hav': 40919,\n",
              " 'haw': 30595,\n",
              " 'figtings': 52215,\n",
              " 'elders': 15495,\n",
              " 'underpanted': 52216,\n",
              " 'pninson': 52217,\n",
              " 'unequivocally': 27652,\n",
              " \"barbara's\": 23673,\n",
              " \"bello'\": 52219,\n",
              " 'indicative': 12997,\n",
              " 'yawnfest': 40920,\n",
              " 'hexploitation': 52220,\n",
              " \"loder's\": 52221,\n",
              " 'sleuthing': 27653,\n",
              " \"justin's\": 32622,\n",
              " \"'ball\": 52222,\n",
              " \"'summer\": 52223,\n",
              " \"'demons'\": 34935,\n",
              " \"mormon's\": 52225,\n",
              " \"laughton's\": 34737,\n",
              " 'debell': 52226,\n",
              " 'shipyard': 39724,\n",
              " 'unabashedly': 30597,\n",
              " 'disks': 40401,\n",
              " 'crowd': 2290,\n",
              " 'crowe': 10087,\n",
              " \"vancouver's\": 56434,\n",
              " 'mosques': 34738,\n",
              " 'crown': 6627,\n",
              " 'culpas': 52227,\n",
              " 'crows': 27654,\n",
              " 'surrell': 53344,\n",
              " 'flowless': 52229,\n",
              " 'sheirk': 52230,\n",
              " \"'three\": 40923,\n",
              " \"peterson'\": 52231,\n",
              " 'ooverall': 52232,\n",
              " 'perchance': 40924,\n",
              " 'bottom': 1321,\n",
              " 'chabert': 53363,\n",
              " 'sneha': 52233,\n",
              " 'inhuman': 13888,\n",
              " 'ichii': 52234,\n",
              " 'ursla': 52235,\n",
              " 'completly': 30598,\n",
              " 'moviedom': 40925,\n",
              " 'raddick': 52236,\n",
              " 'brundage': 51995,\n",
              " 'brigades': 40926,\n",
              " 'starring': 1181,\n",
              " \"'goal'\": 52237,\n",
              " 'caskets': 52238,\n",
              " 'willcock': 52239,\n",
              " \"threesome's\": 52240,\n",
              " \"mosque'\": 52241,\n",
              " \"cover's\": 52242,\n",
              " 'spaceships': 17637,\n",
              " 'anomalous': 40927,\n",
              " 'ptsd': 27655,\n",
              " 'shirdan': 52243,\n",
              " 'obscenity': 21962,\n",
              " 'lemmings': 30599,\n",
              " 'duccio': 30600,\n",
              " \"levene's\": 52244,\n",
              " \"'gorby'\": 52245,\n",
              " \"teenager's\": 25255,\n",
              " 'marshall': 5340,\n",
              " 'honeymoon': 9095,\n",
              " 'shoots': 3231,\n",
              " 'despised': 12258,\n",
              " 'okabasho': 52246,\n",
              " 'fabric': 8289,\n",
              " 'cannavale': 18515,\n",
              " 'raped': 3537,\n",
              " \"tutt's\": 52247,\n",
              " 'grasping': 17638,\n",
              " 'despises': 18516,\n",
              " \"thief's\": 40928,\n",
              " 'rapes': 8926,\n",
              " 'raper': 52248,\n",
              " \"eyre'\": 27656,\n",
              " 'walchek': 52249,\n",
              " \"elmo's\": 23386,\n",
              " 'perfumes': 40929,\n",
              " 'spurting': 21918,\n",
              " \"exposition'\\x85\": 52250,\n",
              " 'denoting': 52251,\n",
              " 'thesaurus': 34740,\n",
              " \"shoot'\": 40930,\n",
              " 'bonejack': 49759,\n",
              " 'simpsonian': 52253,\n",
              " 'hebetude': 30601,\n",
              " \"hallow's\": 34741,\n",
              " 'desperation\\x85': 52254,\n",
              " 'incinerator': 34742,\n",
              " 'congratulations': 10308,\n",
              " 'humbled': 52255,\n",
              " \"else's\": 5924,\n",
              " 'trelkovski': 40845,\n",
              " \"rape'\": 52256,\n",
              " \"'chapters'\": 59386,\n",
              " '1600s': 52257,\n",
              " 'martian': 7253,\n",
              " 'nicest': 25256,\n",
              " 'eyred': 52259,\n",
              " 'passenger': 9457,\n",
              " 'disgrace': 6041,\n",
              " 'moderne': 52260,\n",
              " 'barrymore': 5120,\n",
              " 'yankovich': 52261,\n",
              " 'moderns': 40931,\n",
              " 'studliest': 52262,\n",
              " 'bedsheet': 52263,\n",
              " 'decapitation': 14900,\n",
              " 'slurring': 52264,\n",
              " \"'nunsploitation'\": 52265,\n",
              " \"'character'\": 34743,\n",
              " 'cambodia': 9880,\n",
              " 'rebelious': 52266,\n",
              " 'pasadena': 27657,\n",
              " 'crowne': 40932,\n",
              " \"'bedchamber\": 52267,\n",
              " 'conjectural': 52268,\n",
              " 'appologize': 52269,\n",
              " 'halfassing': 52270,\n",
              " 'paycheque': 57816,\n",
              " 'palms': 20606,\n",
              " \"'islands\": 52271,\n",
              " 'hawked': 40933,\n",
              " 'palme': 21919,\n",
              " 'conservatively': 40934,\n",
              " 'larp': 64007,\n",
              " 'palma': 5558,\n",
              " 'smelling': 21920,\n",
              " 'aragorn': 12998,\n",
              " 'hawker': 52272,\n",
              " 'hawkes': 52273,\n",
              " 'explosions': 3975,\n",
              " 'loren': 8059,\n",
              " \"pyle's\": 52274,\n",
              " 'shootout': 6704,\n",
              " \"mike's\": 18517,\n",
              " \"driscoll's\": 52275,\n",
              " 'cogsworth': 40935,\n",
              " \"britian's\": 52276,\n",
              " 'childs': 34744,\n",
              " \"portrait's\": 52277,\n",
              " 'chain': 3626,\n",
              " 'whoever': 2497,\n",
              " 'puttered': 52278,\n",
              " 'childe': 52279,\n",
              " 'maywether': 52280,\n",
              " 'chair': 3036,\n",
              " \"rance's\": 52281,\n",
              " 'machu': 34745,\n",
              " 'ballet': 4517,\n",
              " 'grapples': 34746,\n",
              " 'summerize': 76152,\n",
              " 'freelance': 30603,\n",
              " \"andrea's\": 52283,\n",
              " '\\x91very': 52284,\n",
              " 'coolidge': 45879,\n",
              " 'mache': 18518,\n",
              " 'balled': 52285,\n",
              " 'grappled': 40937,\n",
              " 'macha': 18519,\n",
              " 'underlining': 21921,\n",
              " 'macho': 5623,\n",
              " 'oversight': 19507,\n",
              " 'machi': 25257,\n",
              " 'verbally': 11311,\n",
              " 'tenacious': 21922,\n",
              " 'windshields': 40938,\n",
              " 'paychecks': 18557,\n",
              " 'jerk': 3396,\n",
              " \"good'\": 11931,\n",
              " 'prancer': 34748,\n",
              " 'prances': 21923,\n",
              " 'olympus': 52286,\n",
              " 'lark': 21924,\n",
              " 'embark': 10785,\n",
              " 'gloomy': 7365,\n",
              " 'jehaan': 52287,\n",
              " 'turaqui': 52288,\n",
              " \"child'\": 20607,\n",
              " 'locked': 2894,\n",
              " 'pranced': 52289,\n",
              " 'exact': 2588,\n",
              " 'unattuned': 52290,\n",
              " 'minute': 783,\n",
              " 'skewed': 16118,\n",
              " 'hodgins': 40940,\n",
              " 'skewer': 34749,\n",
              " 'think\\x85': 52291,\n",
              " 'rosenstein': 38765,\n",
              " 'helmit': 52292,\n",
              " 'wrestlemanias': 34750,\n",
              " 'hindered': 16826,\n",
              " \"martha's\": 30604,\n",
              " 'cheree': 52293,\n",
              " \"pluckin'\": 52294,\n",
              " 'ogles': 40941,\n",
              " 'heavyweight': 11932,\n",
              " 'aada': 82190,\n",
              " 'chopping': 11312,\n",
              " 'strongboy': 61534,\n",
              " 'hegemonic': 41342,\n",
              " 'adorns': 40942,\n",
              " 'xxth': 41346,\n",
              " 'nobuhiro': 34751,\n",
              " 'capitães': 52298,\n",
              " 'kavogianni': 52299,\n",
              " 'antwerp': 13422,\n",
              " 'celebrated': 6538,\n",
              " 'roarke': 52300,\n",
              " 'baggins': 40943,\n",
              " 'cheeseburgers': 31270,\n",
              " 'matras': 52301,\n",
              " \"nineties'\": 52302,\n",
              " \"'craig'\": 52303,\n",
              " 'celebrates': 12999,\n",
              " 'unintentionally': 3383,\n",
              " 'drafted': 14362,\n",
              " 'climby': 52304,\n",
              " '303': 52305,\n",
              " 'oldies': 18520,\n",
              " 'climbs': 9096,\n",
              " 'honour': 9655,\n",
              " 'plucking': 34752,\n",
              " '305': 30074,\n",
              " 'address': 5514,\n",
              " 'menjou': 40944,\n",
              " \"'freak'\": 42592,\n",
              " 'dwindling': 19508,\n",
              " 'benson': 9458,\n",
              " 'white’s': 52307,\n",
              " 'shamelessness': 40945,\n",
              " 'impacted': 21925,\n",
              " 'upatz': 52308,\n",
              " 'cusack': 3840,\n",
              " \"flavia's\": 37567,\n",
              " 'effette': 52309,\n",
              " 'influx': 34753,\n",
              " 'boooooooo': 52310,\n",
              " 'dimitrova': 52311,\n",
              " 'houseman': 13423,\n",
              " 'bigas': 25259,\n",
              " 'boylen': 52312,\n",
              " 'phillipenes': 52313,\n",
              " 'fakery': 40946,\n",
              " \"grandpa's\": 27658,\n",
              " 'darnell': 27659,\n",
              " 'undergone': 19509,\n",
              " 'handbags': 52315,\n",
              " 'perished': 21926,\n",
              " 'pooped': 37778,\n",
              " 'vigour': 27660,\n",
              " 'opposed': 3627,\n",
              " 'etude': 52316,\n",
              " \"caine's\": 11799,\n",
              " 'doozers': 52317,\n",
              " 'photojournals': 34754,\n",
              " 'perishes': 52318,\n",
              " 'constrains': 34755,\n",
              " 'migenes': 40948,\n",
              " 'consoled': 30605,\n",
              " 'alastair': 16827,\n",
              " 'wvs': 52319,\n",
              " 'ooooooh': 52320,\n",
              " 'approving': 34756,\n",
              " 'consoles': 40949,\n",
              " 'disparagement': 52064,\n",
              " 'futureistic': 52322,\n",
              " 'rebounding': 52323,\n",
              " \"'date\": 52324,\n",
              " 'gregoire': 52325,\n",
              " 'rutherford': 21927,\n",
              " 'americanised': 34757,\n",
              " 'novikov': 82196,\n",
              " 'following': 1042,\n",
              " 'munroe': 34758,\n",
              " \"morita'\": 52326,\n",
              " 'christenssen': 52327,\n",
              " 'oatmeal': 23106,\n",
              " 'fossey': 25260,\n",
              " 'livered': 40950,\n",
              " 'listens': 13000,\n",
              " \"'marci\": 76164,\n",
              " \"otis's\": 52330,\n",
              " 'thanking': 23387,\n",
              " 'maude': 16019,\n",
              " 'extensions': 34759,\n",
              " 'ameteurish': 52332,\n",
              " \"commender's\": 52333,\n",
              " 'agricultural': 27661,\n",
              " 'convincingly': 4518,\n",
              " 'fueled': 17639,\n",
              " 'mahattan': 54014,\n",
              " \"paris's\": 40952,\n",
              " 'vulkan': 52336,\n",
              " 'stapes': 52337,\n",
              " 'odysessy': 52338,\n",
              " 'harmon': 12259,\n",
              " 'surfing': 4252,\n",
              " 'halloran': 23494,\n",
              " 'unbelieveably': 49580,\n",
              " \"'offed'\": 52339,\n",
              " 'quadrant': 30607,\n",
              " 'inhabiting': 19510,\n",
              " 'nebbish': 34760,\n",
              " 'forebears': 40953,\n",
              " 'skirmish': 34761,\n",
              " 'ocassionally': 52340,\n",
              " \"'resist\": 52341,\n",
              " 'impactful': 21928,\n",
              " 'spicier': 52342,\n",
              " 'touristy': 40954,\n",
              " \"'football'\": 52343,\n",
              " 'webpage': 40955,\n",
              " 'exurbia': 52345,\n",
              " 'jucier': 52346,\n",
              " 'professors': 14901,\n",
              " 'structuring': 34762,\n",
              " 'jig': 30608,\n",
              " 'overlord': 40956,\n",
              " 'disconnect': 25261,\n",
              " 'sniffle': 82201,\n",
              " 'slimeball': 40957,\n",
              " 'jia': 40958,\n",
              " 'milked': 16828,\n",
              " 'banjoes': 40959,\n",
              " 'jim': 1237,\n",
              " 'workforces': 52348,\n",
              " 'jip': 52349,\n",
              " 'rotweiller': 52350,\n",
              " 'mundaneness': 34763,\n",
              " \"'ninja'\": 52351,\n",
              " \"dead'\": 11040,\n",
              " \"cipriani's\": 40960,\n",
              " 'modestly': 20608,\n",
              " \"professor'\": 52352,\n",
              " 'shacked': 40961,\n",
              " 'bashful': 34764,\n",
              " 'sorter': 23388,\n",
              " 'overpowering': 16120,\n",
              " 'workmanlike': 18521,\n",
              " 'henpecked': 27662,\n",
              " 'sorted': 18522,\n",
              " \"jōb's\": 52354,\n",
              " \"'always\": 52355,\n",
              " \"'baptists\": 34765,\n",
              " 'dreamcatchers': 52356,\n",
              " \"'silence'\": 52357,\n",
              " 'hickory': 21929,\n",
              " 'fun\\x97yet': 52358,\n",
              " 'breakumentary': 52359,\n",
              " 'didn': 15496,\n",
              " 'didi': 52360,\n",
              " 'pealing': 52361,\n",
              " 'dispite': 40962,\n",
              " \"italy's\": 25262,\n",
              " 'instability': 21930,\n",
              " 'quarter': 6539,\n",
              " 'quartet': 12608,\n",
              " 'padmé': 52362,\n",
              " \"'bleedmedry\": 52363,\n",
              " 'pahalniuk': 52364,\n",
              " 'honduras': 52365,\n",
              " 'bursting': 10786,\n",
              " \"pablo's\": 41465,\n",
              " 'irremediably': 52367,\n",
              " 'presages': 40963,\n",
              " 'bowlegged': 57832,\n",
              " 'dalip': 65183,\n",
              " 'entering': 6260,\n",
              " 'newsradio': 76172,\n",
              " 'presaged': 54150,\n",
              " \"giallo's\": 27663,\n",
              " 'bouyant': 40964,\n",
              " 'amerterish': 52368,\n",
              " 'rajni': 18523,\n",
              " 'leeves': 30610,\n",
              " 'macauley': 34767,\n",
              " 'seriously': 612,\n",
              " 'sugercoma': 52369,\n",
              " 'grimstead': 52370,\n",
              " \"'fairy'\": 52371,\n",
              " 'zenda': 30611,\n",
              " \"'twins'\": 52372,\n",
              " 'realisation': 17640,\n",
              " 'highsmith': 27664,\n",
              " 'raunchy': 7817,\n",
              " 'incentives': 40965,\n",
              " 'flatson': 52374,\n",
              " 'snooker': 35097,\n",
              " 'crazies': 16829,\n",
              " 'crazier': 14902,\n",
              " 'grandma': 7094,\n",
              " 'napunsaktha': 52375,\n",
              " 'workmanship': 30612,\n",
              " 'reisner': 52376,\n",
              " \"sanford's\": 61306,\n",
              " '\\x91doña': 52377,\n",
              " 'modest': 6108,\n",
              " \"everything's\": 19153,\n",
              " 'hamer': 40966,\n",
              " \"couldn't'\": 52379,\n",
              " 'quibble': 13001,\n",
              " 'socking': 52380,\n",
              " 'tingler': 21931,\n",
              " 'gutman': 52381,\n",
              " 'lachlan': 40967,\n",
              " 'tableaus': 52382,\n",
              " 'headbanger': 52383,\n",
              " 'spoken': 2847,\n",
              " 'cerebrally': 34768,\n",
              " \"'road\": 23490,\n",
              " 'tableaux': 21932,\n",
              " \"proust's\": 40968,\n",
              " 'periodical': 40969,\n",
              " \"shoveller's\": 52385,\n",
              " 'tamara': 25263,\n",
              " 'affords': 17641,\n",
              " 'concert': 3249,\n",
              " \"yara's\": 87955,\n",
              " 'someome': 52386,\n",
              " 'lingering': 8424,\n",
              " \"abraham's\": 41511,\n",
              " 'beesley': 34769,\n",
              " 'cherbourg': 34770,\n",
              " 'kagan': 28624,\n",
              " 'snatch': 9097,\n",
              " \"miyazaki's\": 9260,\n",
              " 'absorbs': 25264,\n",
              " \"koltai's\": 40970,\n",
              " 'tingled': 64027,\n",
              " 'crossroads': 19511,\n",
              " 'rehab': 16121,\n",
              " 'falworth': 52389,\n",
              " 'sequals': 52390,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concat Train และ Validate Dataset เพื่อคำนวณคำที่ไม่ซ้ำทั้งหมด"
      ],
      "metadata": {
        "id": "rJqLNP6Ef81h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = numpy.concatenate((x_train, x_val), axis=0)\n",
        "\n",
        "print(\"Number of words:\", len(numpy.unique(numpy.hstack(x))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWP9ugv2f9Dr",
        "outputId": "eafd8caa-f881-4146-d5e4-198843662341"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 4998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ดูความยาวของคำในประโยค"
      ],
      "metadata": {
        "id": "sfXG_vWXgCUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Review length: \")\n",
        "result = [len(st) for st in x]\n",
        "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
        "\n",
        "pyplot.boxplot(result)\n",
        "\n",
        "pyplot.savefig('review_length.png', dpi = 300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "DOymcZp-gCtq",
        "outputId": "526c904f-34a5-48d1-f250-947a85a6a24c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review length: \n",
            "Mean 234.76 words (172.911495)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "เติม 0 (ศูนย์) เพื่อทำให้ความยาวของประโยคเท่ากัน (Padding)"
      ],
      "metadata": {
        "id": "EvN8tsFjgFQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
        "x_val = sequence.pad_sequences(x_val, maxlen=max_words)\n",
        "\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6SbFhhnjOCp",
        "outputId": "1db3196e-7c14-4921-f6af-56025c7ae99d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "นิยาม Model"
      ],
      "metadata": {
        "id": "2swTlfD9gGxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(4, input_dim=max_words, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOXtRyQBgHLI",
        "outputId": "9cfc345a-d434-478a-f1eb-d4366ff92fa6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 4)                 2004      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,009\n",
            "Trainable params: 2,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "SuaJ1nEJgK6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=200, batch_size=128, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5PkwUwegLIa",
        "outputId": "e4331ec0-f1cb-4fd2-a59c-913914653a9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "196/196 - 2s - loss: 31.3366 - accuracy: 0.4970 - val_loss: 1.3615 - val_accuracy: 0.5023 - 2s/epoch - 9ms/step\n",
            "Epoch 2/200\n",
            "196/196 - 1s - loss: 1.0672 - accuracy: 0.5011 - val_loss: 0.8800 - val_accuracy: 0.5003 - 716ms/epoch - 4ms/step\n",
            "Epoch 3/200\n",
            "196/196 - 1s - loss: 0.7923 - accuracy: 0.4964 - val_loss: 0.7994 - val_accuracy: 0.5000 - 727ms/epoch - 4ms/step\n",
            "Epoch 4/200\n",
            "196/196 - 1s - loss: 0.7341 - accuracy: 0.5010 - val_loss: 0.7670 - val_accuracy: 0.5000 - 699ms/epoch - 4ms/step\n",
            "Epoch 5/200\n",
            "196/196 - 1s - loss: 0.7102 - accuracy: 0.4976 - val_loss: 0.7539 - val_accuracy: 0.5000 - 713ms/epoch - 4ms/step\n",
            "Epoch 6/200\n",
            "196/196 - 1s - loss: 0.7025 - accuracy: 0.5010 - val_loss: 0.7491 - val_accuracy: 0.4998 - 695ms/epoch - 4ms/step\n",
            "Epoch 7/200\n",
            "196/196 - 1s - loss: 0.7001 - accuracy: 0.4988 - val_loss: 0.7461 - val_accuracy: 0.4997 - 708ms/epoch - 4ms/step\n",
            "Epoch 8/200\n",
            "196/196 - 1s - loss: 0.6983 - accuracy: 0.4970 - val_loss: 0.7423 - val_accuracy: 0.4997 - 695ms/epoch - 4ms/step\n",
            "Epoch 9/200\n",
            "196/196 - 1s - loss: 0.6964 - accuracy: 0.4988 - val_loss: 0.7409 - val_accuracy: 0.4997 - 705ms/epoch - 4ms/step\n",
            "Epoch 10/200\n",
            "196/196 - 1s - loss: 0.6953 - accuracy: 0.4938 - val_loss: 0.7402 - val_accuracy: 0.4997 - 715ms/epoch - 4ms/step\n",
            "Epoch 11/200\n",
            "196/196 - 1s - loss: 0.6948 - accuracy: 0.5004 - val_loss: 0.7402 - val_accuracy: 0.4997 - 706ms/epoch - 4ms/step\n",
            "Epoch 12/200\n",
            "196/196 - 1s - loss: 0.6942 - accuracy: 0.4979 - val_loss: 0.7387 - val_accuracy: 0.4997 - 696ms/epoch - 4ms/step\n",
            "Epoch 13/200\n",
            "196/196 - 1s - loss: 0.6935 - accuracy: 0.4979 - val_loss: 0.7374 - val_accuracy: 0.4998 - 671ms/epoch - 3ms/step\n",
            "Epoch 14/200\n",
            "196/196 - 1s - loss: 0.6928 - accuracy: 0.4985 - val_loss: 0.7373 - val_accuracy: 0.4998 - 682ms/epoch - 3ms/step\n",
            "Epoch 15/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4974 - val_loss: 0.7377 - val_accuracy: 0.4997 - 654ms/epoch - 3ms/step\n",
            "Epoch 16/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4947 - val_loss: 0.7378 - val_accuracy: 0.4998 - 708ms/epoch - 4ms/step\n",
            "Epoch 17/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4977 - val_loss: 0.7378 - val_accuracy: 0.4997 - 668ms/epoch - 3ms/step\n",
            "Epoch 18/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4980 - val_loss: 0.7379 - val_accuracy: 0.4997 - 631ms/epoch - 3ms/step\n",
            "Epoch 19/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.5007 - val_loss: 0.7379 - val_accuracy: 0.4997 - 723ms/epoch - 4ms/step\n",
            "Epoch 20/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4998 - val_loss: 0.7379 - val_accuracy: 0.4997 - 668ms/epoch - 3ms/step\n",
            "Epoch 21/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4994 - val_loss: 0.7380 - val_accuracy: 0.4997 - 680ms/epoch - 3ms/step\n",
            "Epoch 22/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.5005 - val_loss: 0.7380 - val_accuracy: 0.4998 - 671ms/epoch - 3ms/step\n",
            "Epoch 23/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4968 - val_loss: 0.7380 - val_accuracy: 0.4998 - 699ms/epoch - 4ms/step\n",
            "Epoch 24/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4973 - val_loss: 0.7381 - val_accuracy: 0.4997 - 639ms/epoch - 3ms/step\n",
            "Epoch 25/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.5007 - val_loss: 0.7381 - val_accuracy: 0.4997 - 717ms/epoch - 4ms/step\n",
            "Epoch 26/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4966 - val_loss: 0.7382 - val_accuracy: 0.4997 - 1s/epoch - 5ms/step\n",
            "Epoch 27/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.5007 - val_loss: 0.7382 - val_accuracy: 0.4997 - 691ms/epoch - 4ms/step\n",
            "Epoch 28/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4951 - val_loss: 0.7383 - val_accuracy: 0.4997 - 628ms/epoch - 3ms/step\n",
            "Epoch 29/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4992 - val_loss: 0.7383 - val_accuracy: 0.4997 - 688ms/epoch - 4ms/step\n",
            "Epoch 30/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4995 - val_loss: 0.7365 - val_accuracy: 0.4997 - 660ms/epoch - 3ms/step\n",
            "Epoch 31/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4993 - val_loss: 0.7368 - val_accuracy: 0.4997 - 687ms/epoch - 4ms/step\n",
            "Epoch 32/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4998 - val_loss: 0.7369 - val_accuracy: 0.4997 - 700ms/epoch - 4ms/step\n",
            "Epoch 33/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4976 - val_loss: 0.7370 - val_accuracy: 0.4997 - 686ms/epoch - 4ms/step\n",
            "Epoch 34/200\n",
            "196/196 - 1s - loss: 0.6926 - accuracy: 0.5003 - val_loss: 0.7329 - val_accuracy: 0.5000 - 685ms/epoch - 3ms/step\n",
            "Epoch 35/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4970 - val_loss: 0.7318 - val_accuracy: 0.4997 - 700ms/epoch - 4ms/step\n",
            "Epoch 36/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4958 - val_loss: 0.7320 - val_accuracy: 0.4997 - 703ms/epoch - 4ms/step\n",
            "Epoch 37/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4986 - val_loss: 0.7321 - val_accuracy: 0.4997 - 687ms/epoch - 4ms/step\n",
            "Epoch 38/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4982 - val_loss: 0.7322 - val_accuracy: 0.4997 - 642ms/epoch - 3ms/step\n",
            "Epoch 39/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4977 - val_loss: 0.7323 - val_accuracy: 0.4997 - 720ms/epoch - 4ms/step\n",
            "Epoch 40/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4999 - val_loss: 0.7325 - val_accuracy: 0.4997 - 652ms/epoch - 3ms/step\n",
            "Epoch 41/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4944 - val_loss: 0.7326 - val_accuracy: 0.4997 - 714ms/epoch - 4ms/step\n",
            "Epoch 42/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4911 - val_loss: 0.7327 - val_accuracy: 0.4999 - 712ms/epoch - 4ms/step\n",
            "Epoch 43/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4967 - val_loss: 0.7329 - val_accuracy: 0.4999 - 641ms/epoch - 3ms/step\n",
            "Epoch 44/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4994 - val_loss: 0.7330 - val_accuracy: 0.4997 - 693ms/epoch - 4ms/step\n",
            "Epoch 45/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4984 - val_loss: 0.7332 - val_accuracy: 0.4999 - 696ms/epoch - 4ms/step\n",
            "Epoch 46/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4951 - val_loss: 0.7333 - val_accuracy: 0.4997 - 684ms/epoch - 3ms/step\n",
            "Epoch 47/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5002 - val_loss: 0.7335 - val_accuracy: 0.4999 - 681ms/epoch - 3ms/step\n",
            "Epoch 48/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4959 - val_loss: 0.7336 - val_accuracy: 0.4999 - 651ms/epoch - 3ms/step\n",
            "Epoch 49/200\n",
            "196/196 - 1s - loss: 0.6924 - accuracy: 0.4963 - val_loss: 0.7338 - val_accuracy: 0.4997 - 672ms/epoch - 3ms/step\n",
            "Epoch 50/200\n",
            "196/196 - 1s - loss: 0.6942 - accuracy: 0.4967 - val_loss: 0.7273 - val_accuracy: 0.4998 - 686ms/epoch - 4ms/step\n",
            "Epoch 51/200\n",
            "196/196 - 1s - loss: 0.6930 - accuracy: 0.4944 - val_loss: 0.7262 - val_accuracy: 0.5001 - 706ms/epoch - 4ms/step\n",
            "Epoch 52/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4965 - val_loss: 0.7254 - val_accuracy: 0.5002 - 703ms/epoch - 4ms/step\n",
            "Epoch 53/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5019 - val_loss: 0.7255 - val_accuracy: 0.4998 - 650ms/epoch - 3ms/step\n",
            "Epoch 54/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5007 - val_loss: 0.7256 - val_accuracy: 0.4998 - 724ms/epoch - 4ms/step\n",
            "Epoch 55/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4953 - val_loss: 0.7267 - val_accuracy: 0.4998 - 697ms/epoch - 4ms/step\n",
            "Epoch 56/200\n",
            "196/196 - 1s - loss: 0.6926 - accuracy: 0.4956 - val_loss: 0.7262 - val_accuracy: 0.5001 - 715ms/epoch - 4ms/step\n",
            "Epoch 57/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4982 - val_loss: 0.7290 - val_accuracy: 0.4999 - 725ms/epoch - 4ms/step\n",
            "Epoch 58/200\n",
            "196/196 - 1s - loss: 0.6928 - accuracy: 0.4968 - val_loss: 0.7273 - val_accuracy: 0.5000 - 671ms/epoch - 3ms/step\n",
            "Epoch 59/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4998 - val_loss: 0.7273 - val_accuracy: 0.5000 - 684ms/epoch - 3ms/step\n",
            "Epoch 60/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5016 - val_loss: 0.7273 - val_accuracy: 0.4998 - 700ms/epoch - 4ms/step\n",
            "Epoch 61/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5007 - val_loss: 0.7274 - val_accuracy: 0.4998 - 656ms/epoch - 3ms/step\n",
            "Epoch 62/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4983 - val_loss: 0.7274 - val_accuracy: 0.4998 - 640ms/epoch - 3ms/step\n",
            "Epoch 63/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5007 - val_loss: 0.7274 - val_accuracy: 0.4998 - 662ms/epoch - 3ms/step\n",
            "Epoch 64/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4970 - val_loss: 0.7274 - val_accuracy: 0.4998 - 667ms/epoch - 3ms/step\n",
            "Epoch 65/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5006 - val_loss: 0.7274 - val_accuracy: 0.4998 - 689ms/epoch - 4ms/step\n",
            "Epoch 66/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5009 - val_loss: 0.7274 - val_accuracy: 0.5000 - 689ms/epoch - 4ms/step\n",
            "Epoch 67/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4940 - val_loss: 0.7274 - val_accuracy: 0.4998 - 651ms/epoch - 3ms/step\n",
            "Epoch 68/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5012 - val_loss: 0.7275 - val_accuracy: 0.5000 - 672ms/epoch - 3ms/step\n",
            "Epoch 69/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4972 - val_loss: 0.7275 - val_accuracy: 0.5000 - 647ms/epoch - 3ms/step\n",
            "Epoch 70/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4982 - val_loss: 0.7275 - val_accuracy: 0.5000 - 686ms/epoch - 4ms/step\n",
            "Epoch 71/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5003 - val_loss: 0.7275 - val_accuracy: 0.4998 - 661ms/epoch - 3ms/step\n",
            "Epoch 72/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4993 - val_loss: 0.7275 - val_accuracy: 0.4998 - 689ms/epoch - 4ms/step\n",
            "Epoch 73/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4974 - val_loss: 0.7275 - val_accuracy: 0.4998 - 690ms/epoch - 4ms/step\n",
            "Epoch 74/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4967 - val_loss: 0.7276 - val_accuracy: 0.4998 - 667ms/epoch - 3ms/step\n",
            "Epoch 75/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4982 - val_loss: 0.7276 - val_accuracy: 0.5000 - 709ms/epoch - 4ms/step\n",
            "Epoch 76/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4984 - val_loss: 0.7276 - val_accuracy: 0.5000 - 692ms/epoch - 4ms/step\n",
            "Epoch 77/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4960 - val_loss: 0.7276 - val_accuracy: 0.4998 - 689ms/epoch - 4ms/step\n",
            "Epoch 78/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4981 - val_loss: 0.7276 - val_accuracy: 0.4998 - 714ms/epoch - 4ms/step\n",
            "Epoch 79/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 689ms/epoch - 4ms/step\n",
            "Epoch 80/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4999 - val_loss: 0.7277 - val_accuracy: 0.5000 - 694ms/epoch - 4ms/step\n",
            "Epoch 81/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4931 - val_loss: 0.7277 - val_accuracy: 0.4998 - 695ms/epoch - 4ms/step\n",
            "Epoch 82/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4975 - val_loss: 0.7277 - val_accuracy: 0.4998 - 647ms/epoch - 3ms/step\n",
            "Epoch 83/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5008 - val_loss: 0.7277 - val_accuracy: 0.5000 - 667ms/epoch - 3ms/step\n",
            "Epoch 84/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4974 - val_loss: 0.7278 - val_accuracy: 0.5000 - 711ms/epoch - 4ms/step\n",
            "Epoch 85/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4985 - val_loss: 0.7278 - val_accuracy: 0.5000 - 685ms/epoch - 3ms/step\n",
            "Epoch 86/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4931 - val_loss: 0.7278 - val_accuracy: 0.5000 - 701ms/epoch - 4ms/step\n",
            "Epoch 87/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4974 - val_loss: 0.7278 - val_accuracy: 0.5000 - 662ms/epoch - 3ms/step\n",
            "Epoch 88/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5002 - val_loss: 0.7278 - val_accuracy: 0.4998 - 654ms/epoch - 3ms/step\n",
            "Epoch 89/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5016 - val_loss: 0.7279 - val_accuracy: 0.4998 - 626ms/epoch - 3ms/step\n",
            "Epoch 90/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5007 - val_loss: 0.7279 - val_accuracy: 0.4998 - 646ms/epoch - 3ms/step\n",
            "Epoch 91/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4999 - val_loss: 0.7279 - val_accuracy: 0.4998 - 700ms/epoch - 4ms/step\n",
            "Epoch 92/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4982 - val_loss: 0.7279 - val_accuracy: 0.4998 - 701ms/epoch - 4ms/step\n",
            "Epoch 93/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4959 - val_loss: 0.7279 - val_accuracy: 0.4998 - 697ms/epoch - 4ms/step\n",
            "Epoch 94/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4973 - val_loss: 0.7280 - val_accuracy: 0.5000 - 637ms/epoch - 3ms/step\n",
            "Epoch 95/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4965 - val_loss: 0.7280 - val_accuracy: 0.4998 - 687ms/epoch - 4ms/step\n",
            "Epoch 96/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4935 - val_loss: 0.7280 - val_accuracy: 0.4998 - 690ms/epoch - 4ms/step\n",
            "Epoch 97/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4959 - val_loss: 0.7280 - val_accuracy: 0.4998 - 685ms/epoch - 3ms/step\n",
            "Epoch 98/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4984 - val_loss: 0.7281 - val_accuracy: 0.4998 - 663ms/epoch - 3ms/step\n",
            "Epoch 99/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4998 - val_loss: 0.7281 - val_accuracy: 0.4998 - 659ms/epoch - 3ms/step\n",
            "Epoch 100/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4957 - val_loss: 0.7281 - val_accuracy: 0.4998 - 678ms/epoch - 3ms/step\n",
            "Epoch 101/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4976 - val_loss: 0.7281 - val_accuracy: 0.4998 - 654ms/epoch - 3ms/step\n",
            "Epoch 102/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4991 - val_loss: 0.7282 - val_accuracy: 0.4998 - 729ms/epoch - 4ms/step\n",
            "Epoch 103/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4986 - val_loss: 0.7282 - val_accuracy: 0.4998 - 692ms/epoch - 4ms/step\n",
            "Epoch 104/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4954 - val_loss: 0.7282 - val_accuracy: 0.4999 - 648ms/epoch - 3ms/step\n",
            "Epoch 105/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4931 - val_loss: 0.7283 - val_accuracy: 0.4998 - 711ms/epoch - 4ms/step\n",
            "Epoch 106/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4938 - val_loss: 0.7283 - val_accuracy: 0.4999 - 653ms/epoch - 3ms/step\n",
            "Epoch 107/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4978 - val_loss: 0.7283 - val_accuracy: 0.4999 - 657ms/epoch - 3ms/step\n",
            "Epoch 108/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4945 - val_loss: 0.7283 - val_accuracy: 0.4999 - 698ms/epoch - 4ms/step\n",
            "Epoch 109/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.5002 - val_loss: 0.7284 - val_accuracy: 0.4998 - 701ms/epoch - 4ms/step\n",
            "Epoch 110/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4978 - val_loss: 0.7284 - val_accuracy: 0.4998 - 664ms/epoch - 3ms/step\n",
            "Epoch 111/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4961 - val_loss: 0.7284 - val_accuracy: 0.4998 - 686ms/epoch - 4ms/step\n",
            "Epoch 112/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4977 - val_loss: 0.7285 - val_accuracy: 0.4999 - 681ms/epoch - 3ms/step\n",
            "Epoch 113/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4980 - val_loss: 0.7285 - val_accuracy: 0.4999 - 650ms/epoch - 3ms/step\n",
            "Epoch 114/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4980 - val_loss: 0.7285 - val_accuracy: 0.4998 - 699ms/epoch - 4ms/step\n",
            "Epoch 115/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4985 - val_loss: 0.7285 - val_accuracy: 0.4998 - 681ms/epoch - 3ms/step\n",
            "Epoch 116/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4985 - val_loss: 0.7286 - val_accuracy: 0.4999 - 682ms/epoch - 3ms/step\n",
            "Epoch 117/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4959 - val_loss: 0.7286 - val_accuracy: 0.4998 - 733ms/epoch - 4ms/step\n",
            "Epoch 118/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4943 - val_loss: 0.7286 - val_accuracy: 0.4998 - 657ms/epoch - 3ms/step\n",
            "Epoch 119/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4992 - val_loss: 0.7287 - val_accuracy: 0.4998 - 645ms/epoch - 3ms/step\n",
            "Epoch 120/200\n",
            "196/196 - 1s - loss: 0.6925 - accuracy: 0.4947 - val_loss: 0.7275 - val_accuracy: 0.4998 - 693ms/epoch - 4ms/step\n",
            "Epoch 121/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4976 - val_loss: 0.7275 - val_accuracy: 0.5000 - 659ms/epoch - 3ms/step\n",
            "Epoch 122/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4986 - val_loss: 0.7276 - val_accuracy: 0.4998 - 637ms/epoch - 3ms/step\n",
            "Epoch 123/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4970 - val_loss: 0.7276 - val_accuracy: 0.4998 - 654ms/epoch - 3ms/step\n",
            "Epoch 124/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4996 - val_loss: 0.7276 - val_accuracy: 0.4998 - 713ms/epoch - 4ms/step\n",
            "Epoch 125/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7276 - val_accuracy: 0.4998 - 673ms/epoch - 3ms/step\n",
            "Epoch 126/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4969 - val_loss: 0.7276 - val_accuracy: 0.5000 - 705ms/epoch - 4ms/step\n",
            "Epoch 127/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4937 - val_loss: 0.7276 - val_accuracy: 0.4998 - 699ms/epoch - 4ms/step\n",
            "Epoch 128/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4993 - val_loss: 0.7276 - val_accuracy: 0.5000 - 636ms/epoch - 3ms/step\n",
            "Epoch 129/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4995 - val_loss: 0.7276 - val_accuracy: 0.5000 - 692ms/epoch - 4ms/step\n",
            "Epoch 130/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4960 - val_loss: 0.7276 - val_accuracy: 0.4998 - 694ms/epoch - 4ms/step\n",
            "Epoch 131/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4937 - val_loss: 0.7276 - val_accuracy: 0.4998 - 630ms/epoch - 3ms/step\n",
            "Epoch 132/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4956 - val_loss: 0.7276 - val_accuracy: 0.4998 - 710ms/epoch - 4ms/step\n",
            "Epoch 133/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4972 - val_loss: 0.7276 - val_accuracy: 0.4998 - 662ms/epoch - 3ms/step\n",
            "Epoch 134/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4990 - val_loss: 0.7276 - val_accuracy: 0.4998 - 640ms/epoch - 3ms/step\n",
            "Epoch 135/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4982 - val_loss: 0.7276 - val_accuracy: 0.4998 - 654ms/epoch - 3ms/step\n",
            "Epoch 136/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4974 - val_loss: 0.7277 - val_accuracy: 0.4998 - 681ms/epoch - 3ms/step\n",
            "Epoch 137/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4962 - val_loss: 0.7277 - val_accuracy: 0.4998 - 683ms/epoch - 3ms/step\n",
            "Epoch 138/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4989 - val_loss: 0.7277 - val_accuracy: 0.5000 - 695ms/epoch - 4ms/step\n",
            "Epoch 139/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4978 - val_loss: 0.7277 - val_accuracy: 0.5000 - 678ms/epoch - 3ms/step\n",
            "Epoch 140/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4962 - val_loss: 0.7277 - val_accuracy: 0.5000 - 649ms/epoch - 3ms/step\n",
            "Epoch 141/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4990 - val_loss: 0.7277 - val_accuracy: 0.4998 - 688ms/epoch - 4ms/step\n",
            "Epoch 142/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 691ms/epoch - 4ms/step\n",
            "Epoch 143/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4938 - val_loss: 0.7277 - val_accuracy: 0.5000 - 683ms/epoch - 3ms/step\n",
            "Epoch 144/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4985 - val_loss: 0.7277 - val_accuracy: 0.5000 - 709ms/epoch - 4ms/step\n",
            "Epoch 145/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4992 - val_loss: 0.7277 - val_accuracy: 0.5000 - 650ms/epoch - 3ms/step\n",
            "Epoch 146/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4933 - val_loss: 0.7277 - val_accuracy: 0.5000 - 687ms/epoch - 4ms/step\n",
            "Epoch 147/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4997 - val_loss: 0.7277 - val_accuracy: 0.4998 - 704ms/epoch - 4ms/step\n",
            "Epoch 148/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4998 - val_loss: 0.7277 - val_accuracy: 0.5000 - 656ms/epoch - 3ms/step\n",
            "Epoch 149/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4986 - val_loss: 0.7277 - val_accuracy: 0.4998 - 627ms/epoch - 3ms/step\n",
            "Epoch 150/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 693ms/epoch - 4ms/step\n",
            "Epoch 151/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4949 - val_loss: 0.7277 - val_accuracy: 0.4998 - 651ms/epoch - 3ms/step\n",
            "Epoch 152/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4979 - val_loss: 0.7277 - val_accuracy: 0.5000 - 648ms/epoch - 3ms/step\n",
            "Epoch 153/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4996 - val_loss: 0.7277 - val_accuracy: 0.4998 - 703ms/epoch - 4ms/step\n",
            "Epoch 154/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5011 - val_loss: 0.7277 - val_accuracy: 0.5000 - 684ms/epoch - 3ms/step\n",
            "Epoch 155/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4986 - val_loss: 0.7277 - val_accuracy: 0.4998 - 670ms/epoch - 3ms/step\n",
            "Epoch 156/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4971 - val_loss: 0.7277 - val_accuracy: 0.4998 - 695ms/epoch - 4ms/step\n",
            "Epoch 157/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5012 - val_loss: 0.7277 - val_accuracy: 0.5000 - 685ms/epoch - 3ms/step\n",
            "Epoch 158/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5002 - val_loss: 0.7277 - val_accuracy: 0.4998 - 643ms/epoch - 3ms/step\n",
            "Epoch 159/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4937 - val_loss: 0.7277 - val_accuracy: 0.4998 - 736ms/epoch - 4ms/step\n",
            "Epoch 160/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4967 - val_loss: 0.7277 - val_accuracy: 0.4998 - 691ms/epoch - 4ms/step\n",
            "Epoch 161/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4984 - val_loss: 0.7277 - val_accuracy: 0.4998 - 633ms/epoch - 3ms/step\n",
            "Epoch 162/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4986 - val_loss: 0.7277 - val_accuracy: 0.4998 - 684ms/epoch - 3ms/step\n",
            "Epoch 163/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 720ms/epoch - 4ms/step\n",
            "Epoch 164/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4986 - val_loss: 0.7277 - val_accuracy: 0.4998 - 679ms/epoch - 3ms/step\n",
            "Epoch 165/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4982 - val_loss: 0.7277 - val_accuracy: 0.4998 - 708ms/epoch - 4ms/step\n",
            "Epoch 166/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 672ms/epoch - 3ms/step\n",
            "Epoch 167/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4992 - val_loss: 0.7277 - val_accuracy: 0.4998 - 685ms/epoch - 3ms/step\n",
            "Epoch 168/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4980 - val_loss: 0.7277 - val_accuracy: 0.4998 - 649ms/epoch - 3ms/step\n",
            "Epoch 169/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4972 - val_loss: 0.7277 - val_accuracy: 0.4998 - 688ms/epoch - 4ms/step\n",
            "Epoch 170/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5009 - val_loss: 0.7277 - val_accuracy: 0.5000 - 697ms/epoch - 4ms/step\n",
            "Epoch 171/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5000 - val_loss: 0.7277 - val_accuracy: 0.5000 - 727ms/epoch - 4ms/step\n",
            "Epoch 172/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4976 - val_loss: 0.7277 - val_accuracy: 0.4998 - 709ms/epoch - 4ms/step\n",
            "Epoch 173/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4988 - val_loss: 0.7277 - val_accuracy: 0.4998 - 666ms/epoch - 3ms/step\n",
            "Epoch 174/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5005 - val_loss: 0.7277 - val_accuracy: 0.5000 - 638ms/epoch - 3ms/step\n",
            "Epoch 175/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4971 - val_loss: 0.7277 - val_accuracy: 0.4998 - 650ms/epoch - 3ms/step\n",
            "Epoch 176/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4954 - val_loss: 0.7277 - val_accuracy: 0.4998 - 690ms/epoch - 4ms/step\n",
            "Epoch 177/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 697ms/epoch - 4ms/step\n",
            "Epoch 178/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4975 - val_loss: 0.7277 - val_accuracy: 0.5000 - 714ms/epoch - 4ms/step\n",
            "Epoch 179/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4979 - val_loss: 0.7277 - val_accuracy: 0.4998 - 649ms/epoch - 3ms/step\n",
            "Epoch 180/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4941 - val_loss: 0.7277 - val_accuracy: 0.4998 - 692ms/epoch - 4ms/step\n",
            "Epoch 181/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4982 - val_loss: 0.7277 - val_accuracy: 0.4998 - 672ms/epoch - 3ms/step\n",
            "Epoch 182/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5007 - val_loss: 0.7277 - val_accuracy: 0.4998 - 669ms/epoch - 3ms/step\n",
            "Epoch 183/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4982 - val_loss: 0.7277 - val_accuracy: 0.4998 - 709ms/epoch - 4ms/step\n",
            "Epoch 184/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5002 - val_loss: 0.7277 - val_accuracy: 0.5000 - 708ms/epoch - 4ms/step\n",
            "Epoch 185/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4956 - val_loss: 0.7277 - val_accuracy: 0.4998 - 693ms/epoch - 4ms/step\n",
            "Epoch 186/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.5002 - val_loss: 0.7277 - val_accuracy: 0.5000 - 684ms/epoch - 3ms/step\n",
            "Epoch 187/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4952 - val_loss: 0.7277 - val_accuracy: 0.5000 - 652ms/epoch - 3ms/step\n",
            "Epoch 188/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4988 - val_loss: 0.7277 - val_accuracy: 0.4998 - 656ms/epoch - 3ms/step\n",
            "Epoch 189/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4969 - val_loss: 0.7277 - val_accuracy: 0.4998 - 648ms/epoch - 3ms/step\n",
            "Epoch 190/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4996 - val_loss: 0.7277 - val_accuracy: 0.4998 - 712ms/epoch - 4ms/step\n",
            "Epoch 191/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4940 - val_loss: 0.7277 - val_accuracy: 0.4998 - 691ms/epoch - 4ms/step\n",
            "Epoch 192/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4951 - val_loss: 0.7277 - val_accuracy: 0.4998 - 689ms/epoch - 4ms/step\n",
            "Epoch 193/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4966 - val_loss: 0.7277 - val_accuracy: 0.5000 - 713ms/epoch - 4ms/step\n",
            "Epoch 194/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4991 - val_loss: 0.7277 - val_accuracy: 0.4998 - 686ms/epoch - 3ms/step\n",
            "Epoch 195/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4997 - val_loss: 0.7277 - val_accuracy: 0.4998 - 679ms/epoch - 3ms/step\n",
            "Epoch 196/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4982 - val_loss: 0.7277 - val_accuracy: 0.4998 - 647ms/epoch - 3ms/step\n",
            "Epoch 197/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4986 - val_loss: 0.7277 - val_accuracy: 0.4998 - 668ms/epoch - 3ms/step\n",
            "Epoch 198/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4958 - val_loss: 0.7277 - val_accuracy: 0.4998 - 648ms/epoch - 3ms/step\n",
            "Epoch 199/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4950 - val_loss: 0.7277 - val_accuracy: 0.4998 - 646ms/epoch - 3ms/step\n",
            "Epoch 200/200\n",
            "196/196 - 1s - loss: 0.6927 - accuracy: 0.4968 - val_loss: 0.7277 - val_accuracy: 0.4998 - 649ms/epoch - 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Loss"
      ],
      "metadata": {
        "id": "gG-7uA5mgNmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Underfit')"
      ],
      "metadata": {
        "id": "vah2BKNJgN07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "6800e0ae-7b51-45c2-eaf0-8f119dee3808"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"2ae73eb5-b2b4-4dbe-b9db-7e9c368c54ef\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2ae73eb5-b2b4-4dbe-b9db-7e9c368c54ef\")) {                    Plotly.newPlot(                        \"2ae73eb5-b2b4-4dbe-b9db-7e9c368c54ef\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"loss\",\"y\":[31.33655548095703,1.0671637058258057,0.7922991514205933,0.7341152429580688,0.7102455496788025,0.7024720311164856,0.7001212239265442,0.6983133554458618,0.6964041590690613,0.6953108310699463,0.6948118209838867,0.6941794753074646,0.6935311555862427,0.6928308606147766,0.6923978924751282,0.6923964619636536,0.6923988461494446,0.6923929452896118,0.692402720451355,0.692388653755188,0.6924099326133728,0.6923969388008118,0.6923944354057312,0.6923956871032715,0.6923863887786865,0.6923946738243103,0.6923930644989014,0.6923945546150208,0.6923953890800476,0.6923962235450745,0.6924034953117371,0.6923968195915222,0.692402720451355,0.6925694346427917,0.6924644708633423,0.6924581527709961,0.6924532055854797,0.692462682723999,0.6924552917480469,0.6924554705619812,0.6924653649330139,0.692460298538208,0.6924567222595215,0.6924629807472229,0.6924504637718201,0.6924465894699097,0.6924545168876648,0.6924525499343872,0.6924451589584351,0.6941779851913452,0.692989706993103,0.6924891471862793,0.6924977898597717,0.6924875974655151,0.6925032734870911,0.6925591230392456,0.6925226449966431,0.6928497552871704,0.6924742460250854,0.6924711465835571,0.6924649477005005,0.6924703121185303,0.692470371723175,0.6924678683280945,0.6924765110015869,0.6924692392349243,0.6924780607223511,0.6924623250961304,0.6924717426300049,0.6924691200256348,0.6924723982810974,0.6924632787704468,0.6924718022346497,0.6924750804901123,0.6924771666526794,0.692465603351593,0.6924740076065063,0.692473292350769,0.692472517490387,0.6924693584442139,0.692468523979187,0.692474365234375,0.6924644708633423,0.6924762725830078,0.6924710869789124,0.6924670338630676,0.6924742460250854,0.6924751400947571,0.692465603351593,0.6924811005592346,0.6924692392349243,0.6924721598625183,0.6924744248390198,0.6924845576286316,0.6924682855606079,0.6924718022346497,0.6924697756767273,0.6924795508384705,0.6924695372581482,0.692472517490387,0.6924715042114258,0.6924712657928467,0.6924756169319153,0.6924713850021362,0.6924788355827332,0.6924765110015869,0.6924670934677124,0.6924723982810974,0.6924645304679871,0.6924642324447632,0.6924703121185303,0.6924683451652527,0.692470371723175,0.6924798488616943,0.692482590675354,0.6924800276756287,0.6924672722816467,0.6924688220024109,0.6924797892570496,0.692479133605957,0.6926674246788025,0.692660927772522,0.6926619410514832,0.6926626563072205,0.6926664113998413,0.6926656365394592,0.6926596760749817,0.692660391330719,0.6926627159118652,0.6926666498184204,0.6926679611206055,0.6926641464233398,0.6926649212837219,0.6926647424697876,0.6926631927490234,0.6926589608192444,0.6926664113998413,0.6926611065864563,0.6926664113998413,0.6926658749580383,0.6926647424697876,0.6926612257957458,0.6926619410514832,0.6926689743995667,0.6926641464233398,0.6926712989807129,0.6926625967025757,0.6926637291908264,0.6926640868186951,0.6926686763763428,0.6926669478416443,0.6926590800285339,0.6926748156547546,0.692655622959137,0.6926608085632324,0.6926684379577637,0.6926639080047607,0.6926670074462891,0.69266676902771,0.6926653385162354,0.6926590800285339,0.6926597952842712,0.6926621794700623,0.6926574110984802,0.69266676902771,0.6926551461219788,0.6926597952842712,0.692669153213501,0.692659854888916,0.6926578283309937,0.6926666498184204,0.692671000957489,0.6926683783531189,0.6926694512367249,0.692661464214325,0.6926676630973816,0.6926590800285339,0.6926681399345398,0.6926631331443787,0.6926699876785278,0.6926702260971069,0.6926681995391846,0.6926599144935608,0.692660927772522,0.6926724314689636,0.692659854888916,0.6926707029342651,0.6926597952842712,0.692659318447113,0.6926684379577637,0.6926680207252502,0.6926762461662292,0.6926632523536682,0.692661702632904,0.6926639080047607,0.6926641464233398,0.6926702857017517,0.6926712989807129,0.6926630735397339,0.6926671862602234],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_loss\",\"y\":[1.3615381717681885,0.8799717426300049,0.7993626594543457,0.7669766545295715,0.7538546919822693,0.7490684986114502,0.7461377382278442,0.7423259615898132,0.7409412264823914,0.7401690483093262,0.7401576042175293,0.7387346029281616,0.7374445796012878,0.7373408079147339,0.7377469539642334,0.7377778887748718,0.7378137707710266,0.7378503084182739,0.7378837466239929,0.737920880317688,0.7379563450813293,0.7379993796348572,0.738041877746582,0.7380849719047546,0.7381295561790466,0.7381710410118103,0.7382307052612305,0.7382893562316895,0.7383440136909485,0.7365396022796631,0.7367899417877197,0.7368881106376648,0.7369650602340698,0.7329043745994568,0.7318179607391357,0.7319599390029907,0.7320770025253296,0.7322027087211609,0.7323281764984131,0.7324603796005249,0.7325959205627441,0.7327297925949097,0.7328721880912781,0.7330042719841003,0.733164370059967,0.7333096861839294,0.7334553003311157,0.7336192727088928,0.7337785363197327,0.7272752523422241,0.7262189984321594,0.7253994345664978,0.7255167961120605,0.7255856990814209,0.7266638875007629,0.7262193560600281,0.7290201783180237,0.7272849082946777,0.7273168563842773,0.7273331880569458,0.7273500561714172,0.7273638844490051,0.7273814678192139,0.727397084236145,0.727413535118103,0.727429986000061,0.7274464964866638,0.7274666428565979,0.7274797558784485,0.7275000810623169,0.7275139093399048,0.7275324463844299,0.7275484204292297,0.7275644540786743,0.727587103843689,0.7276044487953186,0.7276215553283691,0.7276409864425659,0.7276594638824463,0.7276756167411804,0.7276976704597473,0.7277175784111023,0.7277359366416931,0.7277536988258362,0.7277762293815613,0.727798581123352,0.7278188467025757,0.7278375625610352,0.727860152721405,0.7278797626495361,0.7279038429260254,0.7279225587844849,0.7279428243637085,0.7279632091522217,0.7279859185218811,0.7280113101005554,0.7280356287956238,0.7280624508857727,0.7280880212783813,0.7281143665313721,0.72813880443573,0.7281633019447327,0.728198230266571,0.7282275557518005,0.7282541394233704,0.7282829880714417,0.7283104658126831,0.7283433079719543,0.7283677458763123,0.7283971905708313,0.7284297943115234,0.7284605503082275,0.7284907102584839,0.7285165786743164,0.7285453081130981,0.7285741567611694,0.7286023497581482,0.7286332845687866,0.7286657094955444,0.7274662256240845,0.7275460958480835,0.7275604605674744,0.7275668978691101,0.7275754809379578,0.7275852560997009,0.7275961637496948,0.7276033759117126,0.7276149988174438,0.7276294827461243,0.7276288270950317,0.7276322841644287,0.7276368737220764,0.7276399731636047,0.7276428937911987,0.727644145488739,0.7276506423950195,0.7276570200920105,0.7276598215103149,0.727662205696106,0.7276657819747925,0.7276691198348999,0.7276732921600342,0.7276830673217773,0.7276850938796997,0.7276975512504578,0.7276982069015503,0.7277037501335144,0.7277045249938965,0.7277047038078308,0.7277038097381592,0.7277034521102905,0.7277061939239502,0.7277041673660278,0.7277049422264099,0.7277037501335144,0.7277042269706726,0.7277052402496338,0.727703332901001,0.7277048230171204,0.7277043461799622,0.7277032732963562,0.7277039885520935,0.7277042865753174,0.7277039289474487,0.7277035117149353,0.7277043461799622,0.7277035713195801,0.7277035713195801,0.7277035117149353,0.7277049422264099,0.7277050614356995,0.7277082800865173,0.7277061939239502,0.7277047038078308,0.7277035117149353,0.7277042269706726,0.7277043461799622,0.7277045249938965,0.7277040481567383,0.7277042865753174,0.7277035713195801,0.7277037501335144,0.7277035117149353,0.7277055382728577,0.7277047038078308,0.7277045249938965,0.7277061939239502,0.7277042269706726,0.7277039289474487,0.7277048230171204,0.7277040481567383,0.7277039289474487,0.7277044653892517,0.7277041673660278,0.7277034521102905,0.7277032732963562,0.7277042269706726,0.7277039885520935,0.7277039885520935,0.7277047634124756],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2ae73eb5-b2b4-4dbe-b9db-7e9c368c54ef');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy"
      ],
      "metadata": {
        "id": "HHLepzqHgQnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Underfit')"
      ],
      "metadata": {
        "id": "xniumuxlgQz5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "6214e14f-de2d-45bd-bcf0-72ecbf27bc14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"7fa21004-99ae-4e87-9840-21847b8835a5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7fa21004-99ae-4e87-9840-21847b8835a5\")) {                    Plotly.newPlot(                        \"7fa21004-99ae-4e87-9840-21847b8835a5\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"acc\",\"y\":[0.4970400035381317,0.5010799765586853,0.49639999866485596,0.5009599924087524,0.4976400136947632,0.5009599924087524,0.49880000948905945,0.49696001410484314,0.49880000948905945,0.4938400089740753,0.5004000067710876,0.4978800117969513,0.4979200065135956,0.49851998686790466,0.4973999857902527,0.4947200119495392,0.49772000312805176,0.49803999066352844,0.5007200241088867,0.4997600018978119,0.4994400143623352,0.5005199909210205,0.4967600107192993,0.4973199963569641,0.5007200241088867,0.4965600073337555,0.5007200241088867,0.49507999420166016,0.4991599917411804,0.4995200037956238,0.49928000569343567,0.4997600018978119,0.4975999891757965,0.5003200173377991,0.4970400035381317,0.49584001302719116,0.4986400008201599,0.49823999404907227,0.49772000312805176,0.49987998604774475,0.4944399893283844,0.49107998609542847,0.49671998620033264,0.49935999512672424,0.4984399974346161,0.49507999420166016,0.5001999735832214,0.49588000774383545,0.4963200092315674,0.49667999148368835,0.4944399893283844,0.49647998809814453,0.5018799901008606,0.5007200241088867,0.49531999230384827,0.49564000964164734,0.4981600046157837,0.4968400001525879,0.4997999966144562,0.5016400218009949,0.5007200241088867,0.49827998876571655,0.5007200241088867,0.4970400035381317,0.5005599856376648,0.5008800029754639,0.49399998784065247,0.5012000203132629,0.49724000692367554,0.4981600046157837,0.5002800226211548,0.49932000041007996,0.4973599910736084,0.49671998620033264,0.4981600046157837,0.4983600080013275,0.4959999918937683,0.4981200098991394,0.5007200241088867,0.4999200105667114,0.4930799901485443,0.49751999974250793,0.5008000135421753,0.4973999857902527,0.4984799921512604,0.493120014667511,0.49744001030921936,0.5002400279045105,0.5016000270843506,0.5007200241088867,0.4999200105667114,0.4981600046157837,0.49588000774383545,0.4973199963569641,0.49647998809814453,0.49351999163627625,0.49588000774383545,0.4984000027179718,0.49983999133110046,0.4957199990749359,0.4975599944591522,0.49911999702453613,0.4986400008201599,0.49540001153945923,0.4930799901485443,0.4938400089740753,0.4978399872779846,0.4944800138473511,0.5001999735832214,0.4978399872779846,0.4960800111293793,0.49768000841140747,0.49799999594688416,0.49799999594688416,0.4984799921512604,0.49851998686790466,0.49588000774383545,0.49428001046180725,0.4992400109767914,0.4947200119495392,0.4975999891757965,0.4986000061035156,0.49696001410484314,0.49963998794555664,0.5007200241088867,0.49691998958587646,0.49371999502182007,0.49932000041007996,0.4995200037956238,0.4960399866104126,0.49371999502182007,0.49559998512268066,0.49720001220703125,0.49904000759124756,0.49823999404907227,0.4973599910736084,0.4962399899959564,0.498879998922348,0.49775999784469604,0.49616000056266785,0.49904000759124756,0.5007200241088867,0.49380001425743103,0.49851998686790466,0.4991599917411804,0.4933199882507324,0.4996800124645233,0.4997999966144562,0.4986000061035156,0.5007200241088867,0.49487999081611633,0.4979200065135956,0.49963998794555664,0.5010799765586853,0.4986000061035156,0.497079998254776,0.5012000203132629,0.5001599788665771,0.4936800003051758,0.49667999148368835,0.4984399974346161,0.49856001138687134,0.5007200241088867,0.4986000061035156,0.49823999404907227,0.5007200241088867,0.4991599917411804,0.49799999594688416,0.49724000692367554,0.5008800029754639,0.5,0.4975999891757965,0.49876001477241516,0.5004799962043762,0.497079998254776,0.4954400062561035,0.5007200241088867,0.49751999974250793,0.4979200065135956,0.4940800070762634,0.4981600046157837,0.5007200241088867,0.4981600046157837,0.5001599788665771,0.49559998512268066,0.5002400279045105,0.4952000081539154,0.49880000948905945,0.4968799948692322,0.49955999851226807,0.49404001235961914,0.49511998891830444,0.49663999676704407,0.49908000230789185,0.4996800124645233,0.49823999404907227,0.4986000061035156,0.49584001302719116,0.4950000047683716,0.4968000054359436],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_acc\",\"y\":[0.5022799968719482,0.5003200173377991,0.5,0.5000399947166443,0.5000399947166443,0.4997999966144562,0.4996800124645233,0.4997200071811676,0.4997200071811676,0.4996800124645233,0.4996800124645233,0.4997200071811676,0.4997600018978119,0.49983999133110046,0.4997200071811676,0.49983999133110046,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.49983999133110046,0.49983999133110046,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4999600052833557,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4997200071811676,0.4999200105667114,0.4999200105667114,0.4997200071811676,0.4999200105667114,0.4997200071811676,0.4999200105667114,0.4999200105667114,0.4996800124645233,0.4997999966144562,0.5000799894332886,0.5001599788665771,0.4997999966144562,0.4997999966144562,0.4997600018978119,0.5000799894332886,0.49987998604774475,0.5,0.5,0.4997600018978119,0.4997600018978119,0.4997600018978119,0.4997600018978119,0.4997600018978119,0.4997600018978119,0.5,0.4997600018978119,0.5,0.5,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.5,0.5,0.4999600052833557,0.4999600052833557,0.4999600052833557,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4999600052833557,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4999200105667114,0.4997999966144562,0.4999200105667114,0.4999200105667114,0.4999200105667114,0.4997999966144562,0.49983999133110046,0.49983999133110046,0.4999200105667114,0.4999200105667114,0.49983999133110046,0.49983999133110046,0.4999200105667114,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.5,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.5,0.49983999133110046,0.5,0.5,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.49983999133110046,0.4997999966144562,0.5,0.5,0.5,0.4997999966144562,0.4997999966144562,0.5,0.5,0.5,0.5,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.5,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.5,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.5,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562,0.4997999966144562],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7fa21004-99ae-4e87-9840-21847b8835a5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "จากกราฟ Loss จะเห็นว่าตั้งแต่ Epoch ที่ 1 ค่า Training Loss จะค่อนข้างราบเรียบ ไม่ลดลง และจากกราฟ Accuracy ค่า Training Accuracy จะเหวี่ยงไปมา ไม่มีแนวโน้มจะเพิ่มขึ้น ซึ่งรูปแบบ Learning Curve ดังกล่าว แสดงให้เห็นว่า Model ของเราเกิดปัญหา Underfitting ครับ"
      ],
      "metadata": {
        "id": "LL5CObG-gXrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfit Learning Curve\n",
        "Learning Curve แบบ Overfitting จะบ่งบอกว่า Model มีการเรียนรู้ที่ดีเกินไปจาก Training Dataset ซึ่งรวมทั้งรูปแบบของ Noise หรือความผันผวนของ Training Dataset\n",
        "ซึ่งเราจะจำลองสถานการณ์ของ Model ที่มีปัญหาการเรียนรู้แบบ Overfitting ด้วยการพัฒนา Model เพื่อ Classfify ข้อมูลจำนวน 2 Class โดยมีขั้นตอนดังต่อไปนี้"
      ],
      "metadata": {
        "id": "6ARGCsrBgY7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สร้าง Dataset แบบ 2 Class โดยใช้ Function make_circles ของ Sklearn"
      ],
      "metadata": {
        "id": "yiwDQBD-gfE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y =  make_circles(n_samples=500, noise=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "0AZai3ebgX33"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "แบ่งข้อมูลสำหรับ Train และ Validate โดยการสุ่มในสัดส่วน 50:50"
      ],
      "metadata": {
        "id": "54zIjVc-giFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.5, shuffle= True)\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "5u2nw14SgiPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd94a057-0987-4383-f5b0-42a7164abf50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((250, 2), (250,), (250, 2), (250,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "นำ Dataset ส่วนที่ Train มาแปลงเป็น DataFrame โดยเปลี่ยนชนิดข้อมูลใน Column \"class\" เป็น String เพื่อทำให้สามารถแสดงสีแบบไม่ต่อเนื่องได้ แล้วนำไป Plot"
      ],
      "metadata": {
        "id": "62NMJfCxgnu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.DataFrame(x_train, columns=['x', 'y'])\n",
        "y_train_pd = pd.DataFrame(y_train, columns=['class'])\n",
        "\n",
        "df = pd.concat([x_train_pd, y_train_pd], axis=1)\n",
        "df[\"class\"] = df[\"class\"].astype(str)"
      ],
      "metadata": {
        "id": "7NY5IUiigoBo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"class\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "nGjvx6B_gqLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "70b8b0c8-19c9-4840-cc64-f8b9d128f112"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"e749e735-4dbc-4ef4-80c1-33b86cb7b19d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e749e735-4dbc-4ef4-80c1-33b86cb7b19d\")) {                    Plotly.newPlot(                        \"e749e735-4dbc-4ef4-80c1-33b86cb7b19d\",                        [{\"hovertemplate\":\"class=0<br>x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[-0.6028383525036348,-0.5001534048962979,0.9862535407151367,0.3107489405215032,0.5864099597369272,0.8268679532628274,0.9349325033253533,-0.6982222640452317,-0.20961973836679934,0.8727973824727138,0.010999637845901189,0.7239909671065108,-0.33228224028522185,-0.5058727154377309,-0.8806913947114922,0.9181365577218767,0.6696580366302269,-0.8880009468760773,0.10033711867018956,-0.6720833534470899,0.945360616630127,-1.3147132054203416,-1.0918407366613323,1.107842735670965,0.556653360229603,0.9302548462672364,-0.15810118844575702,-0.28958104269502444,-0.8173631925372329,0.031058079562433366,0.8257111193711605,0.2207216914688167,-0.608043466651247,0.38387066866563124,1.050495279292154,0.06515527834848678,-0.2813211058819132,-0.8332764948858624,-0.9144544793486853,0.4971064570885525,-1.019862737710586,-0.9094024344554456,-1.055463993944147,-0.7059104198909776,-0.2551067054996031,0.2805845870119284,0.8090297650084489,-0.7017784118636543,-0.13663460683224266,-0.32300152809236216,1.073021873137241,0.9180579713185267,0.7170330815441683,0.9441904295913734,-0.9206529503759324,0.04143973860926137,0.141688493988,0.492228969988026,0.7417861880149629,-0.8475751574397932,-0.22361235300597007,-0.04418215713345658,-0.3461345953299879,0.9732378559974486,1.2131565488275908,0.7829695165630282,0.9205675210117161,0.11439239048216493,-0.9390589932112374,0.2880167592328094,-0.9237927882786263,-1.0245178579813718,1.0541590633960147,0.6072129824624914,0.6790713886576252,0.15896915171253628,-0.2292121588898106,1.0136904380650047,-0.6774626091595594,-0.5735728253236895,-0.1453244277367744,-0.8921718353961812,-0.2720268164547762,0.46468648997224704,-0.457591443051186,0.39364228644392885,0.977460695915526,0.24988635824533625,0.7946190703065192,1.0424646819271515,0.9936774087356455,0.7442734140112879,-0.7361916759907358,1.2482294553778768,0.3873838414268476,-0.25398002580409323,0.8621971332626195,-1.0702451841932037,-0.76489468882271,-0.7614813906388476,-0.004441821465416967,0.22925366847151646,0.46957891024418086,-0.3013094997226106,-0.04439582923424784,0.6903066206381665,-0.37606887612652473,-0.7689126749720216,-0.3287722383192827,0.30025293865392455,-0.12623243711752552,1.157076863338538,0.9776009969239322],\"xaxis\":\"x\",\"y\":[-0.9834147553220141,0.3647310801140405,0.5615867384737747,0.7066890018165557,-1.0363921839132426,-0.5587004643836245,-0.09964344688488758,0.1400547032410735,-0.9104517771750895,-0.9478121407735174,-1.0751666686383785,0.47571666110608546,-1.5929146136622814,-0.3861444628810615,0.6958471291054985,-0.7274004418513093,0.56899182363057,0.4889290696383325,0.7460617987468894,0.11156208834301783,-0.2927500971382436,0.22281866857465643,-0.5627192834399729,-0.516310410029065,-0.6981140193911626,-0.5030898575582134,-0.9466008171853808,0.7315776141075806,-0.363755535835687,0.918174054098431,-0.42750007590301725,-0.9031086925240638,-0.8186275933193542,-0.8100594700542361,-0.29674423635867503,1.088465691999609,0.6508538365729002,0.8226961493945876,-0.4484174489562754,0.5055393322935005,-0.30286145140621024,-0.4978417225277726,-0.2703969690968379,-0.496898694673202,-1.238791915762404,-0.5716699214856579,0.7380901565286754,0.5067086760286132,1.1057198841369917,-1.102407514137425,-0.14323872584477765,0.6165334141154795,0.10489381754428184,-0.110663890271541,-0.20850149971354087,-0.6762200344118863,0.8859675944529537,0.9588361528613856,0.4328730554162531,0.35949287560473997,0.9394134798963472,-0.7394231575648559,-1.2093159539176361,1.0572848516226316,-0.7466314618483376,-0.948508751240016,-0.19655372917706965,0.7782379203312062,0.055539104834807035,0.7988517783513546,-0.0974002677968792,-0.3504834087941781,0.5814730234297532,0.18313058862047776,1.2127269483373542,1.1148105982810035,0.8735051122240972,-0.35148163691971135,-1.2338366022748213,-0.8907672900725613,1.2254625409378819,-0.6995003696053557,-0.9037585410844167,0.9405119943770692,0.7478444751163855,-1.0073578745812972,0.1817247672932649,-0.5682347157842149,-0.5264733677464449,0.7284138032463667,0.41152909593819337,-0.9879390659941475,-0.45911206872021104,-0.4738680516246734,-1.0864352904192125,1.1604970522189237,-0.6307527422566914,0.7106537214298205,-1.0173143907211408,-0.5044160609710825,0.679188973084454,0.6996881252434147,0.8537095432740401,-0.590954248375771,-0.9680132438668311,-0.29559639410459376,0.9729335345785154,-0.7324863734317484,0.7662449763834068,0.8989494656856246,-0.9163842719457883,-0.4342231620894847,0.09692474882292584],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"class=1<br>x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"1\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.6682065515511877,0.7798065640017775,0.919782578003745,-0.4864887577914054,1.0589279595493206,-0.515853142957767,0.47684639600203965,-0.7419652536185652,0.7250652476431794,0.5944194606481887,0.7397716197863866,-0.4799029639072829,0.1594053270786248,-0.6716293466928432,0.12994188943446588,-0.6562319751371531,0.6642644139735326,0.8639439783538945,-1.042693321559228,-0.45382487700533686,-0.05940794386176523,-0.3146597702113044,0.7023308059446312,-0.7975335369612013,-0.588387424347603,0.11012361413023566,-0.6233105012727311,-1.023815952915638,-0.47160292393545494,-0.7549693804224226,0.060865961611693484,0.31837621915465697,-1.0292894155056125,0.4892291964110407,-0.6284269499555586,-0.5700465335169437,0.945475736856501,-0.033424744714102894,-0.06569185384846907,-0.3227263920340238,0.6212781164197168,0.7521968012444179,-0.6648984706914347,-0.3942518712743026,-0.031302604221810006,0.7140966301041014,-0.32145395856448006,0.21877362409525383,0.5894053426182773,0.614700043855867,0.6853775924374224,-0.8766945879289579,0.6424128728243598,0.3247510495707939,0.971468172367493,-0.2344195845096752,-0.9733593981375851,-0.7186776037210122,-0.09361871981351527,0.5814242835418219,0.4885500311568605,-1.2445470721588132,0.7236519909126249,-0.2593109875008399,-0.6427102679688017,0.38289734815711707,1.156225995088149,0.22411826718745764,0.9074455515740507,0.40204560992335425,-0.8191584189248016,-0.6175249493027649,0.36091623521743543,0.8652211538914177,-0.5969077978995092,-0.3128576057861311,-0.4966885917462033,-0.4493635653431725,-0.6528338095417001,0.6336070444040938,0.39914731570887707,0.10393996819857156,-0.18064185746099404,-0.2590835902076966,0.7307712573739973,0.7576671813374026,-0.4621784356703802,0.025153769859579955,0.27708865006792793,0.6587573529868057,0.8103392446980727,0.2515151584039446,-0.39777292695427224,-0.4621921881039865,-1.127230239335286,-0.6461671559699558,-0.799119695747086,0.6405392476865723,0.5273862855501356,-0.7218389041100988,-0.014778458397984306,0.62056852732417,0.039765148625581656,0.5964050406736506,-0.05389954047336404,0.4090150731128038,0.6650259076883109,0.02838875504243632,0.12741588821822633,-0.4064946243872737,-0.6394626547100454,0.007127667052617526,0.9000480187750635,-0.6060693981156088,0.8840899174636278,-0.9109267529064085,0.3843546806016461,-0.42052351071352406,0.6936761510112113,0.5536640364117342,0.702786750685005,-0.492029891111945,-0.4538820766040694,0.056911290518817576,-0.18804816952665682,-0.2171095048911784,0.084018971436554,0.8651189921910037,0.9305615536659193,-0.6360006320084843,0.21797258610327297,0.8319331843412551,0.7627337082724673,-0.4142114740913039,0.40605528425398685,0.3669809168422101,-0.8138241822907315],\"xaxis\":\"x\",\"y\":[0.23665821913299062,0.10916149052877687,0.25784745593456065,-0.4403012519667298,0.8147274606760992,0.6343367604785797,-0.2830880961679522,-0.25729143863581694,-0.5024790443832752,-0.4014540540906506,-0.15847752826643524,0.45685220163692597,0.9540136445062348,-0.868460360726498,-1.0155265602001173,0.43389313218957054,0.08975140506816032,-0.07368810940766768,0.23412164808978247,-0.5234983788432629,-1.0846716794139504,0.7299375647675446,0.3185638378382139,-0.6436254408200373,-0.01291886619921312,0.5243347450726425,0.8487321397725238,0.21573893961973484,0.7802982982415444,0.18168818830620836,-0.611519717162902,0.9106441327725426,0.5125675222085974,-0.3529879894868664,0.7956478300530422,0.06042381545827369,0.8706781258269684,-0.8342487365070019,0.7249752910173477,-0.7793070463977376,-0.3735149596024997,-0.1493472872482029,0.8431819944955667,0.7324586233238802,-0.5572491784088388,0.558354521175314,-0.4183462394170111,-0.8356387048134185,-0.3790739149415794,0.02299427238425651,0.2196322718543903,0.19241510364326245,-0.2550732498970174,0.6031775773836827,0.09716948113111082,-0.4823956709551526,0.0026227906246077265,-0.27470291060998,-0.9919548233943944,-0.6117997357171062,-0.9674811283557347,-0.8257661993406844,0.33740690421021435,0.8293217640108796,-0.1314356162518304,0.6927433017741892,-0.11880915153756236,-0.8791245875891722,-0.48237209691702093,-0.5238680975967756,-0.1595930577689676,0.5798857238938677,0.7466128947390244,0.5766187732378742,-0.45419814027256844,0.6825308395186448,0.7300762961131533,0.4531040094852471,-0.4695401807825412,0.7755937422105053,0.263427981533333,0.8765011003313609,-1.2065045248125923,-1.0747486663100603,-0.05956951041613448,0.010511192899183952,0.12951660901687675,0.8877146310668388,-0.6610704098787406,0.5571403556717798,-0.35387063317149153,0.5389729007423035,-1.046282950661556,0.733800143982098,0.3967182133773546,0.23063808925901025,-0.06277377866665612,-0.5308230099383751,-0.5672845295038982,0.41837611022944404,-0.7149453121901831,-0.20754255506137873,-0.7019180228748433,0.7200692196796188,-0.4879776622361027,0.7984220104838695,0.22984821711151088,-0.6534681253064482,-0.6719141229748521,-0.5410967995474887,-0.27865965705064827,-0.48544172690291665,0.6793880679092488,0.40091777414459384,-0.07728060564355665,0.5247259651010047,-0.7765825166673054,0.6856640635020527,-0.6488238839255294,-0.682060403934811,-0.45156438491792195,0.6357786232108141,-0.16800062237969376,-0.9002115751352923,-0.5753546017072309,0.6654962375027349,0.6675251438085389,0.21308874223248334,-0.47993824079197317,-0.07936946486624792,0.829680094701145,-0.6263748392157953,-0.031169150901472237,-0.7060110719838969,0.9194212842599563,-0.542127679839415,-0.4275660048620217],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"class\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e749e735-4dbc-4ef4-80c1-33b86cb7b19d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "นิยาม Model"
      ],
      "metadata": {
        "id": "3qIuixJ3grQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(60, input_dim=2, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(30,  activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mdHtdss0grc4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "08Qy1X6lgvI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=500, verbose=1)"
      ],
      "metadata": {
        "id": "J6DGjYHfgvXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c991b698-4117-4096-d045-b235b5b2e650"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "8/8 [==============================] - 1s 29ms/step - loss: 0.6996 - accuracy: 0.4720 - val_loss: 0.6835 - val_accuracy: 0.5400\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6851 - val_accuracy: 0.5240\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6902 - accuracy: 0.5520 - val_loss: 0.6863 - val_accuracy: 0.5480\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6877 - accuracy: 0.5440 - val_loss: 0.6862 - val_accuracy: 0.5760\n",
            "Epoch 5/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6855 - accuracy: 0.6000 - val_loss: 0.6874 - val_accuracy: 0.5720\n",
            "Epoch 6/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6838 - accuracy: 0.6160 - val_loss: 0.6891 - val_accuracy: 0.5120\n",
            "Epoch 7/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6826 - accuracy: 0.6280 - val_loss: 0.6898 - val_accuracy: 0.5120\n",
            "Epoch 8/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6816 - accuracy: 0.6200 - val_loss: 0.6897 - val_accuracy: 0.5280\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6796 - accuracy: 0.6240 - val_loss: 0.6897 - val_accuracy: 0.5280\n",
            "Epoch 10/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6783 - accuracy: 0.6280 - val_loss: 0.6890 - val_accuracy: 0.5520\n",
            "Epoch 11/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6767 - accuracy: 0.6440 - val_loss: 0.6887 - val_accuracy: 0.5440\n",
            "Epoch 12/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6751 - accuracy: 0.6720 - val_loss: 0.6877 - val_accuracy: 0.5440\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6740 - accuracy: 0.6560 - val_loss: 0.6879 - val_accuracy: 0.5600\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6724 - accuracy: 0.6640 - val_loss: 0.6870 - val_accuracy: 0.5760\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6704 - accuracy: 0.6640 - val_loss: 0.6870 - val_accuracy: 0.5560\n",
            "Epoch 16/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6689 - accuracy: 0.6640 - val_loss: 0.6865 - val_accuracy: 0.5640\n",
            "Epoch 17/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6672 - accuracy: 0.6560 - val_loss: 0.6873 - val_accuracy: 0.5760\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6652 - accuracy: 0.6480 - val_loss: 0.6857 - val_accuracy: 0.6080\n",
            "Epoch 19/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6634 - accuracy: 0.6640 - val_loss: 0.6839 - val_accuracy: 0.5920\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6618 - accuracy: 0.6680 - val_loss: 0.6813 - val_accuracy: 0.5760\n",
            "Epoch 21/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6602 - accuracy: 0.6840 - val_loss: 0.6800 - val_accuracy: 0.5840\n",
            "Epoch 22/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6576 - accuracy: 0.6800 - val_loss: 0.6827 - val_accuracy: 0.6000\n",
            "Epoch 23/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6560 - accuracy: 0.6600 - val_loss: 0.6840 - val_accuracy: 0.5960\n",
            "Epoch 24/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6542 - accuracy: 0.6680 - val_loss: 0.6824 - val_accuracy: 0.6000\n",
            "Epoch 25/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6521 - accuracy: 0.6640 - val_loss: 0.6785 - val_accuracy: 0.6040\n",
            "Epoch 26/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6500 - accuracy: 0.6800 - val_loss: 0.6788 - val_accuracy: 0.6040\n",
            "Epoch 27/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6483 - accuracy: 0.6720 - val_loss: 0.6763 - val_accuracy: 0.6040\n",
            "Epoch 28/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6458 - accuracy: 0.6760 - val_loss: 0.6770 - val_accuracy: 0.6080\n",
            "Epoch 29/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6448 - accuracy: 0.6680 - val_loss: 0.6779 - val_accuracy: 0.5960\n",
            "Epoch 30/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6424 - accuracy: 0.6640 - val_loss: 0.6710 - val_accuracy: 0.6040\n",
            "Epoch 31/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6397 - accuracy: 0.6680 - val_loss: 0.6685 - val_accuracy: 0.6120\n",
            "Epoch 32/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6372 - accuracy: 0.6840 - val_loss: 0.6713 - val_accuracy: 0.6080\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6352 - accuracy: 0.6840 - val_loss: 0.6726 - val_accuracy: 0.6000\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6327 - accuracy: 0.6920 - val_loss: 0.6690 - val_accuracy: 0.6040\n",
            "Epoch 35/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6311 - accuracy: 0.6880 - val_loss: 0.6704 - val_accuracy: 0.6040\n",
            "Epoch 36/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6277 - accuracy: 0.6800 - val_loss: 0.6662 - val_accuracy: 0.6200\n",
            "Epoch 37/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6265 - accuracy: 0.6840 - val_loss: 0.6641 - val_accuracy: 0.6360\n",
            "Epoch 38/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6240 - accuracy: 0.6920 - val_loss: 0.6616 - val_accuracy: 0.6400\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6212 - accuracy: 0.6960 - val_loss: 0.6607 - val_accuracy: 0.6400\n",
            "Epoch 40/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6191 - accuracy: 0.7000 - val_loss: 0.6612 - val_accuracy: 0.6160\n",
            "Epoch 41/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6181 - accuracy: 0.6760 - val_loss: 0.6609 - val_accuracy: 0.6160\n",
            "Epoch 42/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6157 - accuracy: 0.6840 - val_loss: 0.6582 - val_accuracy: 0.6360\n",
            "Epoch 43/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6119 - accuracy: 0.6960 - val_loss: 0.6561 - val_accuracy: 0.6320\n",
            "Epoch 44/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6142 - accuracy: 0.7080 - val_loss: 0.6509 - val_accuracy: 0.6120\n",
            "Epoch 45/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 0.7080 - val_loss: 0.6585 - val_accuracy: 0.6280\n",
            "Epoch 46/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6076 - accuracy: 0.6760 - val_loss: 0.6572 - val_accuracy: 0.6320\n",
            "Epoch 47/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6046 - accuracy: 0.6800 - val_loss: 0.6534 - val_accuracy: 0.6320\n",
            "Epoch 48/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6025 - accuracy: 0.7080 - val_loss: 0.6517 - val_accuracy: 0.6360\n",
            "Epoch 49/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6007 - accuracy: 0.7200 - val_loss: 0.6503 - val_accuracy: 0.6360\n",
            "Epoch 50/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5985 - accuracy: 0.7040 - val_loss: 0.6489 - val_accuracy: 0.6280\n",
            "Epoch 51/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5969 - accuracy: 0.6960 - val_loss: 0.6502 - val_accuracy: 0.6160\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5959 - accuracy: 0.6880 - val_loss: 0.6501 - val_accuracy: 0.6200\n",
            "Epoch 53/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5943 - accuracy: 0.6960 - val_loss: 0.6450 - val_accuracy: 0.6200\n",
            "Epoch 54/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5931 - accuracy: 0.7160 - val_loss: 0.6460 - val_accuracy: 0.6280\n",
            "Epoch 55/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5925 - accuracy: 0.7040 - val_loss: 0.6434 - val_accuracy: 0.6280\n",
            "Epoch 56/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5902 - accuracy: 0.7080 - val_loss: 0.6467 - val_accuracy: 0.6400\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5886 - accuracy: 0.7080 - val_loss: 0.6465 - val_accuracy: 0.6360\n",
            "Epoch 58/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5883 - accuracy: 0.6840 - val_loss: 0.6484 - val_accuracy: 0.6440\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5868 - accuracy: 0.6960 - val_loss: 0.6449 - val_accuracy: 0.6360\n",
            "Epoch 60/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5865 - accuracy: 0.6920 - val_loss: 0.6405 - val_accuracy: 0.6320\n",
            "Epoch 61/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5839 - accuracy: 0.7160 - val_loss: 0.6422 - val_accuracy: 0.6240\n",
            "Epoch 62/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5832 - accuracy: 0.7120 - val_loss: 0.6423 - val_accuracy: 0.6240\n",
            "Epoch 63/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5831 - accuracy: 0.7280 - val_loss: 0.6399 - val_accuracy: 0.6320\n",
            "Epoch 64/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5815 - accuracy: 0.7080 - val_loss: 0.6466 - val_accuracy: 0.6360\n",
            "Epoch 65/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5806 - accuracy: 0.6920 - val_loss: 0.6436 - val_accuracy: 0.6360\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5803 - accuracy: 0.7000 - val_loss: 0.6422 - val_accuracy: 0.6280\n",
            "Epoch 67/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5790 - accuracy: 0.7160 - val_loss: 0.6413 - val_accuracy: 0.6320\n",
            "Epoch 68/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5787 - accuracy: 0.7160 - val_loss: 0.6398 - val_accuracy: 0.6280\n",
            "Epoch 69/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5836 - accuracy: 0.7000 - val_loss: 0.6495 - val_accuracy: 0.6400\n",
            "Epoch 70/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5765 - accuracy: 0.7080 - val_loss: 0.6405 - val_accuracy: 0.6240\n",
            "Epoch 71/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5766 - accuracy: 0.7280 - val_loss: 0.6380 - val_accuracy: 0.6320\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5769 - accuracy: 0.7200 - val_loss: 0.6435 - val_accuracy: 0.6360\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5749 - accuracy: 0.7000 - val_loss: 0.6445 - val_accuracy: 0.6480\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5756 - accuracy: 0.7240 - val_loss: 0.6408 - val_accuracy: 0.6240\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5746 - accuracy: 0.7120 - val_loss: 0.6433 - val_accuracy: 0.6320\n",
            "Epoch 76/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5735 - accuracy: 0.7000 - val_loss: 0.6465 - val_accuracy: 0.6360\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5738 - accuracy: 0.6960 - val_loss: 0.6432 - val_accuracy: 0.6320\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5747 - accuracy: 0.6960 - val_loss: 0.6452 - val_accuracy: 0.6360\n",
            "Epoch 79/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5718 - accuracy: 0.7080 - val_loss: 0.6435 - val_accuracy: 0.6280\n",
            "Epoch 80/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5722 - accuracy: 0.7160 - val_loss: 0.6462 - val_accuracy: 0.6440\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5716 - accuracy: 0.7080 - val_loss: 0.6439 - val_accuracy: 0.6240\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5714 - accuracy: 0.7200 - val_loss: 0.6408 - val_accuracy: 0.6240\n",
            "Epoch 83/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5710 - accuracy: 0.7200 - val_loss: 0.6426 - val_accuracy: 0.6240\n",
            "Epoch 84/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5738 - accuracy: 0.7120 - val_loss: 0.6488 - val_accuracy: 0.6360\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5722 - accuracy: 0.7120 - val_loss: 0.6415 - val_accuracy: 0.6240\n",
            "Epoch 86/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5706 - accuracy: 0.7160 - val_loss: 0.6435 - val_accuracy: 0.6360\n",
            "Epoch 87/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5707 - accuracy: 0.7160 - val_loss: 0.6452 - val_accuracy: 0.6320\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5703 - accuracy: 0.7160 - val_loss: 0.6452 - val_accuracy: 0.6400\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5710 - accuracy: 0.7160 - val_loss: 0.6470 - val_accuracy: 0.6360\n",
            "Epoch 90/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5715 - accuracy: 0.7120 - val_loss: 0.6517 - val_accuracy: 0.6480\n",
            "Epoch 91/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5691 - accuracy: 0.7080 - val_loss: 0.6434 - val_accuracy: 0.6280\n",
            "Epoch 92/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5704 - accuracy: 0.7280 - val_loss: 0.6431 - val_accuracy: 0.6240\n",
            "Epoch 93/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5683 - accuracy: 0.7280 - val_loss: 0.6463 - val_accuracy: 0.6280\n",
            "Epoch 94/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5679 - accuracy: 0.7120 - val_loss: 0.6519 - val_accuracy: 0.6440\n",
            "Epoch 95/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5695 - accuracy: 0.7000 - val_loss: 0.6466 - val_accuracy: 0.6240\n",
            "Epoch 96/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5683 - accuracy: 0.7160 - val_loss: 0.6469 - val_accuracy: 0.6320\n",
            "Epoch 97/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5682 - accuracy: 0.7240 - val_loss: 0.6474 - val_accuracy: 0.6280\n",
            "Epoch 98/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5752 - accuracy: 0.6960 - val_loss: 0.6569 - val_accuracy: 0.6360\n",
            "Epoch 99/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5689 - accuracy: 0.7120 - val_loss: 0.6428 - val_accuracy: 0.6360\n",
            "Epoch 100/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5681 - accuracy: 0.7360 - val_loss: 0.6449 - val_accuracy: 0.6280\n",
            "Epoch 101/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5700 - accuracy: 0.7200 - val_loss: 0.6464 - val_accuracy: 0.6200\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5677 - accuracy: 0.7080 - val_loss: 0.6527 - val_accuracy: 0.6480\n",
            "Epoch 103/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5696 - accuracy: 0.7080 - val_loss: 0.6528 - val_accuracy: 0.6320\n",
            "Epoch 104/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5665 - accuracy: 0.7160 - val_loss: 0.6492 - val_accuracy: 0.6320\n",
            "Epoch 105/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5679 - accuracy: 0.7160 - val_loss: 0.6439 - val_accuracy: 0.6320\n",
            "Epoch 106/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5672 - accuracy: 0.7240 - val_loss: 0.6470 - val_accuracy: 0.6400\n",
            "Epoch 107/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5680 - accuracy: 0.7120 - val_loss: 0.6524 - val_accuracy: 0.6320\n",
            "Epoch 108/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5660 - accuracy: 0.7120 - val_loss: 0.6503 - val_accuracy: 0.6320\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5666 - accuracy: 0.7080 - val_loss: 0.6504 - val_accuracy: 0.6400\n",
            "Epoch 110/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5658 - accuracy: 0.7080 - val_loss: 0.6497 - val_accuracy: 0.6320\n",
            "Epoch 111/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5659 - accuracy: 0.7080 - val_loss: 0.6498 - val_accuracy: 0.6360\n",
            "Epoch 112/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5680 - accuracy: 0.7280 - val_loss: 0.6461 - val_accuracy: 0.6280\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5678 - accuracy: 0.7360 - val_loss: 0.6504 - val_accuracy: 0.6280\n",
            "Epoch 114/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5654 - accuracy: 0.7160 - val_loss: 0.6512 - val_accuracy: 0.6280\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5669 - accuracy: 0.7080 - val_loss: 0.6539 - val_accuracy: 0.6400\n",
            "Epoch 116/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5667 - accuracy: 0.7040 - val_loss: 0.6504 - val_accuracy: 0.6400\n",
            "Epoch 117/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5667 - accuracy: 0.7240 - val_loss: 0.6456 - val_accuracy: 0.6280\n",
            "Epoch 118/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5660 - accuracy: 0.7360 - val_loss: 0.6502 - val_accuracy: 0.6240\n",
            "Epoch 119/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5664 - accuracy: 0.7000 - val_loss: 0.6568 - val_accuracy: 0.6440\n",
            "Epoch 120/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5669 - accuracy: 0.7080 - val_loss: 0.6497 - val_accuracy: 0.6240\n",
            "Epoch 121/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5648 - accuracy: 0.7120 - val_loss: 0.6518 - val_accuracy: 0.6360\n",
            "Epoch 122/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5665 - accuracy: 0.6960 - val_loss: 0.6526 - val_accuracy: 0.6320\n",
            "Epoch 123/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7120 - val_loss: 0.6507 - val_accuracy: 0.6280\n",
            "Epoch 124/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5646 - accuracy: 0.7280 - val_loss: 0.6468 - val_accuracy: 0.6320\n",
            "Epoch 125/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5640 - accuracy: 0.7400 - val_loss: 0.6506 - val_accuracy: 0.6320\n",
            "Epoch 126/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5671 - accuracy: 0.7000 - val_loss: 0.6526 - val_accuracy: 0.6480\n",
            "Epoch 127/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5644 - accuracy: 0.7080 - val_loss: 0.6500 - val_accuracy: 0.6320\n",
            "Epoch 128/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5646 - accuracy: 0.7160 - val_loss: 0.6494 - val_accuracy: 0.6200\n",
            "Epoch 129/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5639 - accuracy: 0.7160 - val_loss: 0.6548 - val_accuracy: 0.6320\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7080 - val_loss: 0.6530 - val_accuracy: 0.6440\n",
            "Epoch 131/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5652 - accuracy: 0.7040 - val_loss: 0.6529 - val_accuracy: 0.6440\n",
            "Epoch 132/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5645 - accuracy: 0.7120 - val_loss: 0.6520 - val_accuracy: 0.6240\n",
            "Epoch 133/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5635 - accuracy: 0.7200 - val_loss: 0.6519 - val_accuracy: 0.6240\n",
            "Epoch 134/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5642 - accuracy: 0.7280 - val_loss: 0.6499 - val_accuracy: 0.6280\n",
            "Epoch 135/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5639 - accuracy: 0.7200 - val_loss: 0.6572 - val_accuracy: 0.6400\n",
            "Epoch 136/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5674 - accuracy: 0.7120 - val_loss: 0.6497 - val_accuracy: 0.6280\n",
            "Epoch 137/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5624 - accuracy: 0.7320 - val_loss: 0.6533 - val_accuracy: 0.6360\n",
            "Epoch 138/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5632 - accuracy: 0.7040 - val_loss: 0.6579 - val_accuracy: 0.6440\n",
            "Epoch 139/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5639 - accuracy: 0.7080 - val_loss: 0.6506 - val_accuracy: 0.6320\n",
            "Epoch 140/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5629 - accuracy: 0.7200 - val_loss: 0.6522 - val_accuracy: 0.6360\n",
            "Epoch 141/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5632 - accuracy: 0.7080 - val_loss: 0.6564 - val_accuracy: 0.6480\n",
            "Epoch 142/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5637 - accuracy: 0.7040 - val_loss: 0.6549 - val_accuracy: 0.6280\n",
            "Epoch 143/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5627 - accuracy: 0.7080 - val_loss: 0.6526 - val_accuracy: 0.6320\n",
            "Epoch 144/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5629 - accuracy: 0.7120 - val_loss: 0.6518 - val_accuracy: 0.6360\n",
            "Epoch 145/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5636 - accuracy: 0.7400 - val_loss: 0.6500 - val_accuracy: 0.6320\n",
            "Epoch 146/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5623 - accuracy: 0.7240 - val_loss: 0.6533 - val_accuracy: 0.6280\n",
            "Epoch 147/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5624 - accuracy: 0.7080 - val_loss: 0.6549 - val_accuracy: 0.6320\n",
            "Epoch 148/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5622 - accuracy: 0.7120 - val_loss: 0.6564 - val_accuracy: 0.6320\n",
            "Epoch 149/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5622 - accuracy: 0.7120 - val_loss: 0.6547 - val_accuracy: 0.6320\n",
            "Epoch 150/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5650 - accuracy: 0.7120 - val_loss: 0.6559 - val_accuracy: 0.6480\n",
            "Epoch 151/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5621 - accuracy: 0.7160 - val_loss: 0.6535 - val_accuracy: 0.6280\n",
            "Epoch 152/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5630 - accuracy: 0.7200 - val_loss: 0.6543 - val_accuracy: 0.6280\n",
            "Epoch 153/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5616 - accuracy: 0.7080 - val_loss: 0.6533 - val_accuracy: 0.6240\n",
            "Epoch 154/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5617 - accuracy: 0.7280 - val_loss: 0.6520 - val_accuracy: 0.6240\n",
            "Epoch 155/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5633 - accuracy: 0.7120 - val_loss: 0.6546 - val_accuracy: 0.6440\n",
            "Epoch 156/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5631 - accuracy: 0.7120 - val_loss: 0.6565 - val_accuracy: 0.6320\n",
            "Epoch 157/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5620 - accuracy: 0.7160 - val_loss: 0.6522 - val_accuracy: 0.6280\n",
            "Epoch 158/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5624 - accuracy: 0.7200 - val_loss: 0.6525 - val_accuracy: 0.6320\n",
            "Epoch 159/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5657 - accuracy: 0.7400 - val_loss: 0.6504 - val_accuracy: 0.6280\n",
            "Epoch 160/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5643 - accuracy: 0.7080 - val_loss: 0.6639 - val_accuracy: 0.6400\n",
            "Epoch 161/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5635 - accuracy: 0.7160 - val_loss: 0.6601 - val_accuracy: 0.6400\n",
            "Epoch 162/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5613 - accuracy: 0.7160 - val_loss: 0.6531 - val_accuracy: 0.6320\n",
            "Epoch 163/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5625 - accuracy: 0.7320 - val_loss: 0.6515 - val_accuracy: 0.6320\n",
            "Epoch 164/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5607 - accuracy: 0.7280 - val_loss: 0.6550 - val_accuracy: 0.6400\n",
            "Epoch 165/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5621 - accuracy: 0.7120 - val_loss: 0.6602 - val_accuracy: 0.6440\n",
            "Epoch 166/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5643 - accuracy: 0.7160 - val_loss: 0.6524 - val_accuracy: 0.6320\n",
            "Epoch 167/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5626 - accuracy: 0.7080 - val_loss: 0.6577 - val_accuracy: 0.6360\n",
            "Epoch 168/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5626 - accuracy: 0.7040 - val_loss: 0.6593 - val_accuracy: 0.6400\n",
            "Epoch 169/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5634 - accuracy: 0.7120 - val_loss: 0.6536 - val_accuracy: 0.6240\n",
            "Epoch 170/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5612 - accuracy: 0.7240 - val_loss: 0.6576 - val_accuracy: 0.6280\n",
            "Epoch 171/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5616 - accuracy: 0.7120 - val_loss: 0.6594 - val_accuracy: 0.6440\n",
            "Epoch 172/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5604 - accuracy: 0.7200 - val_loss: 0.6539 - val_accuracy: 0.6360\n",
            "Epoch 173/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5619 - accuracy: 0.7400 - val_loss: 0.6547 - val_accuracy: 0.6240\n",
            "Epoch 174/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7200 - val_loss: 0.6594 - val_accuracy: 0.6400\n",
            "Epoch 175/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5613 - accuracy: 0.7080 - val_loss: 0.6599 - val_accuracy: 0.6360\n",
            "Epoch 176/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5622 - accuracy: 0.7160 - val_loss: 0.6580 - val_accuracy: 0.6480\n",
            "Epoch 177/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5635 - accuracy: 0.7320 - val_loss: 0.6498 - val_accuracy: 0.6360\n",
            "Epoch 178/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5625 - accuracy: 0.7280 - val_loss: 0.6588 - val_accuracy: 0.6320\n",
            "Epoch 179/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5629 - accuracy: 0.7040 - val_loss: 0.6639 - val_accuracy: 0.6360\n",
            "Epoch 180/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5598 - accuracy: 0.7040 - val_loss: 0.6584 - val_accuracy: 0.6480\n",
            "Epoch 181/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5594 - accuracy: 0.7200 - val_loss: 0.6546 - val_accuracy: 0.6360\n",
            "Epoch 182/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5611 - accuracy: 0.7320 - val_loss: 0.6566 - val_accuracy: 0.6240\n",
            "Epoch 183/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5605 - accuracy: 0.7320 - val_loss: 0.6556 - val_accuracy: 0.6200\n",
            "Epoch 184/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5595 - accuracy: 0.7080 - val_loss: 0.6615 - val_accuracy: 0.6440\n",
            "Epoch 185/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5604 - accuracy: 0.7080 - val_loss: 0.6613 - val_accuracy: 0.6480\n",
            "Epoch 186/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5615 - accuracy: 0.7160 - val_loss: 0.6592 - val_accuracy: 0.6320\n",
            "Epoch 187/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7520 - val_loss: 0.6529 - val_accuracy: 0.6320\n",
            "Epoch 188/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5596 - accuracy: 0.7360 - val_loss: 0.6586 - val_accuracy: 0.6320\n",
            "Epoch 189/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5602 - accuracy: 0.7200 - val_loss: 0.6657 - val_accuracy: 0.6360\n",
            "Epoch 190/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5602 - accuracy: 0.7200 - val_loss: 0.6582 - val_accuracy: 0.6280\n",
            "Epoch 191/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5596 - accuracy: 0.7280 - val_loss: 0.6574 - val_accuracy: 0.6320\n",
            "Epoch 192/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5590 - accuracy: 0.7320 - val_loss: 0.6581 - val_accuracy: 0.6240\n",
            "Epoch 193/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5589 - accuracy: 0.7400 - val_loss: 0.6605 - val_accuracy: 0.6320\n",
            "Epoch 194/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5662 - accuracy: 0.6920 - val_loss: 0.6710 - val_accuracy: 0.6320\n",
            "Epoch 195/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5605 - accuracy: 0.7000 - val_loss: 0.6537 - val_accuracy: 0.6280\n",
            "Epoch 196/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5600 - accuracy: 0.7440 - val_loss: 0.6561 - val_accuracy: 0.6200\n",
            "Epoch 197/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5597 - accuracy: 0.7280 - val_loss: 0.6597 - val_accuracy: 0.6280\n",
            "Epoch 198/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5593 - accuracy: 0.7160 - val_loss: 0.6614 - val_accuracy: 0.6280\n",
            "Epoch 199/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5592 - accuracy: 0.7200 - val_loss: 0.6595 - val_accuracy: 0.6360\n",
            "Epoch 200/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5594 - accuracy: 0.7360 - val_loss: 0.6562 - val_accuracy: 0.6400\n",
            "Epoch 201/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5606 - accuracy: 0.7320 - val_loss: 0.6629 - val_accuracy: 0.6480\n",
            "Epoch 202/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5593 - accuracy: 0.7320 - val_loss: 0.6566 - val_accuracy: 0.6360\n",
            "Epoch 203/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5596 - accuracy: 0.7360 - val_loss: 0.6591 - val_accuracy: 0.6280\n",
            "Epoch 204/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5585 - accuracy: 0.7240 - val_loss: 0.6604 - val_accuracy: 0.6400\n",
            "Epoch 205/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5594 - accuracy: 0.7200 - val_loss: 0.6609 - val_accuracy: 0.6400\n",
            "Epoch 206/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5586 - accuracy: 0.7400 - val_loss: 0.6566 - val_accuracy: 0.6280\n",
            "Epoch 207/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5592 - accuracy: 0.7360 - val_loss: 0.6581 - val_accuracy: 0.6240\n",
            "Epoch 208/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5601 - accuracy: 0.7200 - val_loss: 0.6643 - val_accuracy: 0.6400\n",
            "Epoch 209/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5624 - accuracy: 0.7040 - val_loss: 0.6651 - val_accuracy: 0.6400\n",
            "Epoch 210/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5597 - accuracy: 0.7200 - val_loss: 0.6546 - val_accuracy: 0.6240\n",
            "Epoch 211/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5600 - accuracy: 0.7560 - val_loss: 0.6553 - val_accuracy: 0.6320\n",
            "Epoch 212/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5584 - accuracy: 0.7280 - val_loss: 0.6614 - val_accuracy: 0.6440\n",
            "Epoch 213/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5608 - accuracy: 0.7160 - val_loss: 0.6663 - val_accuracy: 0.6440\n",
            "Epoch 214/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5596 - accuracy: 0.7160 - val_loss: 0.6582 - val_accuracy: 0.6320\n",
            "Epoch 215/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5583 - accuracy: 0.7360 - val_loss: 0.6573 - val_accuracy: 0.6320\n",
            "Epoch 216/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5599 - accuracy: 0.7240 - val_loss: 0.6637 - val_accuracy: 0.6360\n",
            "Epoch 217/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5580 - accuracy: 0.7200 - val_loss: 0.6600 - val_accuracy: 0.6360\n",
            "Epoch 218/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5603 - accuracy: 0.7240 - val_loss: 0.6587 - val_accuracy: 0.6280\n",
            "Epoch 219/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5590 - accuracy: 0.7280 - val_loss: 0.6608 - val_accuracy: 0.6280\n",
            "Epoch 220/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5576 - accuracy: 0.7120 - val_loss: 0.6634 - val_accuracy: 0.6360\n",
            "Epoch 221/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5608 - accuracy: 0.7080 - val_loss: 0.6581 - val_accuracy: 0.6360\n",
            "Epoch 222/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5585 - accuracy: 0.7280 - val_loss: 0.6608 - val_accuracy: 0.6480\n",
            "Epoch 223/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5602 - accuracy: 0.7200 - val_loss: 0.6671 - val_accuracy: 0.6440\n",
            "Epoch 224/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5594 - accuracy: 0.7080 - val_loss: 0.6609 - val_accuracy: 0.6320\n",
            "Epoch 225/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5578 - accuracy: 0.7320 - val_loss: 0.6564 - val_accuracy: 0.6280\n",
            "Epoch 226/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5587 - accuracy: 0.7320 - val_loss: 0.6593 - val_accuracy: 0.6400\n",
            "Epoch 227/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5601 - accuracy: 0.7240 - val_loss: 0.6698 - val_accuracy: 0.6360\n",
            "Epoch 228/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5583 - accuracy: 0.7200 - val_loss: 0.6606 - val_accuracy: 0.6320\n",
            "Epoch 229/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5587 - accuracy: 0.7480 - val_loss: 0.6586 - val_accuracy: 0.6280\n",
            "Epoch 230/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5583 - accuracy: 0.7280 - val_loss: 0.6601 - val_accuracy: 0.6240\n",
            "Epoch 231/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5577 - accuracy: 0.7080 - val_loss: 0.6644 - val_accuracy: 0.6400\n",
            "Epoch 232/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5585 - accuracy: 0.7080 - val_loss: 0.6640 - val_accuracy: 0.6400\n",
            "Epoch 233/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5619 - accuracy: 0.7280 - val_loss: 0.6562 - val_accuracy: 0.6280\n",
            "Epoch 234/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5579 - accuracy: 0.7320 - val_loss: 0.6645 - val_accuracy: 0.6400\n",
            "Epoch 235/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5589 - accuracy: 0.7280 - val_loss: 0.6636 - val_accuracy: 0.6320\n",
            "Epoch 236/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5573 - accuracy: 0.7240 - val_loss: 0.6626 - val_accuracy: 0.6360\n",
            "Epoch 237/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5568 - accuracy: 0.7200 - val_loss: 0.6661 - val_accuracy: 0.6360\n",
            "Epoch 238/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5585 - accuracy: 0.7200 - val_loss: 0.6648 - val_accuracy: 0.6440\n",
            "Epoch 239/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5612 - accuracy: 0.7280 - val_loss: 0.6588 - val_accuracy: 0.6320\n",
            "Epoch 240/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5586 - accuracy: 0.7240 - val_loss: 0.6650 - val_accuracy: 0.6440\n",
            "Epoch 241/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5572 - accuracy: 0.7160 - val_loss: 0.6646 - val_accuracy: 0.6360\n",
            "Epoch 242/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5569 - accuracy: 0.7240 - val_loss: 0.6605 - val_accuracy: 0.6280\n",
            "Epoch 243/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5596 - accuracy: 0.7080 - val_loss: 0.6645 - val_accuracy: 0.6400\n",
            "Epoch 244/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5586 - accuracy: 0.7400 - val_loss: 0.6578 - val_accuracy: 0.6320\n",
            "Epoch 245/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5574 - accuracy: 0.7360 - val_loss: 0.6640 - val_accuracy: 0.6440\n",
            "Epoch 246/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5571 - accuracy: 0.7200 - val_loss: 0.6649 - val_accuracy: 0.6400\n",
            "Epoch 247/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5565 - accuracy: 0.7200 - val_loss: 0.6651 - val_accuracy: 0.6480\n",
            "Epoch 248/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5577 - accuracy: 0.7200 - val_loss: 0.6618 - val_accuracy: 0.6280\n",
            "Epoch 249/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5598 - accuracy: 0.7160 - val_loss: 0.6624 - val_accuracy: 0.6320\n",
            "Epoch 250/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5609 - accuracy: 0.7320 - val_loss: 0.6576 - val_accuracy: 0.6280\n",
            "Epoch 251/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5581 - accuracy: 0.7240 - val_loss: 0.6698 - val_accuracy: 0.6400\n",
            "Epoch 252/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5573 - accuracy: 0.7080 - val_loss: 0.6689 - val_accuracy: 0.6360\n",
            "Epoch 253/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5568 - accuracy: 0.7160 - val_loss: 0.6607 - val_accuracy: 0.6280\n",
            "Epoch 254/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5575 - accuracy: 0.7440 - val_loss: 0.6624 - val_accuracy: 0.6440\n",
            "Epoch 255/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5575 - accuracy: 0.7240 - val_loss: 0.6645 - val_accuracy: 0.6360\n",
            "Epoch 256/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5580 - accuracy: 0.7560 - val_loss: 0.6583 - val_accuracy: 0.6280\n",
            "Epoch 257/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5595 - accuracy: 0.7240 - val_loss: 0.6665 - val_accuracy: 0.6440\n",
            "Epoch 258/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5566 - accuracy: 0.7240 - val_loss: 0.6647 - val_accuracy: 0.6400\n",
            "Epoch 259/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5575 - accuracy: 0.7120 - val_loss: 0.6636 - val_accuracy: 0.6320\n",
            "Epoch 260/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5576 - accuracy: 0.7080 - val_loss: 0.6656 - val_accuracy: 0.6440\n",
            "Epoch 261/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5574 - accuracy: 0.7120 - val_loss: 0.6660 - val_accuracy: 0.6400\n",
            "Epoch 262/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5558 - accuracy: 0.7280 - val_loss: 0.6593 - val_accuracy: 0.6320\n",
            "Epoch 263/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5576 - accuracy: 0.7440 - val_loss: 0.6612 - val_accuracy: 0.6320\n",
            "Epoch 264/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5573 - accuracy: 0.7320 - val_loss: 0.6647 - val_accuracy: 0.6440\n",
            "Epoch 265/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5585 - accuracy: 0.7040 - val_loss: 0.6719 - val_accuracy: 0.6320\n",
            "Epoch 266/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5565 - accuracy: 0.7160 - val_loss: 0.6607 - val_accuracy: 0.6280\n",
            "Epoch 267/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5563 - accuracy: 0.7360 - val_loss: 0.6609 - val_accuracy: 0.6280\n",
            "Epoch 268/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5557 - accuracy: 0.7160 - val_loss: 0.6649 - val_accuracy: 0.6400\n",
            "Epoch 269/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5560 - accuracy: 0.7160 - val_loss: 0.6656 - val_accuracy: 0.6440\n",
            "Epoch 270/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5557 - accuracy: 0.7360 - val_loss: 0.6618 - val_accuracy: 0.6280\n",
            "Epoch 271/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5559 - accuracy: 0.7320 - val_loss: 0.6660 - val_accuracy: 0.6320\n",
            "Epoch 272/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5567 - accuracy: 0.7080 - val_loss: 0.6660 - val_accuracy: 0.6400\n",
            "Epoch 273/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5598 - accuracy: 0.7200 - val_loss: 0.6561 - val_accuracy: 0.6400\n",
            "Epoch 274/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5561 - accuracy: 0.7480 - val_loss: 0.6638 - val_accuracy: 0.6400\n",
            "Epoch 275/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5583 - accuracy: 0.7120 - val_loss: 0.6703 - val_accuracy: 0.6360\n",
            "Epoch 276/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5563 - accuracy: 0.7160 - val_loss: 0.6646 - val_accuracy: 0.6280\n",
            "Epoch 277/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5565 - accuracy: 0.7200 - val_loss: 0.6606 - val_accuracy: 0.6360\n",
            "Epoch 278/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5573 - accuracy: 0.7440 - val_loss: 0.6577 - val_accuracy: 0.6240\n",
            "Epoch 279/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5555 - accuracy: 0.7440 - val_loss: 0.6638 - val_accuracy: 0.6440\n",
            "Epoch 280/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5565 - accuracy: 0.7160 - val_loss: 0.6683 - val_accuracy: 0.6360\n",
            "Epoch 281/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5572 - accuracy: 0.7200 - val_loss: 0.6679 - val_accuracy: 0.6320\n",
            "Epoch 282/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5560 - accuracy: 0.7200 - val_loss: 0.6632 - val_accuracy: 0.6440\n",
            "Epoch 283/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5568 - accuracy: 0.7240 - val_loss: 0.6646 - val_accuracy: 0.6400\n",
            "Epoch 284/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5556 - accuracy: 0.7280 - val_loss: 0.6621 - val_accuracy: 0.6360\n",
            "Epoch 285/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5549 - accuracy: 0.7440 - val_loss: 0.6595 - val_accuracy: 0.6200\n",
            "Epoch 286/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5553 - accuracy: 0.7400 - val_loss: 0.6636 - val_accuracy: 0.6440\n",
            "Epoch 287/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5573 - accuracy: 0.7160 - val_loss: 0.6646 - val_accuracy: 0.6280\n",
            "Epoch 288/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5578 - accuracy: 0.7320 - val_loss: 0.6659 - val_accuracy: 0.6320\n",
            "Epoch 289/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5557 - accuracy: 0.7280 - val_loss: 0.6643 - val_accuracy: 0.6280\n",
            "Epoch 290/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5583 - accuracy: 0.7280 - val_loss: 0.6609 - val_accuracy: 0.6320\n",
            "Epoch 291/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5553 - accuracy: 0.7440 - val_loss: 0.6654 - val_accuracy: 0.6400\n",
            "Epoch 292/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5568 - accuracy: 0.7120 - val_loss: 0.6713 - val_accuracy: 0.6360\n",
            "Epoch 293/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5626 - accuracy: 0.7240 - val_loss: 0.6576 - val_accuracy: 0.6360\n",
            "Epoch 294/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5545 - accuracy: 0.7280 - val_loss: 0.6661 - val_accuracy: 0.6440\n",
            "Epoch 295/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5550 - accuracy: 0.7200 - val_loss: 0.6682 - val_accuracy: 0.6400\n",
            "Epoch 296/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5544 - accuracy: 0.7160 - val_loss: 0.6665 - val_accuracy: 0.6360\n",
            "Epoch 297/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5541 - accuracy: 0.7320 - val_loss: 0.6650 - val_accuracy: 0.6360\n",
            "Epoch 298/500\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5584 - accuracy: 0.7360 - val_loss: 0.6611 - val_accuracy: 0.6280\n",
            "Epoch 299/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5564 - accuracy: 0.7320 - val_loss: 0.6675 - val_accuracy: 0.6320\n",
            "Epoch 300/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5539 - accuracy: 0.7160 - val_loss: 0.6689 - val_accuracy: 0.6320\n",
            "Epoch 301/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5551 - accuracy: 0.7320 - val_loss: 0.6664 - val_accuracy: 0.6400\n",
            "Epoch 302/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5552 - accuracy: 0.7320 - val_loss: 0.6615 - val_accuracy: 0.6240\n",
            "Epoch 303/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5544 - accuracy: 0.7400 - val_loss: 0.6659 - val_accuracy: 0.6440\n",
            "Epoch 304/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5546 - accuracy: 0.7200 - val_loss: 0.6700 - val_accuracy: 0.6360\n",
            "Epoch 305/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5583 - accuracy: 0.7400 - val_loss: 0.6612 - val_accuracy: 0.6280\n",
            "Epoch 306/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5548 - accuracy: 0.7320 - val_loss: 0.6649 - val_accuracy: 0.6400\n",
            "Epoch 307/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5560 - accuracy: 0.7120 - val_loss: 0.6694 - val_accuracy: 0.6360\n",
            "Epoch 308/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5546 - accuracy: 0.7120 - val_loss: 0.6671 - val_accuracy: 0.6360\n",
            "Epoch 309/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5587 - accuracy: 0.7360 - val_loss: 0.6594 - val_accuracy: 0.6320\n",
            "Epoch 310/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5540 - accuracy: 0.7360 - val_loss: 0.6678 - val_accuracy: 0.6400\n",
            "Epoch 311/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5539 - accuracy: 0.7200 - val_loss: 0.6683 - val_accuracy: 0.6440\n",
            "Epoch 312/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5541 - accuracy: 0.7280 - val_loss: 0.6656 - val_accuracy: 0.6440\n",
            "Epoch 313/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5541 - accuracy: 0.7320 - val_loss: 0.6674 - val_accuracy: 0.6400\n",
            "Epoch 314/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5552 - accuracy: 0.7040 - val_loss: 0.6669 - val_accuracy: 0.6400\n",
            "Epoch 315/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5548 - accuracy: 0.7400 - val_loss: 0.6611 - val_accuracy: 0.6240\n",
            "Epoch 316/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5546 - accuracy: 0.7480 - val_loss: 0.6642 - val_accuracy: 0.6320\n",
            "Epoch 317/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5544 - accuracy: 0.7440 - val_loss: 0.6632 - val_accuracy: 0.6360\n",
            "Epoch 318/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5532 - accuracy: 0.7360 - val_loss: 0.6685 - val_accuracy: 0.6400\n",
            "Epoch 319/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5542 - accuracy: 0.7320 - val_loss: 0.6697 - val_accuracy: 0.6400\n",
            "Epoch 320/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5564 - accuracy: 0.7000 - val_loss: 0.6710 - val_accuracy: 0.6400\n",
            "Epoch 321/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5550 - accuracy: 0.7400 - val_loss: 0.6596 - val_accuracy: 0.6360\n",
            "Epoch 322/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5548 - accuracy: 0.7480 - val_loss: 0.6656 - val_accuracy: 0.6400\n",
            "Epoch 323/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5551 - accuracy: 0.7200 - val_loss: 0.6692 - val_accuracy: 0.6440\n",
            "Epoch 324/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5526 - accuracy: 0.7320 - val_loss: 0.6653 - val_accuracy: 0.6360\n",
            "Epoch 325/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5542 - accuracy: 0.7400 - val_loss: 0.6634 - val_accuracy: 0.6320\n",
            "Epoch 326/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5526 - accuracy: 0.7400 - val_loss: 0.6665 - val_accuracy: 0.6440\n",
            "Epoch 327/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5538 - accuracy: 0.7160 - val_loss: 0.6690 - val_accuracy: 0.6400\n",
            "Epoch 328/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5544 - accuracy: 0.7280 - val_loss: 0.6633 - val_accuracy: 0.6320\n",
            "Epoch 329/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5546 - accuracy: 0.7480 - val_loss: 0.6655 - val_accuracy: 0.6320\n",
            "Epoch 330/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5563 - accuracy: 0.7040 - val_loss: 0.6732 - val_accuracy: 0.6320\n",
            "Epoch 331/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5529 - accuracy: 0.7240 - val_loss: 0.6633 - val_accuracy: 0.6240\n",
            "Epoch 332/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5562 - accuracy: 0.7480 - val_loss: 0.6637 - val_accuracy: 0.6320\n",
            "Epoch 333/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5522 - accuracy: 0.7320 - val_loss: 0.6692 - val_accuracy: 0.6400\n",
            "Epoch 334/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5539 - accuracy: 0.7280 - val_loss: 0.6699 - val_accuracy: 0.6360\n",
            "Epoch 335/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5547 - accuracy: 0.7400 - val_loss: 0.6647 - val_accuracy: 0.6320\n",
            "Epoch 336/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5536 - accuracy: 0.7320 - val_loss: 0.6693 - val_accuracy: 0.6400\n",
            "Epoch 337/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5534 - accuracy: 0.7280 - val_loss: 0.6679 - val_accuracy: 0.6400\n",
            "Epoch 338/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5561 - accuracy: 0.7240 - val_loss: 0.6643 - val_accuracy: 0.6400\n",
            "Epoch 339/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5543 - accuracy: 0.7320 - val_loss: 0.6684 - val_accuracy: 0.6440\n",
            "Epoch 340/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5525 - accuracy: 0.7320 - val_loss: 0.6654 - val_accuracy: 0.6280\n",
            "Epoch 341/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5534 - accuracy: 0.7400 - val_loss: 0.6642 - val_accuracy: 0.6400\n",
            "Epoch 342/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5533 - accuracy: 0.7320 - val_loss: 0.6662 - val_accuracy: 0.6400\n",
            "Epoch 343/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5532 - accuracy: 0.7320 - val_loss: 0.6673 - val_accuracy: 0.6280\n",
            "Epoch 344/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.7400 - val_loss: 0.6644 - val_accuracy: 0.6320\n",
            "Epoch 345/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5539 - accuracy: 0.7360 - val_loss: 0.6697 - val_accuracy: 0.6360\n",
            "Epoch 346/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5524 - accuracy: 0.7320 - val_loss: 0.6665 - val_accuracy: 0.6320\n",
            "Epoch 347/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5532 - accuracy: 0.7240 - val_loss: 0.6663 - val_accuracy: 0.6320\n",
            "Epoch 348/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.7280 - val_loss: 0.6658 - val_accuracy: 0.6280\n",
            "Epoch 349/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5531 - accuracy: 0.7280 - val_loss: 0.6686 - val_accuracy: 0.6320\n",
            "Epoch 350/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5527 - accuracy: 0.7280 - val_loss: 0.6690 - val_accuracy: 0.6400\n",
            "Epoch 351/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5550 - accuracy: 0.7280 - val_loss: 0.6621 - val_accuracy: 0.6400\n",
            "Epoch 352/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5540 - accuracy: 0.7360 - val_loss: 0.6670 - val_accuracy: 0.6360\n",
            "Epoch 353/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5524 - accuracy: 0.7280 - val_loss: 0.6715 - val_accuracy: 0.6360\n",
            "Epoch 354/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5535 - accuracy: 0.7240 - val_loss: 0.6662 - val_accuracy: 0.6320\n",
            "Epoch 355/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5525 - accuracy: 0.7320 - val_loss: 0.6681 - val_accuracy: 0.6360\n",
            "Epoch 356/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5531 - accuracy: 0.7280 - val_loss: 0.6673 - val_accuracy: 0.6360\n",
            "Epoch 357/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5522 - accuracy: 0.7360 - val_loss: 0.6687 - val_accuracy: 0.6440\n",
            "Epoch 358/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5527 - accuracy: 0.7320 - val_loss: 0.6676 - val_accuracy: 0.6360\n",
            "Epoch 359/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.7400 - val_loss: 0.6690 - val_accuracy: 0.6360\n",
            "Epoch 360/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5525 - accuracy: 0.7320 - val_loss: 0.6651 - val_accuracy: 0.6280\n",
            "Epoch 361/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5522 - accuracy: 0.7440 - val_loss: 0.6677 - val_accuracy: 0.6400\n",
            "Epoch 362/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5521 - accuracy: 0.7320 - val_loss: 0.6666 - val_accuracy: 0.6360\n",
            "Epoch 363/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5519 - accuracy: 0.7280 - val_loss: 0.6696 - val_accuracy: 0.6400\n",
            "Epoch 364/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5532 - accuracy: 0.7160 - val_loss: 0.6711 - val_accuracy: 0.6320\n",
            "Epoch 365/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5523 - accuracy: 0.7320 - val_loss: 0.6667 - val_accuracy: 0.6280\n",
            "Epoch 366/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5520 - accuracy: 0.7480 - val_loss: 0.6660 - val_accuracy: 0.6320\n",
            "Epoch 367/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5535 - accuracy: 0.7240 - val_loss: 0.6729 - val_accuracy: 0.6360\n",
            "Epoch 368/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5529 - accuracy: 0.7360 - val_loss: 0.6684 - val_accuracy: 0.6400\n",
            "Epoch 369/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5527 - accuracy: 0.7280 - val_loss: 0.6692 - val_accuracy: 0.6400\n",
            "Epoch 370/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5511 - accuracy: 0.7400 - val_loss: 0.6655 - val_accuracy: 0.6360\n",
            "Epoch 371/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5510 - accuracy: 0.7400 - val_loss: 0.6678 - val_accuracy: 0.6360\n",
            "Epoch 372/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5520 - accuracy: 0.7320 - val_loss: 0.6703 - val_accuracy: 0.6360\n",
            "Epoch 373/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5511 - accuracy: 0.7360 - val_loss: 0.6667 - val_accuracy: 0.6320\n",
            "Epoch 374/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5541 - accuracy: 0.7480 - val_loss: 0.6642 - val_accuracy: 0.6400\n",
            "Epoch 375/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5512 - accuracy: 0.7440 - val_loss: 0.6694 - val_accuracy: 0.6280\n",
            "Epoch 376/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5516 - accuracy: 0.7280 - val_loss: 0.6717 - val_accuracy: 0.6240\n",
            "Epoch 377/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5533 - accuracy: 0.7200 - val_loss: 0.6700 - val_accuracy: 0.6360\n",
            "Epoch 378/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5511 - accuracy: 0.7400 - val_loss: 0.6682 - val_accuracy: 0.6400\n",
            "Epoch 379/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5521 - accuracy: 0.7280 - val_loss: 0.6749 - val_accuracy: 0.6360\n",
            "Epoch 380/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5561 - accuracy: 0.7360 - val_loss: 0.6678 - val_accuracy: 0.6400\n",
            "Epoch 381/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5530 - accuracy: 0.7360 - val_loss: 0.6708 - val_accuracy: 0.6320\n",
            "Epoch 382/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5522 - accuracy: 0.7320 - val_loss: 0.6684 - val_accuracy: 0.6360\n",
            "Epoch 383/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5511 - accuracy: 0.7320 - val_loss: 0.6701 - val_accuracy: 0.6360\n",
            "Epoch 384/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.7360 - val_loss: 0.6667 - val_accuracy: 0.6360\n",
            "Epoch 385/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5529 - accuracy: 0.7360 - val_loss: 0.6706 - val_accuracy: 0.6240\n",
            "Epoch 386/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5504 - accuracy: 0.7280 - val_loss: 0.6731 - val_accuracy: 0.6400\n",
            "Epoch 387/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5513 - accuracy: 0.7360 - val_loss: 0.6718 - val_accuracy: 0.6360\n",
            "Epoch 388/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5531 - accuracy: 0.7320 - val_loss: 0.6691 - val_accuracy: 0.6320\n",
            "Epoch 389/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5513 - accuracy: 0.7320 - val_loss: 0.6688 - val_accuracy: 0.6400\n",
            "Epoch 390/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5527 - accuracy: 0.7120 - val_loss: 0.6696 - val_accuracy: 0.6360\n",
            "Epoch 391/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5509 - accuracy: 0.7320 - val_loss: 0.6657 - val_accuracy: 0.6400\n",
            "Epoch 392/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5523 - accuracy: 0.7520 - val_loss: 0.6694 - val_accuracy: 0.6360\n",
            "Epoch 393/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5502 - accuracy: 0.7360 - val_loss: 0.6689 - val_accuracy: 0.6360\n",
            "Epoch 394/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5508 - accuracy: 0.7400 - val_loss: 0.6688 - val_accuracy: 0.6400\n",
            "Epoch 395/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5502 - accuracy: 0.7360 - val_loss: 0.6690 - val_accuracy: 0.6320\n",
            "Epoch 396/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5505 - accuracy: 0.7280 - val_loss: 0.6723 - val_accuracy: 0.6400\n",
            "Epoch 397/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5501 - accuracy: 0.7200 - val_loss: 0.6720 - val_accuracy: 0.6360\n",
            "Epoch 398/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5514 - accuracy: 0.7360 - val_loss: 0.6650 - val_accuracy: 0.6320\n",
            "Epoch 399/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5498 - accuracy: 0.7480 - val_loss: 0.6703 - val_accuracy: 0.6320\n",
            "Epoch 400/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5514 - accuracy: 0.7240 - val_loss: 0.6761 - val_accuracy: 0.6280\n",
            "Epoch 401/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5518 - accuracy: 0.7360 - val_loss: 0.6661 - val_accuracy: 0.6400\n",
            "Epoch 402/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.7400 - val_loss: 0.6671 - val_accuracy: 0.6280\n",
            "Epoch 403/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.7280 - val_loss: 0.6734 - val_accuracy: 0.6320\n",
            "Epoch 404/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5515 - accuracy: 0.7200 - val_loss: 0.6702 - val_accuracy: 0.6320\n",
            "Epoch 405/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5506 - accuracy: 0.7280 - val_loss: 0.6702 - val_accuracy: 0.6360\n",
            "Epoch 406/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5512 - accuracy: 0.7360 - val_loss: 0.6668 - val_accuracy: 0.6360\n",
            "Epoch 407/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5499 - accuracy: 0.7400 - val_loss: 0.6674 - val_accuracy: 0.6400\n",
            "Epoch 408/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5510 - accuracy: 0.7160 - val_loss: 0.6747 - val_accuracy: 0.6240\n",
            "Epoch 409/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5520 - accuracy: 0.7160 - val_loss: 0.6720 - val_accuracy: 0.6440\n",
            "Epoch 410/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5489 - accuracy: 0.7400 - val_loss: 0.6641 - val_accuracy: 0.6360\n",
            "Epoch 411/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5516 - accuracy: 0.7360 - val_loss: 0.6685 - val_accuracy: 0.6400\n",
            "Epoch 412/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5497 - accuracy: 0.7280 - val_loss: 0.6708 - val_accuracy: 0.6400\n",
            "Epoch 413/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5514 - accuracy: 0.7280 - val_loss: 0.6665 - val_accuracy: 0.6320\n",
            "Epoch 414/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5499 - accuracy: 0.7320 - val_loss: 0.6692 - val_accuracy: 0.6360\n",
            "Epoch 415/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5489 - accuracy: 0.7320 - val_loss: 0.6690 - val_accuracy: 0.6440\n",
            "Epoch 416/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5523 - accuracy: 0.7240 - val_loss: 0.6693 - val_accuracy: 0.6440\n",
            "Epoch 417/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5511 - accuracy: 0.7440 - val_loss: 0.6633 - val_accuracy: 0.6360\n",
            "Epoch 418/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5497 - accuracy: 0.7360 - val_loss: 0.6686 - val_accuracy: 0.6280\n",
            "Epoch 419/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5507 - accuracy: 0.7160 - val_loss: 0.6748 - val_accuracy: 0.6280\n",
            "Epoch 420/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5491 - accuracy: 0.7320 - val_loss: 0.6700 - val_accuracy: 0.6360\n",
            "Epoch 421/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5496 - accuracy: 0.7520 - val_loss: 0.6649 - val_accuracy: 0.6360\n",
            "Epoch 422/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5496 - accuracy: 0.7400 - val_loss: 0.6695 - val_accuracy: 0.6400\n",
            "Epoch 423/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.7360 - val_loss: 0.6725 - val_accuracy: 0.6320\n",
            "Epoch 424/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5496 - accuracy: 0.7200 - val_loss: 0.6729 - val_accuracy: 0.6240\n",
            "Epoch 425/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5503 - accuracy: 0.7240 - val_loss: 0.6675 - val_accuracy: 0.6400\n",
            "Epoch 426/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5509 - accuracy: 0.7200 - val_loss: 0.6682 - val_accuracy: 0.6360\n",
            "Epoch 427/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5487 - accuracy: 0.7360 - val_loss: 0.6665 - val_accuracy: 0.6400\n",
            "Epoch 428/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5497 - accuracy: 0.7520 - val_loss: 0.6688 - val_accuracy: 0.6360\n",
            "Epoch 429/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5497 - accuracy: 0.7480 - val_loss: 0.6681 - val_accuracy: 0.6400\n",
            "Epoch 430/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5488 - accuracy: 0.7360 - val_loss: 0.6707 - val_accuracy: 0.6400\n",
            "Epoch 431/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5493 - accuracy: 0.7240 - val_loss: 0.6709 - val_accuracy: 0.6400\n",
            "Epoch 432/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5482 - accuracy: 0.7280 - val_loss: 0.6718 - val_accuracy: 0.6400\n",
            "Epoch 433/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5490 - accuracy: 0.7360 - val_loss: 0.6687 - val_accuracy: 0.6400\n",
            "Epoch 434/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5510 - accuracy: 0.7280 - val_loss: 0.6730 - val_accuracy: 0.6440\n",
            "Epoch 435/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5523 - accuracy: 0.7440 - val_loss: 0.6645 - val_accuracy: 0.6320\n",
            "Epoch 436/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5483 - accuracy: 0.7520 - val_loss: 0.6720 - val_accuracy: 0.6320\n",
            "Epoch 437/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5485 - accuracy: 0.7360 - val_loss: 0.6730 - val_accuracy: 0.6360\n",
            "Epoch 438/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5501 - accuracy: 0.7280 - val_loss: 0.6685 - val_accuracy: 0.6400\n",
            "Epoch 439/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5488 - accuracy: 0.7360 - val_loss: 0.6741 - val_accuracy: 0.6400\n",
            "Epoch 440/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5487 - accuracy: 0.7200 - val_loss: 0.6706 - val_accuracy: 0.6400\n",
            "Epoch 441/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5473 - accuracy: 0.7320 - val_loss: 0.6718 - val_accuracy: 0.6360\n",
            "Epoch 442/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5482 - accuracy: 0.7360 - val_loss: 0.6684 - val_accuracy: 0.6440\n",
            "Epoch 443/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5492 - accuracy: 0.7480 - val_loss: 0.6676 - val_accuracy: 0.6400\n",
            "Epoch 444/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5492 - accuracy: 0.7360 - val_loss: 0.6756 - val_accuracy: 0.6280\n",
            "Epoch 445/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5486 - accuracy: 0.7280 - val_loss: 0.6714 - val_accuracy: 0.6400\n",
            "Epoch 446/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5481 - accuracy: 0.7400 - val_loss: 0.6698 - val_accuracy: 0.6360\n",
            "Epoch 447/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5479 - accuracy: 0.7400 - val_loss: 0.6704 - val_accuracy: 0.6400\n",
            "Epoch 448/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5483 - accuracy: 0.7400 - val_loss: 0.6700 - val_accuracy: 0.6440\n",
            "Epoch 449/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5475 - accuracy: 0.7360 - val_loss: 0.6705 - val_accuracy: 0.6440\n",
            "Epoch 450/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5481 - accuracy: 0.7400 - val_loss: 0.6665 - val_accuracy: 0.6400\n",
            "Epoch 451/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5481 - accuracy: 0.7520 - val_loss: 0.6702 - val_accuracy: 0.6400\n",
            "Epoch 452/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5516 - accuracy: 0.7080 - val_loss: 0.6762 - val_accuracy: 0.6360\n",
            "Epoch 453/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5483 - accuracy: 0.7360 - val_loss: 0.6690 - val_accuracy: 0.6360\n",
            "Epoch 454/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5476 - accuracy: 0.7440 - val_loss: 0.6699 - val_accuracy: 0.6440\n",
            "Epoch 455/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5484 - accuracy: 0.7400 - val_loss: 0.6686 - val_accuracy: 0.6360\n",
            "Epoch 456/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5467 - accuracy: 0.7320 - val_loss: 0.6732 - val_accuracy: 0.6440\n",
            "Epoch 457/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5477 - accuracy: 0.7200 - val_loss: 0.6721 - val_accuracy: 0.6400\n",
            "Epoch 458/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5472 - accuracy: 0.7280 - val_loss: 0.6744 - val_accuracy: 0.6320\n",
            "Epoch 459/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5475 - accuracy: 0.7320 - val_loss: 0.6698 - val_accuracy: 0.6320\n",
            "Epoch 460/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5482 - accuracy: 0.7520 - val_loss: 0.6673 - val_accuracy: 0.6440\n",
            "Epoch 461/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5489 - accuracy: 0.7400 - val_loss: 0.6744 - val_accuracy: 0.6400\n",
            "Epoch 462/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5498 - accuracy: 0.7280 - val_loss: 0.6758 - val_accuracy: 0.6320\n",
            "Epoch 463/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5476 - accuracy: 0.7320 - val_loss: 0.6710 - val_accuracy: 0.6360\n",
            "Epoch 464/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5478 - accuracy: 0.7480 - val_loss: 0.6709 - val_accuracy: 0.6400\n",
            "Epoch 465/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5473 - accuracy: 0.7480 - val_loss: 0.6720 - val_accuracy: 0.6360\n",
            "Epoch 466/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5470 - accuracy: 0.7240 - val_loss: 0.6721 - val_accuracy: 0.6360\n",
            "Epoch 467/500\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5478 - accuracy: 0.7560 - val_loss: 0.6676 - val_accuracy: 0.6320\n",
            "Epoch 468/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5490 - accuracy: 0.7280 - val_loss: 0.6715 - val_accuracy: 0.6440\n",
            "Epoch 469/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5486 - accuracy: 0.7200 - val_loss: 0.6727 - val_accuracy: 0.6320\n",
            "Epoch 470/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5475 - accuracy: 0.7400 - val_loss: 0.6686 - val_accuracy: 0.6440\n",
            "Epoch 471/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5477 - accuracy: 0.7360 - val_loss: 0.6702 - val_accuracy: 0.6480\n",
            "Epoch 472/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5497 - accuracy: 0.7240 - val_loss: 0.6789 - val_accuracy: 0.6280\n",
            "Epoch 473/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5491 - accuracy: 0.7360 - val_loss: 0.6716 - val_accuracy: 0.6240\n",
            "Epoch 474/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5474 - accuracy: 0.7520 - val_loss: 0.6703 - val_accuracy: 0.6360\n",
            "Epoch 475/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5470 - accuracy: 0.7480 - val_loss: 0.6721 - val_accuracy: 0.6360\n",
            "Epoch 476/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5485 - accuracy: 0.7200 - val_loss: 0.6772 - val_accuracy: 0.6320\n",
            "Epoch 477/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5485 - accuracy: 0.7280 - val_loss: 0.6673 - val_accuracy: 0.6360\n",
            "Epoch 478/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5484 - accuracy: 0.7360 - val_loss: 0.6744 - val_accuracy: 0.6400\n",
            "Epoch 479/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5470 - accuracy: 0.7320 - val_loss: 0.6693 - val_accuracy: 0.6440\n",
            "Epoch 480/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5463 - accuracy: 0.7440 - val_loss: 0.6716 - val_accuracy: 0.6360\n",
            "Epoch 481/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5469 - accuracy: 0.7520 - val_loss: 0.6711 - val_accuracy: 0.6400\n",
            "Epoch 482/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5455 - accuracy: 0.7320 - val_loss: 0.6751 - val_accuracy: 0.6320\n",
            "Epoch 483/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5460 - accuracy: 0.7240 - val_loss: 0.6738 - val_accuracy: 0.6400\n",
            "Epoch 484/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5510 - accuracy: 0.7360 - val_loss: 0.6658 - val_accuracy: 0.6400\n",
            "Epoch 485/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5491 - accuracy: 0.7400 - val_loss: 0.6745 - val_accuracy: 0.6360\n",
            "Epoch 486/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5461 - accuracy: 0.7280 - val_loss: 0.6755 - val_accuracy: 0.6360\n",
            "Epoch 487/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5465 - accuracy: 0.7320 - val_loss: 0.6732 - val_accuracy: 0.6360\n",
            "Epoch 488/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5459 - accuracy: 0.7400 - val_loss: 0.6705 - val_accuracy: 0.6360\n",
            "Epoch 489/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5464 - accuracy: 0.7400 - val_loss: 0.6720 - val_accuracy: 0.6320\n",
            "Epoch 490/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5457 - accuracy: 0.7360 - val_loss: 0.6724 - val_accuracy: 0.6320\n",
            "Epoch 491/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5469 - accuracy: 0.7240 - val_loss: 0.6734 - val_accuracy: 0.6320\n",
            "Epoch 492/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5507 - accuracy: 0.7360 - val_loss: 0.6672 - val_accuracy: 0.6360\n",
            "Epoch 493/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5465 - accuracy: 0.7480 - val_loss: 0.6685 - val_accuracy: 0.6440\n",
            "Epoch 494/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5472 - accuracy: 0.7200 - val_loss: 0.6755 - val_accuracy: 0.6280\n",
            "Epoch 495/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5456 - accuracy: 0.7360 - val_loss: 0.6739 - val_accuracy: 0.6360\n",
            "Epoch 496/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5474 - accuracy: 0.7360 - val_loss: 0.6740 - val_accuracy: 0.6280\n",
            "Epoch 497/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5476 - accuracy: 0.7320 - val_loss: 0.6698 - val_accuracy: 0.6320\n",
            "Epoch 498/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5465 - accuracy: 0.7440 - val_loss: 0.6740 - val_accuracy: 0.6360\n",
            "Epoch 499/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5493 - accuracy: 0.7480 - val_loss: 0.6707 - val_accuracy: 0.6440\n",
            "Epoch 500/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5461 - accuracy: 0.7280 - val_loss: 0.6783 - val_accuracy: 0.6360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Loss\n"
      ],
      "metadata": {
        "id": "Ib2daaX0gx1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Overfit')"
      ],
      "metadata": {
        "id": "cQ17ZVqUg0My",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "29e5bd89-ab9a-4df0-ff01-8ae3f93aff17"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"d9f22b4a-f5ed-4749-a35a-1fc4dbcc7dea\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d9f22b4a-f5ed-4749-a35a-1fc4dbcc7dea\")) {                    Plotly.newPlot(                        \"d9f22b4a-f5ed-4749-a35a-1fc4dbcc7dea\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"loss\",\"y\":[0.6995645761489868,0.6933032870292664,0.6901823282241821,0.687669038772583,0.685541570186615,0.6838340163230896,0.6825523972511292,0.6816211342811584,0.6796268820762634,0.6783019304275513,0.6767464280128479,0.675067126750946,0.6739732623100281,0.6723952889442444,0.6704003810882568,0.6688933968544006,0.6672465205192566,0.6652461290359497,0.6634293794631958,0.6617518663406372,0.6601969599723816,0.6576115489006042,0.655982494354248,0.6541811227798462,0.6521487236022949,0.6499716639518738,0.6483201384544373,0.6457973718643188,0.6448236703872681,0.6423580050468445,0.6396876573562622,0.6371973752975464,0.6352066397666931,0.6326689720153809,0.6311357617378235,0.6277279853820801,0.6264527440071106,0.6240016222000122,0.6211668848991394,0.6190668344497681,0.6181272268295288,0.6156637072563171,0.6119178533554077,0.6142452359199524,0.6086660623550415,0.6075835824012756,0.6046154499053955,0.6024706363677979,0.6006910800933838,0.5985048413276672,0.5968912243843079,0.5958815813064575,0.5943434238433838,0.5931188464164734,0.5924679636955261,0.5902351140975952,0.588637113571167,0.5882737636566162,0.5867554545402527,0.5864643454551697,0.583868145942688,0.5832040905952454,0.5831193923950195,0.5814546942710876,0.5806083083152771,0.580315113067627,0.5789774656295776,0.5786815285682678,0.5835637450218201,0.5764980316162109,0.5766070485115051,0.5768935680389404,0.5749416351318359,0.5755739808082581,0.5745996832847595,0.5735123157501221,0.5737950801849365,0.5746915936470032,0.5718097686767578,0.5721820592880249,0.5716069340705872,0.5714242458343506,0.5710094571113586,0.5737858414649963,0.5722493529319763,0.5706061124801636,0.570744514465332,0.5703338980674744,0.5710300207138062,0.5715317130088806,0.569111168384552,0.570401668548584,0.5682953000068665,0.5679445862770081,0.5694707632064819,0.5682774782180786,0.5682493448257446,0.5751749873161316,0.5689287185668945,0.5680771470069885,0.5699695348739624,0.5676969289779663,0.5695657134056091,0.566537082195282,0.5678732395172119,0.5672305226325989,0.5680269598960876,0.5659602880477905,0.5665904879570007,0.5657562613487244,0.5659116506576538,0.5680420994758606,0.5677813291549683,0.5654439330101013,0.5668814182281494,0.5666619539260864,0.5666799545288086,0.5660402774810791,0.5663539171218872,0.56691575050354,0.5648276805877686,0.5664665102958679,0.5653660893440247,0.5646070241928101,0.5639682412147522,0.5670537948608398,0.56437748670578,0.5646165609359741,0.563942551612854,0.564037561416626,0.5651521682739258,0.5645442008972168,0.5635372996330261,0.564235270023346,0.5639240145683289,0.5673754215240479,0.5623611807823181,0.5631610155105591,0.5638837218284607,0.5628705024719238,0.5632328987121582,0.5637403130531311,0.5627099871635437,0.5628610253334045,0.5635573863983154,0.562271237373352,0.5623772740364075,0.5622382760047913,0.5622082948684692,0.565019428730011,0.5621284246444702,0.5630113482475281,0.5616304278373718,0.5617412328720093,0.5632910132408142,0.5631134510040283,0.5619531273841858,0.5624476671218872,0.5657170414924622,0.5642962455749512,0.563498318195343,0.5612888336181641,0.5624533295631409,0.5607455968856812,0.5620751976966858,0.5642938017845154,0.5626469254493713,0.5626322627067566,0.5633941292762756,0.561228334903717,0.5616475343704224,0.5604180693626404,0.5619475245475769,0.5613784790039062,0.5613464713096619,0.5621994137763977,0.5635214447975159,0.5624973773956299,0.5629004836082458,0.559846818447113,0.5594080090522766,0.5611191391944885,0.560515820980072,0.559542715549469,0.560438871383667,0.561487078666687,0.561396062374115,0.5596282482147217,0.5601771473884583,0.5601779818534851,0.5596352219581604,0.5589901208877563,0.5588870644569397,0.5662118792533875,0.5604796409606934,0.5599656701087952,0.55965256690979,0.559310257434845,0.5592350959777832,0.5594306588172913,0.5606134533882141,0.5593040585517883,0.5596113801002502,0.5585499405860901,0.5593599081039429,0.5585896968841553,0.5592074990272522,0.5601020455360413,0.5623669624328613,0.5596795082092285,0.5600480437278748,0.5584272146224976,0.5607752203941345,0.5596277117729187,0.558347225189209,0.5598579049110413,0.557997465133667,0.5602677464485168,0.5590257048606873,0.5576363205909729,0.5608115792274475,0.558544933795929,0.56016606092453,0.5594453811645508,0.5578169822692871,0.5587091445922852,0.5601252913475037,0.5582916736602783,0.5587215423583984,0.5582731366157532,0.5577014088630676,0.5584842562675476,0.561933159828186,0.5578714609146118,0.5589070916175842,0.5572788119316101,0.5568341612815857,0.5585422515869141,0.5611761212348938,0.558566153049469,0.5571823716163635,0.5568596720695496,0.5595893263816833,0.5586466193199158,0.5573753118515015,0.5571063160896301,0.5565228462219238,0.5577296614646912,0.5598305463790894,0.5608598589897156,0.558102548122406,0.5572685599327087,0.5567737817764282,0.5574883222579956,0.5574859380722046,0.5579515695571899,0.559497058391571,0.556552529335022,0.5574816465377808,0.5576271414756775,0.5574324727058411,0.5558167099952698,0.5575509071350098,0.5573452711105347,0.5584781765937805,0.5564749836921692,0.5563071966171265,0.555667519569397,0.5560186505317688,0.555733323097229,0.5558595657348633,0.5566774606704712,0.5598269104957581,0.5560570359230042,0.5583349466323853,0.5563156604766846,0.5565347075462341,0.5573155283927917,0.5554823279380798,0.5564877390861511,0.5572425723075867,0.555984616279602,0.5568341612815857,0.5556387305259705,0.5549139976501465,0.5553292036056519,0.5572787523269653,0.5578185319900513,0.5556528568267822,0.5583370327949524,0.5553178787231445,0.5568444728851318,0.5626308917999268,0.5545048117637634,0.5549579858779907,0.5543821454048157,0.5541121363639832,0.5584384799003601,0.5564377307891846,0.5538619160652161,0.5550509691238403,0.5551764965057373,0.5543580651283264,0.5546217560768127,0.5582724809646606,0.5548278093338013,0.5559931397438049,0.554572582244873,0.558706521987915,0.5539980530738831,0.5538816452026367,0.5540776252746582,0.5540621280670166,0.5552181601524353,0.5547580718994141,0.5546389818191528,0.5544072985649109,0.5532368421554565,0.5541560053825378,0.5563980937004089,0.5550302863121033,0.5547832250595093,0.5550966858863831,0.5526366829872131,0.554184079170227,0.5526296496391296,0.5538497567176819,0.5543980002403259,0.5545560717582703,0.5563395023345947,0.5529082417488098,0.5562337040901184,0.5522457957267761,0.5538702607154846,0.5547358989715576,0.5536103844642639,0.553425669670105,0.5561060905456543,0.5543462634086609,0.5525498986244202,0.5534159541130066,0.5533118844032288,0.5532052516937256,0.5516625046730042,0.5538794994354248,0.5524332523345947,0.5531859397888184,0.5517480969429016,0.5530834794044495,0.5526803135871887,0.5550053119659424,0.5540041327476501,0.5523726940155029,0.553493320941925,0.5525100231170654,0.5530704259872437,0.5522007346153259,0.5527454018592834,0.5516649484634399,0.5525104403495789,0.5522206425666809,0.5520787239074707,0.5518695116043091,0.5532026886940002,0.5523244738578796,0.551990270614624,0.5534608364105225,0.5528638958930969,0.552747368812561,0.5511264204978943,0.550987958908081,0.5519730448722839,0.5511183142662048,0.5541498064994812,0.5511701107025146,0.5516014695167542,0.5533122420310974,0.5510907173156738,0.5521249175071716,0.5560654997825623,0.553035318851471,0.5521508455276489,0.5510849356651306,0.5517479777336121,0.5528703331947327,0.5503852367401123,0.5513243079185486,0.5530693531036377,0.5512743592262268,0.5526795983314514,0.5508798956871033,0.5522698760032654,0.5501882433891296,0.5507954359054565,0.5501624345779419,0.5505253076553345,0.5500800609588623,0.5513986945152283,0.5497705340385437,0.5514168739318848,0.5518254041671753,0.5507885813713074,0.5508431196212769,0.5515099167823792,0.55061274766922,0.5512294769287109,0.5499327182769775,0.5509684085845947,0.5520130395889282,0.5488955974578857,0.5515637993812561,0.5496860146522522,0.5514329075813293,0.5498716831207275,0.5489097833633423,0.5522657036781311,0.5511276721954346,0.549685537815094,0.5506615042686462,0.5491121411323547,0.5496397614479065,0.5495544672012329,0.5507538914680481,0.549589216709137,0.5503211617469788,0.5509428977966309,0.5486864447593689,0.5496962666511536,0.5497196912765503,0.5487658977508545,0.5492809414863586,0.5481847524642944,0.5490230917930603,0.5510136485099792,0.5523030161857605,0.5483136773109436,0.5484786033630371,0.5500971674919128,0.5488422513008118,0.548656165599823,0.5473427772521973,0.5481522083282471,0.5491784811019897,0.5491713881492615,0.5485639572143555,0.5480582118034363,0.5478845834732056,0.5483185052871704,0.5474869012832642,0.5481064915657043,0.5480733513832092,0.5515682697296143,0.5482568740844727,0.5476034283638,0.5483679175376892,0.546710193157196,0.5476536154747009,0.5471843481063843,0.5474580526351929,0.5482045412063599,0.5488585233688354,0.5498114228248596,0.5475752949714661,0.5477854013442993,0.5472957491874695,0.546970784664154,0.5478433966636658,0.5489644408226013,0.5486010313034058,0.5475317239761353,0.5476754307746887,0.549747109413147,0.549142062664032,0.5473771095275879,0.5469536781311035,0.5484864115715027,0.5484576225280762,0.5483532547950745,0.5469688177108765,0.5462707877159119,0.5469405651092529,0.5455265641212463,0.5459792017936707,0.5510193109512329,0.549087405204773,0.5460637211799622,0.5465173721313477,0.5459156036376953,0.5464240908622742,0.5456750988960266,0.546861469745636,0.5506851077079773,0.5465446710586548,0.5471767783164978,0.5455811619758606,0.5473854541778564,0.5476442575454712,0.5464856624603271,0.5493254065513611,0.5461499094963074],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_loss\",\"y\":[0.6834686398506165,0.6851465106010437,0.6862760782241821,0.6861876845359802,0.6873859167098999,0.6890835165977478,0.6897764205932617,0.6897043585777283,0.689696729183197,0.6889957189559937,0.688661515712738,0.6876716017723083,0.6878772974014282,0.6870102286338806,0.687045693397522,0.6865118741989136,0.6872672438621521,0.6857068538665771,0.6838840246200562,0.6813158392906189,0.6799894571304321,0.6826501488685608,0.6840394735336304,0.6823580265045166,0.6784623861312866,0.6788390278816223,0.6762861609458923,0.6769993305206299,0.6778700947761536,0.6710177063941956,0.6684695482254028,0.6712543964385986,0.6725955009460449,0.6690477132797241,0.6704093217849731,0.6661528944969177,0.6640565395355225,0.6615979671478271,0.6606691479682922,0.6612377166748047,0.660868763923645,0.6582050919532776,0.6560532450675964,0.6509449481964111,0.6585432887077332,0.6571725606918335,0.6534134149551392,0.6517203450202942,0.6502534747123718,0.648891031742096,0.6501560211181641,0.6500676870346069,0.6449807286262512,0.6460057497024536,0.643365740776062,0.6466699838638306,0.6465054154396057,0.6483756303787231,0.6448783278465271,0.640501856803894,0.6422421932220459,0.6423385143280029,0.6399106383323669,0.6465868949890137,0.6435759663581848,0.642187237739563,0.6412755250930786,0.639816403388977,0.6495077013969421,0.640483021736145,0.6380170583724976,0.6434535384178162,0.6445415019989014,0.6408189535140991,0.6433154940605164,0.646507978439331,0.6431530714035034,0.6451614499092102,0.6434504389762878,0.6462172269821167,0.6439226269721985,0.6407960057258606,0.642594039440155,0.6487579941749573,0.6415454149246216,0.6435384154319763,0.6451526880264282,0.6452293992042542,0.646991491317749,0.6517282724380493,0.6433719992637634,0.643104612827301,0.6462703943252563,0.6519441604614258,0.6466245055198669,0.6469428539276123,0.6473512649536133,0.6569247841835022,0.6427584886550903,0.6449481844902039,0.6463565230369568,0.6527464985847473,0.6527655124664307,0.6492382884025574,0.6439314484596252,0.6470102071762085,0.6524267792701721,0.6503366827964783,0.6504302024841309,0.6497366428375244,0.6498450636863708,0.6460510492324829,0.6503673791885376,0.6512442827224731,0.6538651585578918,0.6503511667251587,0.6456232666969299,0.6501636505126953,0.6568484306335449,0.6497368812561035,0.6517549157142639,0.6525761485099792,0.6506838798522949,0.6468203663825989,0.6506322622299194,0.6526198387145996,0.6500465869903564,0.6493901014328003,0.654795229434967,0.6530274748802185,0.652890145778656,0.6519920229911804,0.6518601775169373,0.649880051612854,0.6571628451347351,0.6497280597686768,0.6533342003822327,0.6579140424728394,0.6505605578422546,0.6522311568260193,0.6564210653305054,0.6548835039138794,0.6526120901107788,0.6518424153327942,0.6500341892242432,0.6533454060554504,0.6548929214477539,0.6563993096351624,0.6546511650085449,0.6558852195739746,0.6534737348556519,0.6543081998825073,0.653314471244812,0.6519713401794434,0.6546373963356018,0.6564526557922363,0.6521786451339722,0.6525055766105652,0.6504348516464233,0.6639068126678467,0.6600561141967773,0.6531356573104858,0.651515007019043,0.6549956202507019,0.6601539850234985,0.6524421572685242,0.6576701998710632,0.6592891812324524,0.6536260843276978,0.6575872898101807,0.6593987345695496,0.653897225856781,0.65467768907547,0.6593696475028992,0.6598705053329468,0.6580301523208618,0.6498042941093445,0.6587982177734375,0.6639472842216492,0.6583993434906006,0.6545666456222534,0.6566421985626221,0.6555969715118408,0.6615142822265625,0.6612556576728821,0.6592248678207397,0.6529005765914917,0.6585655808448792,0.6657058596611023,0.6581555008888245,0.657394528388977,0.6580643057823181,0.660497784614563,0.6709948182106018,0.6537083983421326,0.6561179161071777,0.6597265005111694,0.6613747477531433,0.6594873666763306,0.6561616659164429,0.6628530025482178,0.6565737128257751,0.6590697169303894,0.6604228615760803,0.6609458327293396,0.6566486358642578,0.6581165790557861,0.6643310189247131,0.6651048064231873,0.654582142829895,0.6553102731704712,0.6614344716072083,0.666300892829895,0.6581578850746155,0.6573374271392822,0.6637401580810547,0.6600006222724915,0.658726692199707,0.6607818007469177,0.6634460091590881,0.6580565571784973,0.6608408093452454,0.6670595407485962,0.6609417200088501,0.6564385890960693,0.6592839956283569,0.6697733402252197,0.6606069207191467,0.6585881114006042,0.6600561738014221,0.6643921136856079,0.6640235185623169,0.6562052965164185,0.6644738912582397,0.6636089086532593,0.6625753045082092,0.6660850644111633,0.6648150086402893,0.6587797999382019,0.6649932265281677,0.6646152138710022,0.6605420708656311,0.6645070314407349,0.6577576994895935,0.6640485525131226,0.6649061441421509,0.665138840675354,0.6617772579193115,0.6624451875686646,0.6575915813446045,0.66978919506073,0.6689227819442749,0.6607109308242798,0.6624261140823364,0.6644927859306335,0.6583482027053833,0.6664994359016418,0.6646996140480042,0.6636444926261902,0.6655640602111816,0.6659703850746155,0.6592649221420288,0.6611815094947815,0.6646900773048401,0.6718836426734924,0.6606655120849609,0.660868227481842,0.664872944355011,0.6656004190444946,0.6618027091026306,0.6660220623016357,0.6659888029098511,0.6561053991317749,0.6638182401657104,0.6703486442565918,0.6646243929862976,0.6606478095054626,0.6577195525169373,0.6637870073318481,0.6682652235031128,0.6678536534309387,0.6632311344146729,0.6646312475204468,0.6620641350746155,0.6594777703285217,0.6636322140693665,0.6646483540534973,0.6658700108528137,0.6643376350402832,0.6609094738960266,0.6654127836227417,0.6712968945503235,0.6576342582702637,0.6660548448562622,0.6681878566741943,0.6665397882461548,0.6650092601776123,0.6610888838768005,0.6675334572792053,0.6689332127571106,0.6663552522659302,0.6614604592323303,0.6659060120582581,0.6700080633163452,0.6612030267715454,0.6648895144462585,0.6694208383560181,0.667070209980011,0.6594476699829102,0.6677622199058533,0.6683475375175476,0.6656266450881958,0.6674336791038513,0.6668959856033325,0.6611141562461853,0.6641952991485596,0.6631602048873901,0.6684791445732117,0.669677734375,0.6709727644920349,0.6596236824989319,0.6655705571174622,0.6691589951515198,0.6653407216072083,0.6634151339530945,0.6664502024650574,0.6689894795417786,0.6633133292198181,0.6654943823814392,0.6731644868850708,0.6632724404335022,0.6636694073677063,0.6691958904266357,0.6699426174163818,0.6646934747695923,0.6693404316902161,0.6678512692451477,0.6643229126930237,0.6684051156044006,0.6654364466667175,0.6641539335250854,0.6661962270736694,0.6672796607017517,0.6644072532653809,0.6697102189064026,0.6665433645248413,0.666271448135376,0.6657809615135193,0.6686339378356934,0.6690142154693604,0.6621345281600952,0.6669514179229736,0.6714961528778076,0.6661558747291565,0.668147087097168,0.667280375957489,0.6687297821044922,0.6675727367401123,0.6690422892570496,0.6651027798652649,0.6677058339118958,0.66663658618927,0.6695783138275146,0.6711324453353882,0.6666883230209351,0.6660093069076538,0.6728968024253845,0.6683515906333923,0.6691968441009521,0.6655257940292358,0.6678084135055542,0.6703019738197327,0.6667267680168152,0.6642375588417053,0.6693909764289856,0.6716769337654114,0.6699556112289429,0.6682199835777283,0.6748877167701721,0.6677678823471069,0.6707668304443359,0.6683723330497742,0.6700568199157715,0.666749119758606,0.6705774068832397,0.6730625033378601,0.6718178391456604,0.6690823435783386,0.6687620878219604,0.6695634722709656,0.6657233834266663,0.6693953275680542,0.6688526272773743,0.6687582731246948,0.6690225601196289,0.6723212003707886,0.6720344424247742,0.6650221943855286,0.6703066229820251,0.6761384010314941,0.6661272048950195,0.6670757532119751,0.6733958125114441,0.6702185273170471,0.670203447341919,0.6668027639389038,0.6673967242240906,0.6746986508369446,0.6720407605171204,0.6640620827674866,0.6684907078742981,0.6708194613456726,0.6665169596672058,0.6692274212837219,0.6690487861633301,0.6692550182342529,0.6632551550865173,0.6685822606086731,0.6748315691947937,0.6700136661529541,0.6648701429367065,0.6695417165756226,0.6725288033485413,0.6728827357292175,0.667488157749176,0.6681737303733826,0.6664756536483765,0.6687595248222351,0.6681374311447144,0.6707444787025452,0.6709255576133728,0.6717596054077148,0.6687333583831787,0.6730197072029114,0.6645326614379883,0.6720033288002014,0.6730139851570129,0.6685086488723755,0.6740955710411072,0.6705652475357056,0.6718010306358337,0.668415904045105,0.66762375831604,0.6756107211112976,0.6713908910751343,0.6697826981544495,0.6704380512237549,0.6700034141540527,0.6704828143119812,0.6664701700210571,0.6702125072479248,0.6761953234672546,0.6690288782119751,0.6698939800262451,0.6685597896575928,0.6732048988342285,0.6720877885818481,0.6744375228881836,0.6698145866394043,0.6673045754432678,0.6744221448898315,0.6757832169532776,0.6709526181221008,0.6708709597587585,0.6719599366188049,0.6720666885375977,0.6676301956176758,0.6714553833007812,0.67268967628479,0.6685547232627869,0.6702333688735962,0.6788573265075684,0.6715556383132935,0.6702732443809509,0.6721131801605225,0.677202045917511,0.6672766208648682,0.6743755340576172,0.6692925691604614,0.6716264486312866,0.671061098575592,0.6751179695129395,0.673803448677063,0.6657742261886597,0.6744818687438965,0.675481379032135,0.6732272505760193,0.6705302596092224,0.6720196008682251,0.6724269390106201,0.6734382510185242,0.6672044992446899,0.6685006618499756,0.6754782795906067,0.6738901138305664,0.6739922761917114,0.6697555780410767,0.6739512085914612,0.670673131942749,0.6783150434494019],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d9f22b4a-f5ed-4749-a35a-1fc4dbcc7dea');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy"
      ],
      "metadata": {
        "id": "LaFTm3IPg3X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Overfit')"
      ],
      "metadata": {
        "id": "m_CeUNuog3ku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "aad17d2f-dbb0-45ef-d7e3-030cf88c9b64"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"67fbd286-05b5-4feb-9820-0712c1426bca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"67fbd286-05b5-4feb-9820-0712c1426bca\")) {                    Plotly.newPlot(                        \"67fbd286-05b5-4feb-9820-0712c1426bca\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"acc\",\"y\":[0.47200000286102295,0.4959999918937683,0.5519999861717224,0.5440000295639038,0.6000000238418579,0.6159999966621399,0.628000020980835,0.6200000047683716,0.6240000128746033,0.628000020980835,0.6439999938011169,0.671999990940094,0.656000018119812,0.6639999747276306,0.6639999747276306,0.6639999747276306,0.656000018119812,0.6480000019073486,0.6639999747276306,0.6679999828338623,0.6840000152587891,0.6800000071525574,0.6600000262260437,0.6679999828338623,0.6639999747276306,0.6800000071525574,0.671999990940094,0.6759999990463257,0.6679999828338623,0.6639999747276306,0.6679999828338623,0.6840000152587891,0.6840000152587891,0.6919999718666077,0.6880000233650208,0.6800000071525574,0.6840000152587891,0.6919999718666077,0.6959999799728394,0.699999988079071,0.6759999990463257,0.6840000152587891,0.6959999799728394,0.7080000042915344,0.7080000042915344,0.6759999990463257,0.6800000071525574,0.7080000042915344,0.7200000286102295,0.7039999961853027,0.6959999799728394,0.6880000233650208,0.6959999799728394,0.7160000205039978,0.7039999961853027,0.7080000042915344,0.7080000042915344,0.6840000152587891,0.6959999799728394,0.6919999718666077,0.7160000205039978,0.7120000123977661,0.7279999852180481,0.7080000042915344,0.6919999718666077,0.699999988079071,0.7160000205039978,0.7160000205039978,0.699999988079071,0.7080000042915344,0.7279999852180481,0.7200000286102295,0.699999988079071,0.7239999771118164,0.7120000123977661,0.699999988079071,0.6959999799728394,0.6959999799728394,0.7080000042915344,0.7160000205039978,0.7080000042915344,0.7200000286102295,0.7200000286102295,0.7120000123977661,0.7120000123977661,0.7160000205039978,0.7160000205039978,0.7160000205039978,0.7160000205039978,0.7120000123977661,0.7080000042915344,0.7279999852180481,0.7279999852180481,0.7120000123977661,0.699999988079071,0.7160000205039978,0.7239999771118164,0.6959999799728394,0.7120000123977661,0.7360000014305115,0.7200000286102295,0.7080000042915344,0.7080000042915344,0.7160000205039978,0.7160000205039978,0.7239999771118164,0.7120000123977661,0.7120000123977661,0.7080000042915344,0.7080000042915344,0.7080000042915344,0.7279999852180481,0.7360000014305115,0.7160000205039978,0.7080000042915344,0.7039999961853027,0.7239999771118164,0.7360000014305115,0.699999988079071,0.7080000042915344,0.7120000123977661,0.6959999799728394,0.7120000123977661,0.7279999852180481,0.7400000095367432,0.699999988079071,0.7080000042915344,0.7160000205039978,0.7160000205039978,0.7080000042915344,0.7039999961853027,0.7120000123977661,0.7200000286102295,0.7279999852180481,0.7200000286102295,0.7120000123977661,0.7319999933242798,0.7039999961853027,0.7080000042915344,0.7200000286102295,0.7080000042915344,0.7039999961853027,0.7080000042915344,0.7120000123977661,0.7400000095367432,0.7239999771118164,0.7080000042915344,0.7120000123977661,0.7120000123977661,0.7120000123977661,0.7160000205039978,0.7200000286102295,0.7080000042915344,0.7279999852180481,0.7120000123977661,0.7120000123977661,0.7160000205039978,0.7200000286102295,0.7400000095367432,0.7080000042915344,0.7160000205039978,0.7160000205039978,0.7319999933242798,0.7279999852180481,0.7120000123977661,0.7160000205039978,0.7080000042915344,0.7039999961853027,0.7120000123977661,0.7239999771118164,0.7120000123977661,0.7200000286102295,0.7400000095367432,0.7200000286102295,0.7080000042915344,0.7160000205039978,0.7319999933242798,0.7279999852180481,0.7039999961853027,0.7039999961853027,0.7200000286102295,0.7319999933242798,0.7319999933242798,0.7080000042915344,0.7080000042915344,0.7160000205039978,0.7519999742507935,0.7360000014305115,0.7200000286102295,0.7200000286102295,0.7279999852180481,0.7319999933242798,0.7400000095367432,0.6919999718666077,0.699999988079071,0.7440000176429749,0.7279999852180481,0.7160000205039978,0.7200000286102295,0.7360000014305115,0.7319999933242798,0.7319999933242798,0.7360000014305115,0.7239999771118164,0.7200000286102295,0.7400000095367432,0.7360000014305115,0.7200000286102295,0.7039999961853027,0.7200000286102295,0.7559999823570251,0.7279999852180481,0.7160000205039978,0.7160000205039978,0.7360000014305115,0.7239999771118164,0.7200000286102295,0.7239999771118164,0.7279999852180481,0.7120000123977661,0.7080000042915344,0.7279999852180481,0.7200000286102295,0.7080000042915344,0.7319999933242798,0.7319999933242798,0.7239999771118164,0.7200000286102295,0.7480000257492065,0.7279999852180481,0.7080000042915344,0.7080000042915344,0.7279999852180481,0.7319999933242798,0.7279999852180481,0.7239999771118164,0.7200000286102295,0.7200000286102295,0.7279999852180481,0.7239999771118164,0.7160000205039978,0.7239999771118164,0.7080000042915344,0.7400000095367432,0.7360000014305115,0.7200000286102295,0.7200000286102295,0.7200000286102295,0.7160000205039978,0.7319999933242798,0.7239999771118164,0.7080000042915344,0.7160000205039978,0.7440000176429749,0.7239999771118164,0.7559999823570251,0.7239999771118164,0.7239999771118164,0.7120000123977661,0.7080000042915344,0.7120000123977661,0.7279999852180481,0.7440000176429749,0.7319999933242798,0.7039999961853027,0.7160000205039978,0.7360000014305115,0.7160000205039978,0.7160000205039978,0.7360000014305115,0.7319999933242798,0.7080000042915344,0.7200000286102295,0.7480000257492065,0.7120000123977661,0.7160000205039978,0.7200000286102295,0.7440000176429749,0.7440000176429749,0.7160000205039978,0.7200000286102295,0.7200000286102295,0.7239999771118164,0.7279999852180481,0.7440000176429749,0.7400000095367432,0.7160000205039978,0.7319999933242798,0.7279999852180481,0.7279999852180481,0.7440000176429749,0.7120000123977661,0.7239999771118164,0.7279999852180481,0.7200000286102295,0.7160000205039978,0.7319999933242798,0.7360000014305115,0.7319999933242798,0.7160000205039978,0.7319999933242798,0.7319999933242798,0.7400000095367432,0.7200000286102295,0.7400000095367432,0.7319999933242798,0.7120000123977661,0.7120000123977661,0.7360000014305115,0.7360000014305115,0.7200000286102295,0.7279999852180481,0.7319999933242798,0.7039999961853027,0.7400000095367432,0.7480000257492065,0.7440000176429749,0.7360000014305115,0.7319999933242798,0.699999988079071,0.7400000095367432,0.7480000257492065,0.7200000286102295,0.7319999933242798,0.7400000095367432,0.7400000095367432,0.7160000205039978,0.7279999852180481,0.7480000257492065,0.7039999961853027,0.7239999771118164,0.7480000257492065,0.7319999933242798,0.7279999852180481,0.7400000095367432,0.7319999933242798,0.7279999852180481,0.7239999771118164,0.7319999933242798,0.7319999933242798,0.7400000095367432,0.7319999933242798,0.7319999933242798,0.7400000095367432,0.7360000014305115,0.7319999933242798,0.7239999771118164,0.7279999852180481,0.7279999852180481,0.7279999852180481,0.7279999852180481,0.7360000014305115,0.7279999852180481,0.7239999771118164,0.7319999933242798,0.7279999852180481,0.7360000014305115,0.7319999933242798,0.7400000095367432,0.7319999933242798,0.7440000176429749,0.7319999933242798,0.7279999852180481,0.7160000205039978,0.7319999933242798,0.7480000257492065,0.7239999771118164,0.7360000014305115,0.7279999852180481,0.7400000095367432,0.7400000095367432,0.7319999933242798,0.7360000014305115,0.7480000257492065,0.7440000176429749,0.7279999852180481,0.7200000286102295,0.7400000095367432,0.7279999852180481,0.7360000014305115,0.7360000014305115,0.7319999933242798,0.7319999933242798,0.7360000014305115,0.7360000014305115,0.7279999852180481,0.7360000014305115,0.7319999933242798,0.7319999933242798,0.7120000123977661,0.7319999933242798,0.7519999742507935,0.7360000014305115,0.7400000095367432,0.7360000014305115,0.7279999852180481,0.7200000286102295,0.7360000014305115,0.7480000257492065,0.7239999771118164,0.7360000014305115,0.7400000095367432,0.7279999852180481,0.7200000286102295,0.7279999852180481,0.7360000014305115,0.7400000095367432,0.7160000205039978,0.7160000205039978,0.7400000095367432,0.7360000014305115,0.7279999852180481,0.7279999852180481,0.7319999933242798,0.7319999933242798,0.7239999771118164,0.7440000176429749,0.7360000014305115,0.7160000205039978,0.7319999933242798,0.7519999742507935,0.7400000095367432,0.7360000014305115,0.7200000286102295,0.7239999771118164,0.7200000286102295,0.7360000014305115,0.7519999742507935,0.7480000257492065,0.7360000014305115,0.7239999771118164,0.7279999852180481,0.7360000014305115,0.7279999852180481,0.7440000176429749,0.7519999742507935,0.7360000014305115,0.7279999852180481,0.7360000014305115,0.7200000286102295,0.7319999933242798,0.7360000014305115,0.7480000257492065,0.7360000014305115,0.7279999852180481,0.7400000095367432,0.7400000095367432,0.7400000095367432,0.7360000014305115,0.7400000095367432,0.7519999742507935,0.7080000042915344,0.7360000014305115,0.7440000176429749,0.7400000095367432,0.7319999933242798,0.7200000286102295,0.7279999852180481,0.7319999933242798,0.7519999742507935,0.7400000095367432,0.7279999852180481,0.7319999933242798,0.7480000257492065,0.7480000257492065,0.7239999771118164,0.7559999823570251,0.7279999852180481,0.7200000286102295,0.7400000095367432,0.7360000014305115,0.7239999771118164,0.7360000014305115,0.7519999742507935,0.7480000257492065,0.7200000286102295,0.7279999852180481,0.7360000014305115,0.7319999933242798,0.7440000176429749,0.7519999742507935,0.7319999933242798,0.7239999771118164,0.7360000014305115,0.7400000095367432,0.7279999852180481,0.7319999933242798,0.7400000095367432,0.7400000095367432,0.7360000014305115,0.7239999771118164,0.7360000014305115,0.7480000257492065,0.7200000286102295,0.7360000014305115,0.7360000014305115,0.7319999933242798,0.7440000176429749,0.7480000257492065,0.7279999852180481],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_acc\",\"y\":[0.5400000214576721,0.5239999890327454,0.5479999780654907,0.5759999752044678,0.5720000267028809,0.5120000243186951,0.5120000243186951,0.527999997138977,0.527999997138977,0.5519999861717224,0.5440000295639038,0.5440000295639038,0.5600000023841858,0.5759999752044678,0.5559999942779541,0.5640000104904175,0.5759999752044678,0.6079999804496765,0.5920000076293945,0.5759999752044678,0.5839999914169312,0.6000000238418579,0.5960000157356262,0.6000000238418579,0.6039999723434448,0.6039999723434448,0.6039999723434448,0.6079999804496765,0.5960000157356262,0.6039999723434448,0.6119999885559082,0.6079999804496765,0.6000000238418579,0.6039999723434448,0.6039999723434448,0.6200000047683716,0.6359999775886536,0.6399999856948853,0.6399999856948853,0.6159999966621399,0.6159999966621399,0.6359999775886536,0.6320000290870667,0.6119999885559082,0.628000020980835,0.6320000290870667,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.628000020980835,0.6159999966621399,0.6200000047683716,0.6200000047683716,0.628000020980835,0.628000020980835,0.6399999856948853,0.6359999775886536,0.6439999938011169,0.6359999775886536,0.6320000290870667,0.6240000128746033,0.6240000128746033,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.628000020980835,0.6320000290870667,0.628000020980835,0.6399999856948853,0.6240000128746033,0.6320000290870667,0.6359999775886536,0.6480000019073486,0.6240000128746033,0.6320000290870667,0.6359999775886536,0.6320000290870667,0.6359999775886536,0.628000020980835,0.6439999938011169,0.6240000128746033,0.6240000128746033,0.6240000128746033,0.6359999775886536,0.6240000128746033,0.6359999775886536,0.6320000290870667,0.6399999856948853,0.6359999775886536,0.6480000019073486,0.628000020980835,0.6240000128746033,0.628000020980835,0.6439999938011169,0.6240000128746033,0.6320000290870667,0.628000020980835,0.6359999775886536,0.6359999775886536,0.628000020980835,0.6200000047683716,0.6480000019073486,0.6320000290870667,0.6320000290870667,0.6320000290870667,0.6399999856948853,0.6320000290870667,0.6320000290870667,0.6399999856948853,0.6320000290870667,0.6359999775886536,0.628000020980835,0.628000020980835,0.628000020980835,0.6399999856948853,0.6399999856948853,0.628000020980835,0.6240000128746033,0.6439999938011169,0.6240000128746033,0.6359999775886536,0.6320000290870667,0.628000020980835,0.6320000290870667,0.6320000290870667,0.6480000019073486,0.6320000290870667,0.6200000047683716,0.6320000290870667,0.6439999938011169,0.6439999938011169,0.6240000128746033,0.6240000128746033,0.628000020980835,0.6399999856948853,0.628000020980835,0.6359999775886536,0.6439999938011169,0.6320000290870667,0.6359999775886536,0.6480000019073486,0.628000020980835,0.6320000290870667,0.6359999775886536,0.6320000290870667,0.628000020980835,0.6320000290870667,0.6320000290870667,0.6320000290870667,0.6480000019073486,0.628000020980835,0.628000020980835,0.6240000128746033,0.6240000128746033,0.6439999938011169,0.6320000290870667,0.628000020980835,0.6320000290870667,0.628000020980835,0.6399999856948853,0.6399999856948853,0.6320000290870667,0.6320000290870667,0.6399999856948853,0.6439999938011169,0.6320000290870667,0.6359999775886536,0.6399999856948853,0.6240000128746033,0.628000020980835,0.6439999938011169,0.6359999775886536,0.6240000128746033,0.6399999856948853,0.6359999775886536,0.6480000019073486,0.6359999775886536,0.6320000290870667,0.6359999775886536,0.6480000019073486,0.6359999775886536,0.6240000128746033,0.6200000047683716,0.6439999938011169,0.6480000019073486,0.6320000290870667,0.6320000290870667,0.6320000290870667,0.6359999775886536,0.628000020980835,0.6320000290870667,0.6240000128746033,0.6320000290870667,0.6320000290870667,0.628000020980835,0.6200000047683716,0.628000020980835,0.628000020980835,0.6359999775886536,0.6399999856948853,0.6480000019073486,0.6359999775886536,0.628000020980835,0.6399999856948853,0.6399999856948853,0.628000020980835,0.6240000128746033,0.6399999856948853,0.6399999856948853,0.6240000128746033,0.6320000290870667,0.6439999938011169,0.6439999938011169,0.6320000290870667,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.628000020980835,0.628000020980835,0.6359999775886536,0.6359999775886536,0.6480000019073486,0.6439999938011169,0.6320000290870667,0.628000020980835,0.6399999856948853,0.6359999775886536,0.6320000290870667,0.628000020980835,0.6240000128746033,0.6399999856948853,0.6399999856948853,0.628000020980835,0.6399999856948853,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.6439999938011169,0.6320000290870667,0.6439999938011169,0.6359999775886536,0.628000020980835,0.6399999856948853,0.6320000290870667,0.6439999938011169,0.6399999856948853,0.6480000019073486,0.628000020980835,0.6320000290870667,0.628000020980835,0.6399999856948853,0.6359999775886536,0.628000020980835,0.6439999938011169,0.6359999775886536,0.628000020980835,0.6439999938011169,0.6399999856948853,0.6320000290870667,0.6439999938011169,0.6399999856948853,0.6320000290870667,0.6320000290870667,0.6439999938011169,0.6320000290870667,0.628000020980835,0.628000020980835,0.6399999856948853,0.6439999938011169,0.628000020980835,0.6320000290870667,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.628000020980835,0.6359999775886536,0.6240000128746033,0.6439999938011169,0.6359999775886536,0.6320000290870667,0.6439999938011169,0.6399999856948853,0.6359999775886536,0.6200000047683716,0.6439999938011169,0.628000020980835,0.6320000290870667,0.628000020980835,0.6320000290870667,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6439999938011169,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.628000020980835,0.6320000290870667,0.6320000290870667,0.6399999856948853,0.6240000128746033,0.6439999938011169,0.6359999775886536,0.628000020980835,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6320000290870667,0.6399999856948853,0.6439999938011169,0.6439999938011169,0.6399999856948853,0.6399999856948853,0.6240000128746033,0.6320000290870667,0.6359999775886536,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6439999938011169,0.6359999775886536,0.6320000290870667,0.6439999938011169,0.6399999856948853,0.6320000290870667,0.6320000290870667,0.6320000290870667,0.6240000128746033,0.6320000290870667,0.6399999856948853,0.6359999775886536,0.6320000290870667,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6439999938011169,0.628000020980835,0.6399999856948853,0.6399999856948853,0.628000020980835,0.6320000290870667,0.6359999775886536,0.6320000290870667,0.6320000290870667,0.628000020980835,0.6320000290870667,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.6439999938011169,0.6359999775886536,0.6359999775886536,0.628000020980835,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6320000290870667,0.628000020980835,0.6320000290870667,0.6359999775886536,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6359999775886536,0.6320000290870667,0.6399999856948853,0.628000020980835,0.6240000128746033,0.6359999775886536,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.6359999775886536,0.6240000128746033,0.6399999856948853,0.6359999775886536,0.6320000290870667,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6399999856948853,0.6320000290870667,0.6399999856948853,0.6359999775886536,0.6320000290870667,0.6320000290870667,0.628000020980835,0.6399999856948853,0.628000020980835,0.6320000290870667,0.6320000290870667,0.6359999775886536,0.6359999775886536,0.6399999856948853,0.6240000128746033,0.6439999938011169,0.6359999775886536,0.6399999856948853,0.6399999856948853,0.6320000290870667,0.6359999775886536,0.6439999938011169,0.6439999938011169,0.6359999775886536,0.628000020980835,0.628000020980835,0.6359999775886536,0.6359999775886536,0.6399999856948853,0.6320000290870667,0.6240000128746033,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6439999938011169,0.6320000290870667,0.6320000290870667,0.6359999775886536,0.6399999856948853,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.6439999938011169,0.6399999856948853,0.628000020980835,0.6399999856948853,0.6359999775886536,0.6399999856948853,0.6439999938011169,0.6439999938011169,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6439999938011169,0.6359999775886536,0.6439999938011169,0.6399999856948853,0.6320000290870667,0.6320000290870667,0.6439999938011169,0.6399999856948853,0.6320000290870667,0.6359999775886536,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6320000290870667,0.6439999938011169,0.6320000290870667,0.6439999938011169,0.6480000019073486,0.628000020980835,0.6240000128746033,0.6359999775886536,0.6359999775886536,0.6320000290870667,0.6359999775886536,0.6399999856948853,0.6439999938011169,0.6359999775886536,0.6399999856948853,0.6320000290870667,0.6399999856948853,0.6399999856948853,0.6359999775886536,0.6359999775886536,0.6359999775886536,0.6359999775886536,0.6320000290870667,0.6320000290870667,0.6320000290870667,0.6359999775886536,0.6439999938011169,0.628000020980835,0.6359999775886536,0.628000020980835,0.6320000290870667,0.6359999775886536,0.6439999938011169,0.6359999775886536],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('67fbd286-05b5-4feb-9820-0712c1426bca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในการวิเคราะห์ปัญหา Overfitting เราจะพิจารณาจากกราฟ Loss เป็นหลัก ซึ่งจากกราฟ Loss ด้านบน พบว่ายิ่งมีการ Train มากขึ้น ค่า Training Loss จะลงอย่างต่อเนื่อง ขณะที่ Validation Loss จะลดลงถึงจุดหนึ่งแล้วหลังจากนั้นกลับมีการเพิ่มค่าขึ้นเรื่อยๆ"
      ],
      "metadata": {
        "id": "KDxN0Pcig8Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Good Fit Learning Curve\n",
        "Good Fitting เป็นเป้าหมายในการ Train Model ซึ่งกราฟแบบ Good Fitting จะบ่งบอกว่า Model มีการเรียนรู้ที่ดี เราสามารถนำ Model ไป Predict ข้อมูลที่ไม่เคยพบเห็นได้อย่างแม่นยำ หรือเรียกว่า Model มีความเป็น Generalize ต่อ Data ใหม่ๆ (มี Generalization Error น้อย)\n",
        "\n",
        "เราจะจำลองสถานการณ์ของ Model แบบ Good Fitting ด้วยการพัฒนา Model เพื่อ Classify ข้อมูลจำนวน 3 Class ตามขั้นตอนดังต่อไปนี้"
      ],
      "metadata": {
        "id": "08VLhUglg8P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สร้าง Dataset แบบ 3 Class โดยใช้ Function make_blobs ของ Sklearn"
      ],
      "metadata": {
        "id": "4AUX7WGYhChw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_blobs(n_samples=3000, centers=3, n_features=2, cluster_std=2, random_state=2)"
      ],
      "metadata": {
        "id": "X10xd2MphB9B"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "แบ่งข้อมูลสำหรับ Train และ Validate ด้วยการสุ่มในสัดส่วน 60:40"
      ],
      "metadata": {
        "id": "TfqAYLeihFgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.4, shuffle= True)\n",
        "\n",
        "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "id": "bNuFxi4ThD2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7c2227-7f6f-4431-8f1d-5b851b059c68"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1800, 2), (1200, 2), (1800,), (1200,))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "นำ Dataset ส่วนที่ Train มาแปลงเป็น DataFrame โดยเปลี่ยนชนิดข้อมูลใน Column \"class\" เป็น String เพื่อทำให้สามารถแสดงสีแบบไม่ต่อเนื่องได้ แล้วนำไป Plot"
      ],
      "metadata": {
        "id": "gw8p4E2lhIHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.DataFrame(x_train, columns=['x', 'y'])\n",
        "y_train_pd = pd.DataFrame(y_train, columns=['class'])\n",
        "\n",
        "df = pd.concat([x_train_pd, y_train_pd], axis=1)\n",
        "df[\"class\"] = df[\"class\"].astype(str)"
      ],
      "metadata": {
        "id": "wvcZuRnKhIqc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"class\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dO_9wh5KhSzQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "092198b6-2435-4c68-92cb-f1a7b7dc1aa7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"50b3632b-9e02-419e-846d-fdda8eb64c1c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"50b3632b-9e02-419e-846d-fdda8eb64c1c\")) {                    Plotly.newPlot(                        \"50b3632b-9e02-419e-846d-fdda8eb64c1c\",                        [{\"hovertemplate\":\"class=0<br>x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"0\",\"showlegend\":true,\"x\":[-0.4815693097877277,0.43629689871790056,0.0028343011889495617,-3.6802561868975103,-0.4642052318190837,0.3794761346102917,-1.626478359226932,-1.9071183511467424,2.7078306365260643,-1.2840077820827143,0.9033153032739518,-0.34651790837018337,-0.5021344897978153,-6.319260549897683,-2.417325963113064,0.9381846055633711,0.3472508321565566,-2.8724647468277107,-0.9298205124763688,0.3949038248155705,-1.2600332166262822,0.3610267073017277,-2.6069394490679194,-2.5010011736099678,-0.17719386806707615,-2.3254121372083487,-0.7180056534331714,1.1142577137103085,-0.8313676490019144,-1.9693492985236782,-0.42481988044423524,-1.6144395058606849,1.042941024321621,-0.5270101350918427,-2.586602492743996,0.40312693153260537,-4.045278786861028,-2.337053749192342,2.949304014323502,-2.031440803317912,1.3425102114597758,-0.3898481941497811,0.810854018117086,-4.740063094795341,-0.6020269970800441,2.22289161253819,-1.9577458892183175,0.39732401385620064,-5.1693135171202655,-1.865520713226796,-1.7321650839636904,-1.5631487044519754,-2.179448253943589,-4.338868141602584,3.044822867084374,-0.19027835507844593,0.7997401138377733,-2.465009737021023,-0.19416346906831183,-3.4775798511520377,1.1203757472703701,-1.710950259437543,-2.0269437057673065,-3.839215724860959,2.3285222356596793,-0.4324600359009131,-7.933842172073015,0.6579084207220396,-5.012713985961383,-5.326829835175633,-1.722462809092431,-0.5315225066007583,-2.1947196139813916,-0.5245836175555425,-0.13803491939173407,-2.279819515285988,-3.0845940933009284,-3.1580999526376745,-1.7943518956085627,0.9198470722717613,0.4500128151521019,-1.0151001422434747,1.0140979276639577,-0.7247400296458929,2.0873857255201944,-0.41612399530803756,-0.08264673671840472,1.3910303795215189,-3.7333080541616965,-3.012353170973358,-3.4091232605167434,-2.340380773691214,0.09457015264598478,-1.0445088576492612,-4.936812720283781,0.45909443792996507,1.529873786035564,1.6588561467338416,-0.09148364307796597,-0.9460045801817913,-0.5461657612351324,-1.4746077972605567,0.35323230237606706,-1.614049634945301,-0.5208455868394503,1.2167986388800291,-1.1591770591541006,-2.3550893969503433,-0.21073804451533018,-2.0878864958358037,-0.7246583233467173,-0.9276650712802956,-1.919593641257411,0.98964958203387,-1.8922401966218043,0.4438203388549502,0.023730618394316005,4.7387604199532225,-3.1031411928092636,-0.494655630948142,-2.9323046719490904,-0.7046076124476787,-2.560939776413767,0.27885815158707095,-1.0304546643882357,0.7859561746953179,0.3555992265459844,-1.306781459861665,-2.9971741117977,-5.487218642346384,-2.9077535870572158,-2.97743395936125,-1.1093730903146244,-1.9046432964772122,-0.7863869402263096,0.9795948647670008,-1.0134432012713053,-2.635453111816008,-0.3692422351904776,-0.24242970333697822,-0.7000853130180247,-0.7972226994765401,-5.386996168211382,-1.6300956446294448,2.340961365686558,2.5862119755382307,-0.05981378434661,1.2846283505226785,-5.371835316342597,-2.4047235766768313,-1.5763963395145284,-1.5270496498117814,0.12392509265578844,-0.610910559953711,-0.23818942053993641,-2.9041350206278436,-2.5717677383300748,-0.4561505786897082,-0.5190042371177632,0.43386248734883526,-0.1798259246911349,-3.494979409711199,-2.665898491164781,2.9508283972394933,0.3175186612080976,-1.8486252649956985,-0.7690180214421857,-1.6563927366564646,5.103700536130886,-1.6468554272729876,-2.05335351893535,0.17060456155419224,4.110752771278311,-1.6619510864705151,0.9109825613517479,1.1197153902999966,-2.9626840033917183,-0.42174681380727774,0.7790461209772221,-1.6851247217722978,-0.7303691161132598,-4.524790422449003,0.26530552249598216,-1.1195607205403166,-1.126804153411182,-2.360393983560966,-3.04551960030829,-4.679085493983308,-1.722743842622509,-2.075513469331397,-1.7763886280620302,-3.253241287017609,-4.233590120080056,-2.7401581925712186,-3.052540207461658,-1.1478491234909194,3.5504116016017653,-3.6701017470096917,-4.109259268686269,2.5512558070847358,3.4808669637028986,-0.2889251664675767,-1.8471409898865465,0.09554636068188982,2.8024930836171764,0.9556715097417587,-5.539840366768593,0.14402049769178138,-2.030888363544216,-2.2167944762763887,1.2974368565638281,-1.508944551220166,-2.592699643731791,-4.245032912805266,-2.5420284925775323,1.066961039829982,-0.8452419884148437,-1.5824571192445722,-2.1049392795274864,-0.3722727883446878,1.7138016218850471,-2.7187995954632367,0.19133943647008755,0.22721390159071375,-4.289769877398697,-1.1050072707436567,-0.8428704404257267,-0.2535990685412588,-1.160640050557048,-0.7570932850355507,-1.2098814819097454,-0.2951309564519782,1.4475486223237581,-0.5081433185112448,-0.17477456595152674,-1.6235544495766754,-3.2080323838194964,2.3381880130661172,-0.5922165090756706,1.311093141476288,-4.122536411768179,0.9765515205651494,0.583672758624699,-1.420057574538014,-4.916606870696131,0.26259352580062223,-3.4459610715541147,-1.5792846624557217,1.5289575356072125,1.2441841247024388,-0.5081058061359611,-1.921971372824701,-4.019379302412462,5.279388845062874,3.3726798450824402,-3.3382461263933783,0.005806530490289674,-2.4676131229251386,0.8055081255865244,-3.9387948421689667,0.5855818340162207,-0.07707032982552531,-4.081792866783047,0.5384639175981096,-1.9737486266894888,-0.08040503585211112,-1.9000683069151594,-1.5728912413599325,-4.855514027587796,-2.1089191522185162,0.13508377492195356,1.9620839981862863,-0.5748764820835445,-0.9737017249275158,-2.009060140385242,-1.5889218818761535,2.8703527022541184,0.3596682759088421,-3.3960063948846027,2.256492906330394,-1.8555093764153776,-0.8640959165890105,-0.3890235614204518,3.3341286042244294,1.1944559868260978,-1.6041409784679965,-2.2701532670527564,-3.841892313045266,-2.0065509934576315,-1.7854383705974244,-4.584502539643086,-2.79855264042565,0.44696224415153285,0.21509592639861075,-1.4270750518375437,-3.722960287351462,-3.414236204726571,-0.7212344440701802,2.3817913568093316,-0.10680316209982887,-1.748383955419386,-2.1003754406052577,-2.0311427010159466,-0.430499871553673,-2.5554119821285273,0.29801734000470215,-2.7321665007814846,-3.422770390158008,-0.8969546639443686,-6.599000869926901,-2.1885576665921453,-0.673633091342461,-1.572030779040097,-2.5949357499033816,-1.633813734724938,-0.24189084299568986,-3.057715044594178,-1.6468165700644406,-5.239852497405887,-1.2390917193666229,0.2244383736756761,0.3788149040668629,-1.214971018534942,-1.7527487392019052,-2.3255856629780265,-1.4335587470944686,-1.4664976898120996,-0.49870492220514906,-0.852658088819922,-1.387192469128939,0.022685348148403506,-0.9432565791883603,-3.350601375875139,-6.149637110202013,-0.705468180975456,-1.454574851239366,0.6405424587775406,-2.5132755949825576,-1.3431967029671623,-0.817467723982169,-2.9800929625900565,-3.54636850451389,0.2516329605175325,-0.9884218108722822,-3.5821078454422217,0.8166753596643193,-3.645858748106653,-2.4174922317130725,-1.4109213178380755,3.8221742847198783,2.305083747636643,-2.2896387053980343,-1.8775536435862508,-1.2953042339865162,0.5543389981990263,0.157252611081814,-1.3592424365471363,-2.4362802546780546,-1.697509215384954,-3.1379998488141023,-2.7843684695274558,-1.8961074202759765,-3.567703618953528,-0.7742537253435596,-1.4090073027221437,2.0987553865923685,-4.282182136774994,2.513762529944589,-2.2414727083379136,-2.3967348239927406,-0.701596885759401,2.074986170923874,-2.768489682295151,-4.200409906085603,-3.002988468321471,-1.428107906908097,0.048340869631335215,-2.6764439523257106,-4.512453160937389,-0.5191580167299239,0.6902340237265379,-2.953966866790323,0.8915108508842686,-1.1770467904283248,-0.6816806179145833,-6.110039344239311,-3.6097246089334374,-4.205458648481045,-0.907058917187899,-2.4811427679785676,-2.5871848452290176,0.23301422889997547,-1.6080276218391143,-0.6082851669412054,-2.9491323719308813,1.3014339665274943,0.3693040537140384,-3.402833445021601,1.3221388879488623,-5.666035362574871,-0.49575626858249067,-3.4789262061396538,-0.18557835013829949,-4.186830656473502,-6.679313006210247,-4.316847210335274,-0.15476380090120911,2.5450993998216265,-3.124989214498224,1.3488478976605607,0.22821998832412715,-2.869846846285795,-1.911488377765472,-0.7510564464266044,-0.44356913558421496,-4.614056212921314,-2.4459621683855124,-0.2463106089666487,0.7762678411523627,-1.17241420204303,-2.2058337098965985,0.600803643801435,-3.086823777479162,-0.20198531599834713,-3.478696575416773,1.1383395444759983,-1.94145364413508,0.6414619325265314,-1.6857834321976446,-2.396875908191117,0.350310125714828,-0.6370901003509242,-3.401700029820789,-2.4554606495648033,-1.8068781112609011,-1.895972735681554,-2.3876950108834825,-4.145847427381865,-0.44805845902439967,-0.5570889238176943,-2.8437004032329654,-1.403372126404024,0.7403084104342694,-5.1593613470939195,0.2989431636609241,-3.010521885593248,-1.486806293761275,2.459495270086186,-0.6976339371586358,-5.61733895496647,-4.547869219959035,-2.353589695852338,0.7032186206815902,-0.6566700284407051,1.1990256677789723,-1.8729190995660971,-0.10769376171839262,-1.2603498191970055,-2.6806581318856533,-3.4089080701764343,-0.601182413909527,-0.9997936132715894,-0.7669610531573335,-1.215112520717759,-3.0039081106125303,-3.8303270649254464,-2.469546954998063,-0.2444296434112081,-3.1018415065461777,-3.6091555561573365,-0.0126803391398429,-1.5739883324575552,-0.9746814384963565,0.9467304971988435,-4.2180180048423,-1.486024019682124,-2.6453491585352777,-0.2800560345360419,-0.9777076065260638,1.045218856338753,-3.2576580681036016,-0.6495497934802454,-0.6250145086766978,-0.5618352935723158,0.4615795720564937,-3.164171180883767,-1.2800824342167325,-1.3379736484332794,-3.516423875064802,-1.4407131412348528,-0.5481839902634642,-1.8261474487118756,0.08941798073789409,0.7069092317981527,-0.8537846012682246,-0.7754855078053776,-2.140773064519075,0.5315873555699411,1.5571535478034857,0.2836795089500055,0.9664426331255989,-0.5892032170526116,-1.1144569965370164,1.6222378424031079,-1.5942391584784743,-3.465434564385834,-2.5174895921919034,-1.4873999175953867,-1.9739830207485212,-2.2176149027624126,-4.63752144525895,1.4938782676772138,-0.8875761689966055,-1.5932064820443261,-2.9769308720300423,-2.294249561662126,-2.515003378137217,-0.4454835105867425,-3.9006430150555556,-2.3555887331252086,-3.5487389232211615,-4.513172763859412,-0.643460489600506,-4.05444078381505,-0.5499949242782435,0.7206298159414652,-0.6374303655997624,0.46855437177373327,-0.7898179876958388,-2.5168529273352824,2.8786513453131963,-1.349472689162355,-4.020949922989067,-2.3405874624078105,-0.49019551640061776,-6.142440338337083,-0.5469155617469356,1.969149298027705,-0.20331472853631483,1.2103723943934965,-1.9968469115263754,-4.440241002535247,-0.057229207211918265,1.0989543166487414,-0.9199685872894987,-2.497930348753709,-1.7644259882863453,-1.3418626089247245,0.11188485324195874,-2.6772818570327024,-0.5336479956746085,-1.4227184509501742,-5.113362995178168,-1.2082335971531017,0.1475677550021397,0.3583117102595976,-1.2845148479792483,2.644637335486971,-3.8624000504830107,-1.879769447560594,-0.9801004277251959,-2.1348038287185496,-2.6570040988241095,-1.2760477724970147,-0.4995090575384007,-3.775046820382604,-0.8682510427266532,1.7973700306944829,0.30630173734452604,0.27662979824849265,-1.5613597716495415,-1.7173377164490047,-1.2753832885127607,-0.11399460442281106,-2.9383725351894823,1.5041582730403409,1.289030149907469,-4.9809099047428775,-2.8501942182087925,1.1898618686291544,-0.2920302152894111,3.927297378544778,-2.855601037798526,-2.5956386820577624,-2.2041126534666855,-1.3818353024317467,1.8736783991210628,-0.1560196819796289,-2.9687184383909875,0.17801613773488278,1.2150879075403997,-2.221715491373722,-3.0463530793877296,-2.023140205871541],\"xaxis\":\"x\",\"y\":[-5.187694629701332,-10.205987338880872,-9.919551518272913,-9.052421121736376,-9.468654950616427,-8.809343460142458,-10.360645734958302,-7.939451887303351,-7.5148349000402845,-11.944804344828404,-8.533153657189109,-11.455396296033571,-10.735329418218457,-6.699150508603411,-12.768560986221404,-13.011310819993685,-9.305213248845304,-7.819718294728405,-8.18877471471923,-8.90066102408466,-8.523866145546641,-10.459789365963424,-10.047249507965976,-13.023299633250224,-4.897059337812259,-10.33949989014245,-10.79950817083881,-9.057901007919577,-8.536955761655936,-13.485633829735686,-7.991194119781419,-10.634697224012243,-8.709319268234985,-9.752434892048537,-7.796562800299494,-8.592904238483609,-10.974113343140754,-7.636450722563362,-9.7636908496226,-9.630416889230135,-13.481628755062975,-10.186007162316086,-6.743443314640237,-5.258867120472099,-9.495130351767335,-8.04280600741787,-9.953843425147376,-9.18181946335536,-10.980496425978838,-6.311603412179228,-8.507074883505542,-9.50518681979434,-10.759124922453642,-9.750997515168768,-8.16637286965233,-9.397613461345815,-6.519777264943993,-12.061342194793488,-8.311323275591855,-6.312501250650821,-8.888494594603413,-10.370384906256156,-7.910157804513137,-8.01617533661443,-9.288682734344707,-9.103553373307346,-10.925608764164735,-10.12285771763052,-15.75456955592865,-8.945530210484911,-7.107402526927572,-8.578607453637046,-10.952756409129183,-10.15501864391058,-10.603472564828465,-6.539010757584637,-10.663419914381064,-8.223923749042873,-9.755431955618867,-8.256120380711073,-9.96943533998011,-10.173460205911995,-9.290361394418426,-9.691575970695267,-6.252157463071908,-11.517200821561309,-9.952742429342086,-8.876216691085975,-8.76887173235253,-6.680430408111887,-9.993131008931478,-11.99988939416894,-10.63010055511786,-10.554845981117197,-9.028930595576057,-11.655656513127056,-9.927676701351697,-11.08945090320221,-9.637690038213702,-6.398899598560494,-9.44004719659643,-7.441452051273361,-9.34983054251068,-9.32328917397703,-11.319395934456427,-11.972928469550874,-12.15865312317922,-10.883038034312351,-13.171864492032135,-12.250512696513903,-11.642825911862682,-8.506319744705962,-10.595704557450583,-10.499324437766576,-10.59974173749819,-9.03341706286472,-6.762886627596984,-9.178181178914803,-12.58154742868237,-9.756552422454472,-9.322626708933369,-8.072573593424108,-9.666121942322667,-7.428279462312084,-7.968471077337069,-10.139480232843587,-9.900219051173963,-11.018121862628822,-12.679285167388727,-10.984225046927085,-10.046067964420443,-8.818396067228393,-8.150806807597721,-5.1921191852201,-12.419026962016737,-9.716992500968336,-8.285041356988508,-12.36035341691941,-9.711264419378269,-8.752579353149525,-8.232924002142688,-8.4866240655927,-9.957505123295142,-12.355762227903734,-6.934363783340194,-11.623068014716521,-8.634481562556429,-8.783160108877773,-8.492487631904464,-9.245648198414674,-6.478601532929018,-11.640643216447666,-10.193747224715151,-11.669808457113787,-12.584395868495296,-6.388797919546311,-13.615279162606658,-9.366304062271873,-12.686103850113419,-5.7245319340983,-12.731922361282688,-9.83404764594837,-7.742675010045245,-8.027900714772601,-12.924534396637933,-10.306185836214016,-10.99849840852024,-9.029553210194571,-12.153929095280636,-11.80108915599539,-11.250630770028872,-10.200424281732234,-7.685795999462801,-10.73198239966592,-11.889811301259401,-9.441573452201844,-5.750284366589805,-10.875772075454117,-9.168349017450758,-10.81649186938831,-6.210572017053735,-11.336833703159703,-7.356836736062557,-7.894426171720738,-11.070923024197292,-9.990497785404372,-7.711986975039438,-13.188053093618686,-11.827786606327741,-9.480217995220578,-9.294331789185938,-6.394402685235034,-9.72517960393092,-8.361431246447601,-11.633259911169572,-8.410358662275033,-10.337362044102983,-10.457336451825903,-10.398331734925488,-12.802285045006276,-11.23263802355907,-10.02511761609506,-8.404626837860935,-8.698109634801698,-8.553966826902428,-10.029959541169225,-11.21572637283719,-13.082907445014017,-11.327747159441596,-11.990915977667884,-9.211624439100294,-10.47782375128569,-9.074038417360628,-7.788851579758014,-9.342260740316036,-14.168681745442097,-10.48872472799206,-8.330824391851095,-11.122472702720186,-5.344053601149941,-12.543697578627054,-11.772186230973153,-9.13502434395425,-8.1822030070662,-10.534407208342127,-10.491160337424061,-11.731386606733425,-10.15116361435761,-11.474125673090779,-12.966244734805127,-9.087904823502779,-9.338915621653545,-12.586991029097604,-8.953636768007582,-11.162361388425168,-7.290619834348579,-10.930230182369021,-9.871551181696654,-9.805482073563265,-10.587088719313655,-9.788465754796071,-9.347481069396583,-6.544811098002875,-11.589954631012889,-5.408175486054118,-9.01316161612175,-11.394726403572042,-9.966356563705581,-10.484500333142279,-7.833486940367531,-9.885452852200352,-8.98244826397358,-4.140800873842785,-7.5397527233616275,-10.749581618715085,-8.305616173151062,-11.344902857175324,-7.182474228006209,-11.165684652447613,-10.972668191947413,-8.601922600132383,-9.474156382013234,-8.866424310047652,-11.845686095054736,-11.613107943857935,-6.16773689068566,-11.588139385790086,-8.334789546425212,-7.840733290860305,-9.346848538738131,-9.253367607587457,-8.056108816483178,-9.230229165438043,-12.94881400716724,-11.233434318772963,-12.980052971066737,-8.068984131230767,-10.743712728612365,-11.299490593295872,-10.248445729642409,-9.421355251732473,-9.05195632449634,-10.129322259306093,-10.750557663685179,-9.599810304903105,-8.01243752565318,-8.806445936805082,-8.861782808672672,-9.72315191470018,-10.138324605579578,-10.832798941443835,-8.26059984093245,-11.291239123636117,-9.947269365539729,-7.226687525646292,-5.872522080282723,-10.787337653718732,-13.125937900323848,-7.601123377966716,-9.819195062111095,-13.986845926946645,-9.921359696455669,-10.39327690854487,-7.654464396243158,-11.856699936212742,-12.724822969404835,-8.827840963255476,-8.364049922842325,-10.805893387030672,-9.66438060922349,-8.452733928511277,-11.141225052862984,-11.370071914845598,-11.21745831838066,-4.12218689374216,-8.337627052012001,-10.662831268314129,-9.092803590525408,-14.080418303758474,-11.702460481232944,-8.487881407115932,-8.220231428887933,-12.762595813051856,-6.345465358623408,-10.517300128822116,-8.977817270820745,-9.210212531330031,-9.556515959948783,-10.548159567660884,-7.272923481354846,-12.397750860767081,-10.071953010941778,-8.064420297036419,-9.256022353808875,-11.884207037620286,-10.877609242569445,-9.945972427502003,-5.866169432785668,-8.43786142218529,-10.329977078940342,-11.186158956819009,-8.615290254128793,-10.013948839434535,-10.28168747639649,-10.334520305152713,-8.815950317193925,-7.352853595085569,-10.250542032519926,-7.949243556371983,-7.5932956693868014,-10.339502000461863,-11.380853124481625,-8.005441210152954,-11.34913668627596,-6.92982156180455,-8.161474336319213,-8.829468496668213,-9.03409479354888,-9.206239380832251,-7.521413686266337,-10.130511145416687,-12.139305649055647,-8.888085092106046,-10.638848197928787,-9.668783944675367,-10.745400316444139,-14.512354275730559,-10.739633665039186,-8.932075449792235,-12.416638208454186,-8.680880389744416,-12.14379882016423,-7.731454056492074,-5.239909721660725,-11.441631698742626,-11.454468091391455,-8.01170602796027,-9.0015729668964,-9.611119347413146,-9.915745900931933,-8.345154726130602,-10.296388210175804,-7.677116572560994,-9.567329198591285,-8.12294100739416,-4.946927554908354,-12.886366725970197,-10.715164304112022,-9.798333811819054,-6.2517288893156575,-6.663807395517225,-7.783438415819974,-8.738231040921445,-5.672723545081668,-11.47539473759053,-8.753258126478544,-6.985049522248106,-9.698509011280143,-11.48408566970796,-8.28254413258432,-8.323953377429408,-9.604848434348899,-10.014111100217036,-10.15751091815463,-9.237920280316045,-6.563533421130163,-9.810505478029086,-9.572001769706302,-10.519424733858601,-10.263472012610043,-9.409624798531947,-10.292047897080634,-12.181479568773263,-8.029750710806757,-7.939439376743179,-9.030839759022205,-11.784370735667187,-8.980226723778038,-10.92523481566198,-6.820640207008971,-9.910380863950747,-9.130192112810084,-9.496759985514307,-10.673794763055108,-10.844536652370225,-11.36892712909157,-8.907058959122011,-7.964734669108634,-8.139181837289456,-8.937295605615732,-13.63132009376771,-10.094894985589802,-8.298142321986816,-11.486708454865724,-10.836196346928322,-9.915137892494622,-9.809733409706364,-10.617706625654375,-9.597169888725974,-5.118012989936136,-7.9434205716376525,-11.387269667695492,-10.366945838429901,-10.736759938008062,-9.642971278318312,-5.977839040499926,-7.78477422964546,-10.940240636053193,-9.876137259691626,-10.811835695610513,-8.301686755700795,-11.946553649809275,-7.895493927455673,-10.963578108180265,-10.960568064927338,-8.310755166030201,-9.522249946986333,-7.854608015691058,-8.29548977691552,-10.137964348963358,-7.794689348539038,-9.624262407288212,-11.459033460981416,-11.069831290709526,-10.573286843641744,-11.282475991224242,-10.111099048864801,-10.932106069592177,-5.517148694569582,-11.233673851489616,-11.418474880607276,-6.4026485883197415,-10.022092035078279,-10.968070030285848,-10.393364868174217,-8.143017304961562,-8.316247701038632,-5.905678862041127,-7.951336401517235,-11.357594070829187,-9.008692469724574,-5.749058952916146,-12.103811552841817,-8.237034534506307,-8.295861769879924,-10.94832549637279,-8.396770219144116,-7.136110242313849,-9.622429465202414,-12.06592298580544,-10.486478979083552,-7.500491915551823,-12.89655346454436,-12.836606128023298,-6.376448496093852,-9.796093818177543,-13.382029488902306,-8.7495930700337,-10.941876130425882,-10.319157107107886,-11.230203556768812,-10.247081562636502,-11.656264956913546,-11.611331123599602,-9.2046373283508,-9.917988676605713,-10.390550369494163,-10.338664669702109,-8.658588330765431,-9.51601860819626,-7.237702165796042,-9.893284138142294,-11.233268103472078,-10.325831657236806,-12.198383453667446,-9.997574215884244,-9.550235128364585,-9.7763095317904,-8.754607419683,-8.955021834008821,-10.896317138569747,-7.857167657638372,-7.816589032330213,-10.03480809019432,-7.933311296632776,-10.243660398472874,-11.374369092053993,-10.374808716663086,-12.589372063161068,-8.723886432028225,-10.506462630326794,-8.089087858568165,-7.254415782806166,-9.320614565747393,-11.889102302331622,-10.321057555025005,-12.390394997210763,-8.23002844396135,-10.2597129460809,-7.106647865427674,-11.321636545616073,-9.132908002329215,-6.699163050108616,-9.439072267727934,-9.615337451435208,-9.905863639894708,-11.700491365203774,-11.664619450128507,-5.509325204828457,-11.339259212489813,-9.317611777372644,-8.268557611329978,-13.9361579480371,-7.998247270364442,-12.22131920359559,-11.993099793795544,-9.760059128979426,-6.681201694727634,-5.399639855971051,-11.295841723682463,-8.663875574882676,-5.218982387772924,-6.130258770236141,-9.989942188573776,-7.530456538985044,-9.63656786269928,-8.858603606869076,-11.097405336665563,-10.825025145172793,-9.236108751034706,-9.973650886552285,-10.593035652853802,-8.06997028694115,-8.458420712889392,-9.30605492662551,-11.658978718208791,-10.790627093794136,-8.820499235444334,-11.150804713714445,-10.77138683283059,-6.388920071706886,-7.60964306352485,-8.631968018891591,-11.98437217607083,-8.779698375266682,-12.150299931086476,-12.294874533269402,-10.113663419110964,-10.601115589297942,-7.124284410680552,-6.307870921082307,-9.673414304852642,-6.390701710498476,-3.1595434243504066],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"class=1<br>x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"1\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"1\",\"showlegend\":true,\"x\":[-1.694316201568995,2.97363400452804,-1.795528198907017,2.7272425637128377,2.2635843155435573,-0.1952357824273543,-0.6786988075384943,2.58124232685255,0.13805992903234465,0.4377631193749093,4.512955579057843,1.5406156291582263,3.891814894491314,0.2944043246012519,2.758449332120115,3.937115577993575,0.5320527857513443,2.9746982207413817,-2.1923814311851753,5.492623253413849,4.305866794894973,2.0199692422242608,0.8671657899062009,1.6128753056961977,-2.6052804333094284,-0.22072835632785082,2.0493072785580644,-3.100955463483139,2.423425901401413,1.5135396679087356,3.9051372827265016,3.4351362145754263,3.5578093853543074,3.970941493082175,3.8782471563893206,-1.917196596404156,3.2697700829524137,-0.16132558778715178,0.0012083371007928223,1.9784133166710427,4.718146039093526,0.8906087438953264,3.8748335744304123,-0.1782644381340519,5.099772324842034,-0.9119697258723882,-3.019601185192486,0.965067262630572,2.4513596250750718,1.1538120913562318,1.6186129193399659,2.1591398070698675,0.9087064792689461,0.7591591347990804,4.471517254654129,0.21626325846045358,4.952608968639915,3.682384008927366,4.793147412276374,0.6284742289072366,2.27880941418685,2.5601183395199536,0.7162168182281328,4.890528541844313,1.0757198704695914,4.221865501109894,0.8485026041413307,2.164202437590877,3.6642830234659627,3.4917611293108193,-1.8323754047738667,1.164750344309234,-0.35362904663612205,2.987409139490214,1.1735985776840343,-4.656437118205637,2.521040109993278,0.45867956570646384,1.1723616940743002,3.043100176194717,3.1940354524169314,-1.434583455957323,2.38463008223012,3.1910233409279,0.7731113923993486,0.7457427024022536,0.32404941396172116,-3.7933143839664316,-0.10147099517156444,3.0769065641711153,3.885213597413742,3.69065704990836,2.4421848954850067,-1.124727109130911,1.1346920487168877,2.192038001904523,0.7069651537192019,-1.8733256040823454,0.39463757850281833,1.2980447927211545,1.1136185836860364,0.14237419069836554,1.265269184303645,-2.897703700041267,0.33510117957485785,1.6467868920178148,0.9388396140603966,-0.09967503410594114,-1.1778751388297524,3.0499250671115483,1.119368432562742,1.0516608816730662,-2.154966980101013,1.6751729763910919,1.279526616101204,1.8121600880432358,-1.4294519926540707,3.09773142953174,3.040566062411657,-0.5214831316000053,-2.4954036931456742,-1.7560748331599365,-0.41047657719229735,2.081632152791454,-1.0378823244471982,2.0649412369955433,3.517730928868433,5.358885445324127,4.316586433736541,1.7953584142800334,0.8161251578517472,-0.16228645695835775,4.193270171797639,1.5562889798182207,1.7298130407130703,2.297534733925522,-0.15891198712556864,-0.21403880380263773,2.0672692410985074,1.392942456035439,2.1453601211171875,1.5691128309512568,0.10608694217945003,1.4592162745736825,2.7902222614692516,2.6847451623207985,1.4970861336565062,-0.08500595511254039,3.111964915185262,0.8466309380749455,1.9203689772760564,1.7999083219903569,2.638944829747407,1.2600398460002162,-0.6918113679250995,0.5653464922899586,-2.078499674085353,-2.4173599961750365,-0.5582901852996769,1.7595765991339005,3.011264607379659,6.032390357995185,3.8690109393773544,0.26557696530254626,3.196545489444614,2.274467676413959,0.8550782678256779,0.5905534648375148,-0.8697637242898923,0.04098993427465536,-1.8599508166987544,3.285813877840974,1.5399821150080661,-3.0814906350301534,1.3767457817464148,1.899708326988247,0.9093952713244013,0.44510786487580856,-0.19783266075180506,1.4707983458666187,1.0225504982617872,1.1765647292567125,1.428726310904854,-0.6546113580371784,5.132617806909093,2.0169410854219088,2.5760975314833274,-0.049739889371875234,-1.7209029301410124,1.7293354000677348,-1.0772155643389487,1.6758909265897581,1.4846581010116544,2.564531110604417,2.322008941653321,1.9547544039169251,0.5578825639231645,0.7344848154764341,-4.197107007322759,1.1913424483271347,2.2592951176594154,2.7183971131825864,1.9438840749657949,2.77890829766385,2.9228607560872213,-0.39677545042887563,1.4502471856342245,1.991615707449937,6.494314284642665,2.4487663805169535,0.5816856829262254,2.7490320141137956,3.479857898450648,3.6577165879857847,3.545349409073669,2.6485424970352875,3.1037602128234707,2.706356626127758,-0.5662798976358108,1.3530942413242368,2.3234383263441307,3.4967015027144805,3.2157046264150155,3.0211046848488627,-0.5075978897772546,1.097415288691203,1.9365226034911927,-0.20206211474420277,2.4624796267178057,0.7613745017982666,1.625649648262329,-2.340611028725313,4.512733240162689,-0.8159308283697446,0.36381554180997966,3.8722413814863765,-1.8825195322512855,2.624559785710741,1.2689994434129308,-1.2334662000186611,0.9008545216793581,3.8430762321361636,2.553353649347322,1.4432909444642785,-0.5145422462663494,1.8395215605096813,1.3540750751399617,-0.24708792879834007,2.5187681886435174,-0.4242596824098037,6.013623746934028,2.248567564388119,3.529014252408325,-2.7012417977576075,-1.9099190654960227,0.8642932360396254,3.2075446433150954,4.65248924389282,1.0270359510951952,1.7963204445720244,1.7889613352485094,2.449885197970345,-0.5259972418625272,0.5505236614189373,-1.2752278166453888,0.5626725793837062,-2.268481475813677,-0.2578272246849376,0.6557989055510135,2.3432716900217407,2.692648765789521,-1.1086708582096199,-1.690492691261261,0.5139923186125961,2.3381649413812386,0.7110632076462433,1.6251228195153735,2.398038742321221,-0.6620536114210203,0.30020640727131487,3.0472877657097577,-1.9124132100270907,-1.0260786779854736,1.1032050308246937,-0.4584328608790489,2.4248012209703647,2.5202777776147194,1.8827221450163871,-0.827639377557764,-2.3211668574382625,-1.6546943476147011,0.5473847475798976,-0.4180628784018492,0.15120367170867077,0.8313565815109344,4.890425819783996,1.6532831706961248,2.6436797501148153,0.7838566348432102,4.443231002308173,-3.184172011069883,0.457489738882373,2.3870729124995593,0.7379607935462089,3.7427617567213964,3.1992880289946393,1.4852768335877897,1.447763815183222,-0.7591353550940727,-1.9664970375941544,-0.2960702228274479,1.0453198124091647,1.1858278589568854,3.2481870612809156,-2.2796274995885293,2.0402038938876936,3.2261802313337884,-0.9000972708897979,1.045038980387672,-1.9388622129763435,1.4457178667738027,6.086402304254171,-0.22049942513332832,-0.08392170420440093,1.0314537557741497,0.1278173389166395,-0.33535776049931787,6.442408227184134,1.5398639617675485,2.3269007918319096,3.1097738428647856,1.872039513027816,0.8472971232206279,1.5298188506004426,3.178283413997097,-1.088851523061174,0.8544575988386778,-2.814602411895834,0.07591388450099701,0.17103088165699187,-0.30081833149493464,0.5995584445714683,-2.1822708394486425,-0.1319287592673586,-2.1749497939771736,0.27130738629617934,-1.2085896471465296,1.5808359686471931,2.9490387075708924,-0.3837218507128455,1.145753643646095,1.0361278903239304,1.4904703258087317,-1.9984351275775554,-0.16800668653549944,-0.48597572896536745,1.7135320121555937,2.5842097556281884,1.5746695519800025,-3.3542250961146403,2.7884078044174605,-1.277705631640036,-0.2992453226177656,3.669578588239486,1.4970427445242853,2.605903074640665,0.44809692386171773,-4.966401835312912,2.2677364212419766,0.2032297534520483,-0.48447082125752905,-3.2071338265150615,-0.7629945518592318,3.551977307445554,-0.6285165783090367,-4.684213490362552,0.6348976884252935,-0.10301517226340007,-2.111307266783656,1.9522694824590485,-0.5317508142817178,3.275803883774128,2.2620677778472214,0.9434062077861395,3.8757472962538544,3.091747948481914,1.7815796464837004,-0.8420691452120745,4.897008816196369,-0.989728130162729,-0.5938443711534551,-0.18984764587351055,-2.96709666778131,5.064757725808697,-2.3084856846658024,-0.4244342832891548,0.9724215686332124,0.8757664239800093,0.7817555092413421,0.5161368033322995,0.697726613053728,1.194047802550012,1.9187015814644595,0.31206632292298586,-1.5347004203751964,-0.4712554307948893,-1.226668632629818,-0.46887558160199516,3.216797396935386,0.6300281290745544,-0.4025129173266886,-2.116300007637411,0.2786851462882859,1.8931895280496747,1.8419573004065635,4.535283064243423,-0.6288488079649754,1.8846374691733137,0.1898731794771158,2.101365211999001,3.956787168172396,3.4219989960513764,2.834741119892285,3.981839634602602,2.793974404244952,0.20988687084150504,-2.8753869338521088,2.789882414177243,-1.1690980584802282,0.9207724020629913,1.9802174625953586,-0.5177996923071946,1.5251727973684117,-2.879403308815822,3.540077019748557,2.153524305285683,-0.5113815301081053,1.0912980494346456,5.854377142394915,-0.4277840214210129,2.9571022029485965,4.575456856021427,2.723395204978595,2.6863171119585947,-0.17761902508093952,-0.36509422973737715,-1.0635396094835263,2.275764970948222,3.623730139682036,-0.27883815737216366,4.4688985052450825,-0.23284474611856432,2.8279007306838064,3.950554110886726,-0.23245729109637248,-0.08556363513730547,-0.6271044087290589,1.7886325590484082,1.2700109996489086,1.3678901624125563,-2.350930301445059,2.0564557495792437,-0.8301996844190904,-0.6875182990744588,-0.041258032089906305,3.9740577234016405,1.4141456624068338,0.9833168880919054,2.535634955350793,-1.800300899883366,0.0032477887831239283,1.667527275583706,-2.09881989109027,0.2156406695512122,2.30844635864136,3.500227087785737,1.1290722978377798,-0.9132885234562957,4.169066753516622,0.05616519598671055,0.8553703063518581,-0.7708234147077304,1.4509323811578723,-0.15374023957739325,-0.249461899811535,-1.466360335122844,0.2648830575092185,2.2496958026313947,2.6117808631737134,-2.615107777017316,0.4742552313552799,-0.782349277367437,2.4028352572928853,0.3194143033885305,-1.5457974392059368,4.3105765920947,1.2246895744911057,0.1805930658118925,-1.7411141688100216,1.8549777495305204,2.361636638162611,4.416790516047371,4.514126353807934,1.923811599299341,0.582255731016837,0.651846345852988,3.5928802516287157,2.1693549820137377,0.7884605138311245,3.635231832519388,-0.846570962987053,1.2390896906468871,0.3922567707109651,-0.33934551452891837,1.752835260516834,-2.111445387331798,-1.7392429569378924,0.06379192356389785,-2.4849587494419176,-0.008654708148288615,-0.5666366122662185,3.3010268388408344,0.5512364629142519,-1.6773108468160132,1.3835680628572398,2.3330672811658872,0.08663872582549659,5.821435486926812,-2.215502459329517,-0.36281016561026846,-0.24116519358609456,-0.9056287734295614,2.645280684482204,4.631030338896026,-0.8501588684650474,-3.8043947731690526,3.6480737058059933,4.403010884418514,-0.9629225337173128,0.9293975919959774,-0.9876590657708573,1.505790967555038,5.323789645155381,-0.5540647255150903,1.0289120088343475,-3.880056037888904,1.6299686735446794,-1.6380884374455498,-0.7373772725350995,-0.795001272428496,2.7841966055530056,-0.9167432487946239,0.4375642494151507,1.4799803777365856,2.926247540553195,3.1711747910744936,0.6473628180765172,-1.5356553407594187,0.7744467982731291,2.210400704503119,-1.0472711110747306,2.136151954172825,0.11492893073875832,3.9163883815328697,6.688263814266388,1.5237983742854728,3.2321020796632705,-1.146038687970048,-0.8093103583399293,0.6020523820999195,-2.1903258952303792,-1.4391725720969917,-4.358803936036979,3.27918102283118,0.4729239588959784,4.0713964379213055,2.206652114448008,2.438996113803337,3.690539940008246,1.9280463558784766,3.6501139683374593],\"xaxis\":\"x\",\"y\":[-3.6813149644802667,-4.286489540181028,-2.0188489048508864,-3.563874225650003,0.35547061189964735,-3.1266244787855006,2.7948086107656334,-0.8003791260121897,0.006211484717754345,-5.814009567886069,-2.022569885469676,-4.496692004974994,-4.06183784118539,-1.6020131730828187,-1.773299890539401,-3.174192017626984,-5.944959446186207,0.8951492340786849,2.470767397877659,-0.05571628544634799,-1.7089017962200184,1.4953202390743994,0.23933233960472955,-2.7843020967905456,3.127295437750087,-4.184938694130528,-4.644647709405707,-1.9401219772804708,2.2567921462404326,2.3204433117071868,-0.1836984118147842,-4.888601036863404,1.3594722497307714,-2.7042465117476455,-4.573003667701313,-1.4023585075170077,-3.2590283170704355,1.7514775109541492,-2.519885771298269,-2.96639257105589,-4.004558943082683,1.1722290710600194,-0.8028945137578924,-5.394369123619609,-1.9428179225229139,2.901988605454929,-3.159052613706116,-1.9931269237734077,-2.4606030932346057,-4.92154834770639,2.8344170925918135,-3.7162692363983587,-2.634278512470085,-0.8121197333249534,-0.5556802055322244,-3.28149978874909,0.9768737265600347,-3.340992226898713,1.269376171503728,-0.7936958093345048,-2.8924102903586313,-0.9048858812393703,-2.949211639860116,0.03921422710847833,-5.83904453835646,0.704262771545431,-0.9929620061163074,2.573810929916372,-1.861473750577192,1.120070001551897,-0.2667749663109096,-5.327489664190915,-2.981575108836884,0.1659537451797879,-0.04776977737009824,-3.097178227414516,-2.907195499192896,0.994656036922914,-4.1780354882622674,-1.3421188353017333,1.0835727007367515,1.9127560915776467,-0.6071903535198415,-1.0274524405181809,-3.168740563575037,-1.414576024968212,1.0171535973969812,-1.371816654185703,-2.79425144978915,0.9916349995864588,-3.016813075649926,-1.1603097373965725,-2.3710296399783015,-5.556822354324186,-2.223468761565657,0.9710523808726124,3.2059570978875014,-0.09068332669794921,-1.4061592817960118,-4.4610606939811355,-1.9147072059579762,0.7476750717317637,-0.2688129573920721,1.1905900597066417,0.9234039207134987,0.8745796994290016,-0.1634934011443887,1.2465773570020544,-2.291572324573441,-3.2463589347817505,-0.8425492484317718,-2.391445541224707,-2.83037100624812,-2.9300009693899867,-1.4790471585072744,-4.353155682438691,-0.6459916575667978,2.1278386260175353,-0.0050537894092701485,1.3937368513755803,2.241047218223756,-3.594565670053246,-1.4304033312870277,-3.637568928940028,-2.197192468459722,-3.74154722414677,-1.0525534855217913,-0.7144136460208175,-3.24975695790219,-0.280364723587583,-3.323769526644699,0.48990483186215306,0.051972099290803486,-1.0120716882511105,-2.9501435713230952,-0.06402037985263154,-0.8570154519353567,0.7378992979587871,0.9943159267098074,-1.4409174028444338,0.6747491473827609,-0.48567005135940067,-4.148415667623817,-1.7553624397757202,-0.7589472446731228,-4.328640080967098,-5.071327062682448,-1.6699205649820725,-0.8427449691839108,-3.2926454014623943,-0.6738440615752675,-2.8400816442769075,-5.752115117647678,-1.633322037622687,-1.0483537894340882,-0.2407085443836905,-1.1804429981946685,0.35535380080491863,-2.743319563435119,-2.0332485102323603,0.8897012781988276,0.7445244481860067,0.47892068736633275,-3.827621914298938,-2.334403549872967,-5.236612370917722,1.446262681463081,-2.978733513923927,-0.7249833294944646,1.3558692928691287,-1.9486084998810058,-0.8477784902725791,-0.9736836774885439,1.2904754822925981,-2.040791247218288,-0.7717348908494822,-1.088457545602088,-0.4767416575025478,-1.9472853832495836,-0.7948072782221731,3.4175303107146187,1.5858012933959298,-0.03702406422834814,-3.3336239929863187,2.3752012618736855,-3.486524264925294,-0.3530161422879471,-2.278204985660632,-2.395598827684463,-0.37298548992369307,-1.1971837723336112,-4.139144897151924,-1.8859392766171976,-2.045110750077117,-3.8835487854846718,-0.5049188996288194,2.1969847271673393,0.032934181340570357,-1.6135256419727457,-2.7299893071267167,-1.1354441545365104,-1.5413669504295922,-3.4607381789465537,1.4315857835957688,-2.4987795925118057,-2.8941571748059687,-1.9219243686714418,-1.9630505944568721,1.6287318568062652,-2.206490758572922,-5.279621636793191,0.39425791399219556,0.24294320171815587,-3.6495356000546373,-2.7085745484710313,-2.9167598611900614,0.10489189146865052,-3.505197792474691,-0.6932051345917575,-1.6082506575408604,-5.218761500239192,-1.674814339569198,-4.582558690378676,-3.0076537705707596,0.1609083589151823,-6.806709621882101,0.6531013238735441,-1.242561359372453,-4.160082347588221,-1.0919178742539724,-2.005893856013955,0.14093970434135028,0.14329483704781087,-1.6614844905164758,-3.2968276813428865,-3.874940430473392,-0.5771611246525865,-4.094075964166041,-2.6436781893633094,-5.291613125307422,-4.057853394424924,0.710410256321917,-2.5618080900787197,-0.5040987808029068,-0.2628697487485099,-2.324669871724706,-3.321505772774794,1.1110626505293588,0.7258991155909951,2.565049824357072,-3.961825672577953,-1.1978596434721827,-2.4299124539769235,-0.14568526793387337,-6.859415391674218,-0.23583924757640284,-2.1166820445619,-2.1807922343285018,-4.055412208937418,-3.809256261523563,-1.8018913692440943,-2.47069131086555,-0.9719554371424082,-2.0114625625056677,-2.837343866182702,0.4672843754215983,-3.0017209582097175,-0.3091711846591928,-0.07805806951118388,-1.3121359758046087,-2.1448218541322195,1.7434414816252302,0.23801729738671118,0.6586267201906237,-2.3883106799922427,-2.42339472450214,-1.679893149973067,-0.7203525570200221,2.6669981398433222,-2.0004758993257785,-3.3169156906429724,-4.313757297852652,-1.8396184685088885,-1.581310112409561,-2.0782461113792747,-0.9318780571478906,-4.787215492501387,-4.158134703736848,-2.0545470556003544,-2.1468677017083433,-0.965244303490274,1.1975432101622556,-3.0338556230328475,-3.377243304874248,-3.8009774502787095,-0.5782090786537063,0.19910377986106176,2.7731187390023138,0.3266349420860637,-3.1582204398219966,-2.6230518472610544,-0.8673566457157743,-6.044001856282574,-0.860655902180268,0.5754309866460636,-2.1161551126886327,0.6438851855465144,0.012538680007620151,-1.260435331556074,-3.9018157039597576,-2.507195326962437,-0.008305062152033305,-0.8207131761424329,2.061813053236138,-1.182123544967624,-4.086871836380643,-2.10518903775719,-1.6512636232454572,-3.049290253895933,-3.0780218083273843,-2.9118965935437773,0.27651421383904595,-2.067861167250782,-1.1967749389430684,-2.4477397704591297,-1.6885090937155722,-1.8677618861584053,-0.8864113142847918,-1.6035531527548792,-3.667238782124949,-1.4247756367137192,-2.5279555977513644,-1.0485408024881462,-0.3842357018771746,0.03565101332168785,0.4287941010369145,-2.466363484304339,-7.021986806054959,-2.166809654701024,-1.5077297343789193,0.4864131249452939,-1.8777752815444746,-2.0975983514890135,-0.9404637324750789,-4.40314541344471,-0.3919050781899881,-1.840541539824417,-0.5904982680441806,-2.1346011168541,-0.9110765113552848,-0.4825958259269344,-2.5317182108229455,0.4949058678116902,-0.7733674411189643,-2.8733019338008825,-4.249871267413691,0.7155477446899448,0.8323620712095359,-1.0327423185236777,-0.4083664569322015,-3.9039085488192935,-3.267173447141459,0.09697736670533885,-0.12115581713403101,0.7680403220451444,5.24506860073128,3.478761802768621,-1.5743737603052306,-2.352969216902282,-1.989143774653606,-3.376747095601205,-0.29120894446581214,-0.26767196292923123,-0.9684709821508418,-1.5531025475877192,-1.1127312426512808,-3.5171635887760244,-4.62342920077862,-0.4366808951818981,1.8901651516851135,1.194236010882205,1.4959806333309884,1.5569405309587387,-1.4183217692646268,-2.766812851723496,0.9423325461012064,-1.379410430531225,-0.894406722722945,-1.9832599859230369,-2.283972613485223,-4.19038359841192,2.603384366693383,-1.1813155731666158,-6.527405970686088,-1.800763628266865,-1.4878366836719583,-1.6309856529028108,-1.0591115557707562,0.12241310049230436,-0.799903237796511,-1.0112821530942684,-1.7394045393087842,-2.5362796203538496,0.20177230042389338,-4.21161085387097,-2.077652163952175,-0.8348449284349826,1.6998717641583743,-1.2352840616015444,-1.848923990435266,-0.16883101891151941,0.9884138987181816,-0.13410753670653808,0.15397437128416547,-3.1812387882535065,-2.552995750286657,-3.0814476962934014,1.4266637129131854,-2.7484940125920962,1.3571248213605283,-3.551391069467003,0.8616690740306545,0.9007737179621582,-0.556430135092353,-2.793180223856722,-0.861893794535721,-3.5888359922459117,-0.35304838803386684,-4.469415674761743,2.1712217745443865,-5.1560144767537,-1.765900231148956,-1.70880447275823,-1.8532803272123277,2.550184922991575,0.7986341595114519,-2.803757818898778,-0.7431937754213043,-1.3521110123680264,-2.2692889795796924,-3.0145776632176355,-5.646581976015136,-3.542322736466899,-1.0682814800626095,-3.8334339535198123,1.4367065440805362,-4.4577666837770185,1.4590611733775147,-3.122310149829558,0.2289760890120056,-1.4621975105326426,-3.9608797701698544,-0.13521016444387945,-1.6543919821947735,-0.7170704261575087,0.3428128762391107,-4.27460751117966,-1.45005533681373,0.4492260211223833,-2.1463633591912696,-2.3924357757776096,-2.74654655794762,0.4367716736702705,-4.71173562433288,0.07131730016946158,1.2302783016689545,0.0446300118427716,-4.916212467062115,-1.5231060516400594,-0.7653143811468806,-1.8273982849549162,1.8675370866307213,2.7878373279257636,-0.6775689084904509,1.0168577914927202,-0.6928961056750857,-2.2721384941649294,-1.5926692420876285,-1.6383725870177481,-5.202257819520581,-2.0758478827440863,-2.4421190794150123,2.1855485468697107,-0.03288868935478084,-2.648285807227274,-1.2749769201544203,0.1058718862750152,-7.93379180491344,-2.784836743404672,-3.756110189487033,0.16786975188587538,-1.3085804359142896,-3.0370372344823924,-0.24508762364661352,-2.873114641745783,-1.4344234947734686,-0.28563275733921323,-2.880902695505387,1.6199297276998195,0.2776226972471323,-1.3944066641672046,-1.9802141449111872,-1.9972376381454042,-1.2880408756188382,-0.840858135111948,0.5719555690725084,-1.18636319249696,2.090534908748426,-0.29650722461105794,-2.170157678106423,-1.1091009439639077,-4.612493268270396,1.754813769367757,0.5542694610076684,-1.5413812284327064,-1.5297622002826954,0.7469863338441112,-4.003171065465455,-1.6090342155644144,-2.464827051113055,-1.3596350629445852,0.37094873948914575,-2.739199219332754,-1.6894132965167008,0.9545673297069941,-0.8991126858242697,-0.727885361829274,-2.1128601628738544,-2.947611872432499,-3.468323615654179,-0.8663750279699396,-3.1138757719986643,-0.19940574917996812,-0.7381610712206813,-2.298471451900438,0.5320225199333226,-2.974772309008947,-3.5294991771328244,1.8802158477957538,-1.7603346671229256,-0.8839830741663195,-3.082969758928229,-0.8992143349647037,-0.29275174991513575,-0.15626853675965302,-3.7740349886099054,0.5390800220860053,-3.915657573194242,-2.574992481627694,-7.848614713581086,-1.5556938592293956,-2.367892541493943,-1.6862437627714706,-2.0958669522524453,-0.7278993048001833,-2.092257974110281,-1.2783060181143708,-1.6587541277558213,1.1334345751705799,-2.4303621042914387,1.1902474450415799,1.262433322125068,-1.3716660242150924,-1.6251860205419448,-1.5257249344791644,1.247053656683617,-2.0299662311536326,0.8365974511820213,-1.3617724687533281,1.5020486886189417,-4.268717936888347,-7.962691542528578,-3.217866979859067,-5.605426634935826,-2.033795364016255,-6.320679366727869,-0.8831128633213823,-3.436999370368038,1.310592287292358,-0.8771250895417955,-2.904297600498589],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"class=2<br>x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"2\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"2\",\"showlegend\":true,\"x\":[1.6585766627923788,0.29907139334092414,-3.7750097135187812,-5.911783411882157,-1.735703113713912,-4.794637088464485,2.1947582520493083,-2.8804524206737083,-3.64667722669452,0.13821049868097002,-0.1845957522259969,-4.6096216112938135,-3.9709118509283967,-3.092265085881021,-1.4118173478675422,-2.2266813703131536,-3.077459497641918,0.3666382398407628,-0.7741711911021507,-2.904435625643733,-3.4817294429123495,-0.920939155140109,-1.1805237774224155,-1.8957747267633596,-0.9414472719665169,-0.2848004238191437,1.0984533142222728,-2.8812327672647093,0.25373644477112456,1.395740129999087,-2.390189414639792,-1.1841173935765403,-4.578355813921407,-0.4390255582580034,-6.003170346860698,0.9384043643767499,-3.845021704068275,-0.56713381849129,-0.4618882814432881,-0.8253107696020439,-3.712330667422925,-2.06934438239467,-0.44757956888522843,-1.7622002271921062,-1.8593914263826057,-4.202859975938274,-0.18589332557625204,-1.9803946846885565,-1.190252751708862,-3.540408101318325,-2.1183817589477036,1.824836818211638,-0.49726449586206334,1.7599156605146553,-1.5301412985361142,-2.6815413634187077,-2.605452568937805,-2.7519980411586764,0.2256269299427731,-1.5001008744552866,-0.9160918225504058,-1.0207316508383615,-0.8264926366616874,-1.9722353565161406,-1.5958851815643034,-4.37240120952025,-5.442001049541192,-0.6159234053911878,-6.446689114084755,-3.3279154913206668,0.43801095579141647,-1.2106517459431865,-3.393475592756904,-0.8111459548796338,0.7132744047343977,-0.13510322782283612,-3.060579440579957,-2.7818489070988894,-3.467795748352917,-0.02420300299707745,-2.45618342871658,-3.4546549966139466,2.2644284997617046,-1.5611169556250966,0.05909631021073336,-4.901995129895321,-2.061259006444484,-1.179859798263945,-1.531484213678658,-0.7554541365740236,-5.69827958764614,-2.84747575476783,2.306451618409228,-0.2732548151062939,-4.495481383169999,-1.2574552544803876,-6.113617474748861,-2.8840340834636287,0.5163301850205793,-1.322149037226076,-3.2997377897967324,-3.1660328579879153,-1.8594945695502427,1.336859025767105,-1.7268317192815117,1.7395742351337722,1.032323582393225,2.097070722108549,0.777060189971718,-3.4988721116343724,-3.803074590466556,-1.8752995514375117,1.0720266031233976,-1.7562505868209946,0.4711098857660381,-2.7606200819796913,0.9267709926825409,-3.0742528424337676,-4.515882449997352,-1.5786446507248268,-0.11689072021142999,-1.2557239665341164,-0.05830677054867284,-2.8939439166121383,-3.053844932766351,0.02722871473325128,-2.3985690209471255,-2.3957416608896587,-3.485420142508369,1.3190682885423173,-4.198174493170889,-1.0608277308554883,-2.531660748543432,3.286198069793536,-3.167751330170166,0.4810723246999,-1.9750114804130547,-0.1683427620982494,-0.5057612159877003,-0.8828033654717192,-4.1002339934630445,3.351476300542319,-2.199033692817734,-3.055010529460935,-2.2275117925830226,-0.6554330422491179,0.4512038968678116,0.01126171468002446,0.25026770776239804,0.5396288488450653,3.236292338121821,-1.0927375468301155,-0.6247386640625314,-6.27373176554209,-3.4436703030905838,-4.336642128563805,-1.280491483118251,-1.613369317715544,-2.056539790628483,-4.668285412233806,-3.7078395958624006,-0.5962941684821099,-1.524733599448402,-2.0698244234802816,-0.4607919641521738,-0.9535862491250298,-1.0521398646805245,-6.304281764589611,-0.7162126883927763,1.9039545717720312,-3.20398393571433,-4.072316187349616,1.4609516216391958,-4.86499599006106,-3.768248845704689,-3.8322632986665406,-1.7665706881614045,0.9383852427278367,1.0608381281783164,-4.93713847391834,-2.730024604432047,-0.6219661560838815,-1.4274960710480553,-2.9597675774733165,0.1084821241121885,-1.4536277291978552,-2.9891998420115407,0.2450473192979956,0.16635663194256534,2.8418041233650557,-1.9063054648673547,-1.6826010343118574,-0.8041218419043595,-0.6490431703639451,-4.336819329791371,-1.2439425665814534,-2.913564334063258,-0.7307033140825981,-1.892401456045947,-1.9952134381055662,-3.0219681094758224,-2.286789477212721,-4.65148618155958,-0.2588896366612199,-5.482167670454634,0.18835304767959338,-2.2374851589399936,-1.323684196899666,-0.8615637788239033,-0.6620909943342013,-5.711206446546642,-3.67209234113818,-1.202425872867189,-0.9131537357148962,-4.551285335224577,-2.240972997709223,-3.074622998257788,-0.43458925905405543,0.010875362574182645,-0.8650354821521056,-2.420280359002596,-3.0103729181344847,-1.4080523622817398,0.10935232328851074,-1.2553435992186561,0.36759538717997864,-1.2503222438532158,-1.4887498146137212,-1.3895683261436635,-0.7499377505138003,0.12112863103734872,-4.371555252753352,-1.0368872073874942,-2.1637158428989496,-2.6280352711486445,-4.383672093484741,-1.3026502449660817,0.09443854823036757,2.43619816864637,-1.7424502277056293,-1.3129635293124675,0.3109346135165234,-2.451830637177907,-3.393168835505703,-2.636460591833448,-2.135126098489919,-3.5722781345389896,2.870511688555646,-1.5357896184743915,-2.3042618081962285,-1.0052774317171649,-2.4374133562863634,-4.4441456855949175,1.4968839982323767,-1.3642805373229132,0.23121482070863886,-3.2617627198673516,0.1667712094767675,1.4939856650123744,-0.09579304373804898,2.4042789637712745,-4.74000732989686,-1.2003878802157129,-2.3715336447848836,-0.8014830409868212,-0.7797032545059774,-0.09824867199638465,-2.0398300521108452,-2.1827681493263684,-1.0808592807536386,-3.2865147039560076,-1.458794138628793,-0.8844490840193108,-1.3508936817651485,0.67215861169203,-1.8387455759231062,-0.5412457192771076,-3.521761196803512,-1.3667254969835105,-3.375358831210052,-0.563426812702132,-0.42404636261354467,-1.101065286110229,-0.8783639015542158,-4.955312062885406,-3.28948701350453,-2.7916567893636577,-1.9184486559705471,-1.9610964583345392,-0.07512539304258836,-2.129238862607952,-0.6362826906632248,-1.171148493004605,-2.9402092732105274,-1.839036595730402,-1.891559024327817,0.7541040536907175,0.4259976976220554,-2.4027515921251625,2.7941669399576208,-2.5476056511392176,-4.23194625178096,-1.398944995924915,-2.2029939687827795,1.1200652491845222,-1.509943426506964,-0.15429276634926747,-2.4737626029445714,-2.5441719010758,-0.3668581283739967,-1.6207907416407479,-0.7189326691950261,-2.987463453029526,0.45052491328953437,0.10048640422099453,-2.435080688825118,-2.8780579257544323,0.0917431480142552,-2.0910094180925842,1.715382379098187,-4.8881756652906105,-1.8929380409712628,-2.6495740525406735,0.5704323410216232,-0.6292826045672576,-1.6952294271118333,-1.2201973188516277,-2.876617001637595,-4.867568976174228,1.9064105388249963,1.673091628419773,-3.3804144676700427,-2.7330347583082815,-1.1301563340456688,-1.0745261292315933,0.7863974871902975,2.082401457425242,-3.938032017975566,-2.7736512533169115,-1.5385428874828302,-2.0430883564071283,-1.6641081830522617,2.7347467497741036,-1.7358367628119928,-2.9466789062472944,0.9758179942555993,-4.426053129351361,0.0809687963613468,-0.4862859498863401,0.2359802736851122,-1.3765356387985188,-2.7594944478875707,-5.494139855191676,-1.3773830655770503,-3.026515521727804,-3.910598789850763,2.504750713338659,0.9408521696908636,-0.6401077567189342,-1.48931181912961,-2.2431606743708348,-1.1053462481300949,0.44559136783081277,-4.671382474517602,-2.9232169647241957,-2.839777604632312,-1.9830359576442185,-4.530264092838133,-1.5101310284661928,-1.689354118831379,-0.6901559193596833,-0.020875698314289703,-2.668285372644179,-3.772528712441431,-3.9837575734812627,-3.575251805144753,-2.915684196634289,-0.5267310072207705,-0.2648763785166073,3.1022355244327073,0.21468339606835118,-0.9776068988347058,-1.583078841576468,-0.38900875027592785,-4.339613940107283,-5.121935754422376,-0.8612433434699467,-3.8723847387992154,-0.7122746987799679,-2.4518299415080196,-1.0110674989721251,1.1125482124460504,-1.3905065197010027,-1.9416067315132446,-2.988167754208473,-2.4250873252362366,-0.7432495520608164,-1.2175819464897546,-0.2742495422490405,-1.6660893633859455,-2.7975988999920824,-1.9399794908213932,-2.3151626247064816,-0.0303600815728704,-5.282371418077373,0.31267751221790285,-3.7283131724253544,1.2375459873990349,-3.5806393633403215,-2.994855697465442,-0.43196116732020284,0.07970433619969208,-1.0309100611458366,-1.523366973489168,-3.1216352104927427,-0.23223789722965615,0.7948747972434691,-3.996234895863853,-1.7047839071555366,-2.348939213154224,-1.5776490633626217,-2.324549148840355,-4.469019279523488,-3.3140182567214715,-1.9770879409438884,-0.4792779983255495,-2.072576932739575,1.4360144842709865,0.5490983141566979,-4.676274778493416,-4.298329706921253,-3.5961043798762478,-2.7738195434057946,-1.0294648709733536,-1.5118906234589655,1.0421526379513613,-2.7663571722258222,2.083908668125101,-3.3459255791734064,-0.7340435616467665,-2.7880791906233844,-2.0811861371006675,-3.8136277421153815,0.008400696564895505,-0.7536869105467063,-0.7836721623005124,0.731258975773994,0.17048075877096713,-0.28304916144199654,-5.1384257681850265,-1.6668712657996767,-2.296738059330742,-3.247835071521548,-1.110759840289957,-0.9312607438269088,-1.782797432852205,-0.6510623421050823,-1.7992250026454057,0.04296826739034221,-0.7796564344366677,-2.1657661352949997,-2.3946416335717533,0.004506687813010002,-1.7469890668112709,-3.3283941730550115,2.188127236602856,-4.0500026719726385,-1.2741757488970922,-0.8379024015276787,3.231094074283912,-4.266850730523974,-0.06164474700743794,-2.4988079742578155,-1.4849621330161857,-2.8753127940304175,1.3647934413488039,-2.593484719902529,-3.7731504788754227,-2.8553289173469225,-2.937752288602705,-1.34793984238689,-4.10408087008376,-5.94603555256915,-2.218797488735784,1.1322887895556737,0.32776546102704884,-1.8661300178804037,-3.2042392660885426,-3.474073104937438,-4.5285021618124865,-1.6697612596558689,-4.379828504247139,-2.4395894077331435,-2.3954033709283493,-0.8805474212799078,-4.317569920519788,-2.337738432292738,-0.6385901090475053,-0.6261381500563341,-1.3759126598730709,-2.4986011981404657,-3.5948757018332484,3.643696281365438,-3.688303663918279,-1.0657028428148876,-0.12480969152750543,-4.89044142555579,-1.2236260841374098,-3.871777652018348,-3.0922423083373003,-0.764397450296274,-4.202261090737862,-0.775648150934616,-3.166882253017317,-0.3966708922029718,-3.458200113116253,-2.223229116628282,-0.3242552542711554,-0.4237495396419362,-2.5724160442996777,-1.1460560293147004,-2.0468844292501083,-3.9659689294193217,-1.0406077949294184,-1.7358931328965255,-0.32784618732636184,-1.747065424961446,-3.3174777458600024,-1.7315957967976654,-5.481305827558506,-2.7684143407789077,-1.2755490111424748,-2.395785389481217,-2.4708047624530343,-1.4301899158600206,-3.157278021090608,-0.5797900760041426,-2.4601520538129327,-2.7816664140327108,-3.119554245030393,-2.780067523939471,0.7868139719088076,-5.3005379239244945,-3.137219461415883,-1.4552280420574917,-1.2793361691230247,-1.4775879333607504,-1.834041744591456,1.4646696267437473,-3.1724192552499604,-4.932970456491313,-2.439428884551626,-2.4610337149284085,-1.9196793837688308,-1.7648457130261228,-3.653109940056501,-2.18965645738538,-3.439064275725179,0.7370328116730631,-2.9996333253863496,-5.678606629436924,-4.678365680786267,-3.3167802457053073,-3.914363609097039,0.5709894442836636,-1.441229761505576,-1.7857474384963092,0.05163576657713187,-0.3147401358601316,-3.2285943410684728,-3.502777854425955,-1.6562997375397353,-0.43914041329984554,-1.2923066711661209,-1.1397156252711653,-0.19082068707261657,1.4089737185076259,-3.3401143611121196,-1.471655185080818,-4.042839341130422,-7.218586683803458,-1.7196780231052256,-2.9230981965113756,-3.8978409850814963,-0.9960514356601491,-3.8784580248627383,-7.6839669928148115,-3.2024006621464762,-4.788934893902248,-4.379546076536605,-2.393993030991066,-3.6996015469428776,-4.378649162687914,-1.6557456819984904,0.28497539684248174,-1.4768627699153556,-2.8993538274753705,-3.45873247158202,-2.9213796299666654,-3.6620716013516637,-3.2090110075511133,-1.9471218398168564,-1.6914691465546026,-4.06635388394003,-3.2773033242521974,-1.9393048327296742,1.0336822282680425,-2.0026053530715515,-0.5842893478417917,0.23788437610615154,0.8952555019503592,-1.150024198378114,-0.4728854665310973,1.355898793014132,-2.7115810063332066,-3.1687196064348617,0.5752232976626268,-3.401825816882227,-3.382092417111487,-1.1251590554685307,0.07091457656578815,-2.049523891750032,-2.645729860602888,-1.585282423693562,-3.6413366412154735],\"xaxis\":\"x\",\"y\":[-5.762906544309241,-5.78079118043416,-2.7564493403503967,-3.8522255779402697,-3.5906055614664036,-4.492615230749349,-1.6103011382772625,-2.338571610754325,-2.5386002287307203,-4.554890551049356,-5.467042428963138,-6.771489797564362,-4.510175184329225,-2.894881097445981,-7.430680809436812,-2.8719076468465343,-1.1453403065587318,-4.624204076948505,-2.954916032137409,-1.5690554789842561,-5.87580182867223,-2.603908784902346,-3.8494312548647933,-4.339013461069546,-4.901430604496317,-3.1370656891427444,-2.1366834413902653,-5.174332315620383,0.8571119449964808,-4.50183949000964,-1.44867517282615,-3.7865857435101864,-6.710525649195109,-3.0962828144603534,-3.2798151027368876,-6.112801983772604,-4.504622644123302,-1.4642445704625817,-3.1791583165854944,-5.466993742020952,-9.673914441094624,-4.06432606823152,-1.3188348513066543,-3.729756041560518,-3.9481020745673536,0.6183449651704587,-2.4566369749179047,-0.6145972416905994,-4.529394379897675,-1.1090551408680742,-2.126907249130481,-5.050131828616859,-2.0711943534803616,-2.131174640180947,-2.3736987954276696,-5.714812155220331,-2.0108209348823305,-3.59778929390344,-0.7404513246894879,-7.804026823803612,-5.311836388827448,-3.448173982273328,-0.970859884902429,-1.2580450185986294,-2.247661528923249,-0.5259108449215728,-8.307962197018202,-1.4317809816925788,-2.3452463728097075,-2.0622991937084874,-4.2478435362722875,-4.192252498459325,-2.5972239469795992,-2.789028317422578,-2.641985918306173,-4.200511155954575,1.2801117468692675,-1.0048539816130893,-4.297972923484024,-4.7558751396713586,-3.7621297026813814,-3.243340201926733,-7.005703517926323,-1.775027300915073,-4.033515421594287,-5.033238637419169,-4.788983502387065,-0.44056434569435066,-3.37643803834841,0.1187289478379423,-6.288076400741957,-3.2485353508498203,-4.664673102274126,-2.3570835530368797,-5.041875915631822,-2.1591404935697103,-3.627830183980382,-4.104800965914977,-5.453887366231234,-6.393200788305856,-4.272466430088365,1.202003680753216,-0.3731357096907715,-5.648163498533901,-2.840622181793913,-1.652087623954933,-3.4918178262105917,-2.5803005636151015,-1.6476184704341175,-2.4955546173876515,-4.386417788526283,-0.4527566956209448,-8.842774860402704,-2.9493774293961823,-5.38599914813931,-6.110502807161204,-4.419552536374648,-2.487759246109328,-1.8015314483935359,-4.968250824413638,-3.271197689386409,-2.99099752736263,-4.525026348792919,-2.6432326146973177,-7.193909403496036,0.14943759538397172,-2.632099215083729,-1.0239118842261736,-2.8488264702939796,-3.076695679091541,-2.3413954389998466,-4.6428822687249385,-1.2118173650294994,-4.963162089297269,-5.921450567475077,-4.8515621343562465,-2.831326747058748,-2.4185079719619305,-6.8241963238991215,-1.8908530762261895,-4.939735485149518,-7.624425370283358,-4.654373853275658,-0.1192795622401901,-1.5827763105440005,-4.391926387286508,-3.667650223457356,-4.246307613735137,-5.383087159346649,1.045421895604517,-3.1712267400940695,-2.428173526781422,0.3418417680888344,-5.663512767237402,-5.391074759575737,0.6816709486973664,-3.5195082924217593,-3.646193250546914,-6.437663055707237,-0.8398563924853648,-2.5047694317572455,-4.7051080267540435,-4.340048194984433,-5.706833167676192,-6.398955296856764,-1.5640832188524167,-5.05001711131359,-2.9995038625511237,-6.052123439388957,-3.542805877459099,-1.4917515525053529,-2.59022367792176,-1.8721599718176252,-2.418974279428815,-2.6942032722446556,-2.3576055732530725,-5.737581649278313,-4.665850570141181,-3.727392604025975,-7.595554060763678,-5.405415801196414,-6.037169843356846,-7.837220008556645,-6.352168859626058,-2.3745766966177144,-0.7500062083056087,-2.654112788986304,-4.0468611239726835,-4.715654851993998,-4.641747834797329,-3.6541686750344606,-3.6024765925493774,-4.743938364716528,-2.1667517316185174,-2.707982933809027,-4.032946180557417,-6.553273718968121,-3.216236322134604,-5.096383090706876,-2.5888708916682193,-5.3416085169328404,-4.441295278694714,-2.4659075388834237,-4.947046898187769,-2.248975875005609,-5.6306770950696405,-3.4302170062654893,-3.322706754343836,-5.777949653618051,-5.5410713265942855,-0.10648189353452286,-2.7007091959609437,-2.618576820605604,-3.194080291346387,-1.4465918017654193,-3.5717061822889433,-5.963411764084851,-0.3265479486159393,-3.8954664339276253,-3.7703167225127117,-3.0379390289660018,-4.123391265841278,-0.27700904142716753,-5.244841740381406,-2.8291353553961254,-2.272342143822649,-7.691774284019801,-2.774485141530839,-1.0224760022172141,-1.3604338102681872,-1.436553865844629,-4.760287959569629,-2.526739426212214,-2.0951369527304258,-4.969006954517662,-2.6943721663922857,-6.99836667301876,-4.202603028590569,-6.763778465604376,-1.5414875312526342,-0.8978086583597604,-1.5393443261709963,-2.86873953364276,-3.584569358777381,-4.509366466161397,-1.5471377289753625,-6.867283560273274,-6.514775784814756,-5.500595422339142,-4.571824801018029,-2.8443448382306262,-3.662755316274733,-4.00018820216597,-5.502696642336229,-1.6025235533077156,-1.9934910425578418,-4.1549300902810815,-4.609097109373831,-3.439648851701106,-3.844106395678401,-5.0984638218759795,-3.5990333611573417,-4.771338196617329,-4.709739815376164,-1.6746141503413177,-2.7623264096553255,-3.0828704929739734,-4.925711983390676,-5.533308869372631,-1.381915104819647,-0.8967885204666333,0.7988905798421895,-5.164818525098203,-7.130811098832458,-3.292246661035116,-4.6022592995118625,-4.594324919462773,0.062012811938480183,1.5400186573379369,-3.353342851806862,-1.986554016063231,-6.016102017537385,0.35433482304639696,-0.6276081520155161,-3.8308573824709193,-8.644250542792928,-5.526259006408871,-3.425491866993908,-3.116676067475802,-3.6426167031050634,-4.674250604630708,-2.3093610356554923,-4.8535098942057,-3.6512944897741306,1.7685242247727846,1.8891263309826787,-3.709109297422952,0.5862916850911923,-1.0554371216788216,-2.1691050023658085,-2.3971765906482414,-5.650896844662229,-3.6864370021714086,-4.719773323700688,-8.749874560389536,-4.002949614044666,-6.332523661442236,-1.2014892599322171,-2.812042077195865,-3.53975508785663,-5.758373957193683,-3.845762582536233,-4.160056349342452,-3.752690351891185,-6.112529095398163,-5.18630187305787,-1.950205526793778,-1.651577890455348,-3.508725958795223,-2.2244950246892623,-3.0467837248273675,-1.6756214534791076,-6.5010812954164745,-2.460331137050746,-3.078508972156142,-3.452205908472475,-1.6072801948986717,-4.285089374370079,-5.634675970186219,0.9039961072117597,-2.384859704956045,-5.985015649548524,-3.567726442186015,-2.8654137612829698,-3.424138872639992,1.6988748992662899,1.691953287651863,-0.9541298639362634,-4.02337167866629,0.5301144193640894,-5.260647495829682,-6.213981496691799,-3.5502140636759236,-5.082182348095642,-4.920476449319013,1.1336311954798015,-2.9353659811679584,-4.916597989297542,-4.083589016445288,-7.700084678974254,-6.107740507950717,-4.849234971383433,-5.197951188456862,-6.517209556502667,0.03891542928021652,-1.909883463874525,-4.687974373616725,-3.80035509697127,-6.469491139313883,-4.891765178747692,-7.226164139970399,-3.1254131769671276,-1.1682902880734716,-5.91234878122968,-4.894343648675158,-3.84966119809493,-3.316520636182878,-4.61609507269829,-6.8038530786191584,-3.6151961697876174,-2.0883773367423206,-3.4583067354983257,-5.077153580041508,0.23308156097730537,-0.5363817655981764,-5.5207084128841,-3.1090909523201002,-1.9969106100481109,-4.479365844021728,-1.6830568862971476,-5.901525687499783,-6.166528299117523,-1.3627110028853275,-6.110505859697496,-7.686150563686076,-3.420394009506437,-1.7417482327996476,0.30246462875450497,0.2941165686483478,-3.0501488797472773,-4.216839561065598,-2.696958396138987,-5.489016798593482,-2.622390389329814,-7.833268013110769,-3.2699890415942923,-4.36710598541579,-0.2954629813472396,-4.048882410176223,-3.3824027158029177,-4.146712589105086,-6.496519489548906,-5.265027739874151,-4.461451204395372,-0.33672966540044547,-4.160752626712699,-1.788369319815058,-0.5523851805295026,-1.0625014491063438,-5.165422050949923,-2.060562654854583,-3.261809915369689,-6.203669528900088,-3.569421577546715,-6.015541895791369,-4.181748103812551,-2.222319780231284,-6.201780225245635,-2.134803627796301,-5.608914989031776,-2.5322515747908394,-4.202602647969999,-0.49815676716873547,-3.263314465032603,-3.18872804710267,-5.029088388913834,0.1825636689226573,-0.15964309655950704,0.3775649659396052,-7.148262191383484,-3.3610381459958933,-6.0088144383232915,-3.332492702528959,-5.721836362784702,-3.683898208374199,-5.6016845843834595,-0.5212827776690769,1.4048398861242744,-2.95023365838209,-4.332752384754331,3.082891396270079,-1.1762071041187712,-4.028386071969757,-3.7668998553979107,-4.405207746442463,-6.115765169191064,-3.7512766780849685,-2.0318079152830384,-1.9412259714127178,-2.042162990443737,-5.6111977386086815,-2.2995533497987677,-9.638804601625669,-1.628336680113992,-3.1727315101404057,-5.187845624922838,-2.130643907113771,-5.898528020041761,-7.751944021962804,-2.4347153395736134,-2.625117036094882,-3.119131498458579,-0.38726662735251294,-2.752204874758665,-2.541207411529477,-5.016734040190007,-4.046565853507175,-1.0854528916064088,-4.090444483242932,-3.2188569078596188,-0.013149374312987128,-2.9083931341035765,-2.2587182759078344,0.3793219277998179,-1.9494056924151193,-3.212475926887552,-4.034402255920023,-3.308562613311894,-6.3357695054037775,-6.101527998772431,-4.041408322059591,-2.1016511372186013,-4.3871601245630645,-6.2959425262905135,-5.4051318516776705,-2.252108887532638,-3.3633561997061117,-5.8046139963678804,-1.8424269953079575,-2.7890027989039186,-3.572336287191371,-4.833940111808826,-2.5702217710032236,-4.903801377679333,-3.8101872966946284,-3.035666846108453,-2.9500270237724986,-2.4610963983989755,0.008519891539432667,-5.0932064038489315,-3.2204434152392967,-4.035204292897203,-5.259298626698536,-5.1151417954206035,-6.368031597502502,-3.6254091769599697,-1.8635479546439535,-2.796473673038763,-3.266644217030232,-0.5564402917347944,-1.3517975119657653,-5.823664467604905,-5.4339523922665105,-2.203979794508052,-3.7386037794892917,-1.222660088800457,-4.395714844520581,-3.978759445543025,-1.0400818078460445,-4.240500297429856,-2.44179513760431,-4.334078219493545,-3.8609737492587994,-0.5710590609530328,-3.2828033149620643,1.0763525659398354,0.5063075852705943,-3.7623391013693666,-1.1810995231275592,-5.801600750016771,-4.213707200583342,-1.594273840924384,-4.717378616182348,-4.47078068491237,-3.2295315044075967,-4.277695382925672,-5.39150027176338,-3.5850657648883444,-5.546850557029545,-5.979587385542141,-3.9841791789775725,-2.681900951147214,-2.2742826986740683,-2.0526778270349144,-3.924818122513107,-4.297495850904947,-3.321434026144646,-3.5331560999952702,-7.820164438382214,-4.648368361361557,-4.470065818779307,-1.892004107543638,0.11917579776937881,-6.5898666320580785,-3.8815047505604428,-5.3835074765481075,-3.6881093845842257,-3.4126439588915805,-5.768906984754748,-5.963127768707513,-4.548856497232708,-1.023057238867016,-2.8130781956781292,-4.6257394281530475,-1.2745012922913368,-2.004115460868985,-2.871846857646677,-1.8558813349596344,-4.06043954557609,-1.206586564756047,-3.3286656066110676,-6.124177851230414,-2.8026457123940043,-5.704620448894129,-2.3439995186953686,-3.523708301502086,-4.998728940782721,-3.3139714592276928,-2.8055751124109864,-5.00557779385587,-5.399197009463681,-0.13491513899249075,-4.40805641924795,-4.349745590978382,-6.710389365178887,-3.0255385854477166,-4.129971486207005,-1.3360954586726979,-2.096197349855186,-2.5481329616218944,-6.127640909813273,-2.5325429404851536,-1.1985588690417623,-2.5679691797056474,-2.58956751440319,-1.1193252725878922,-1.1081809291949272,-3.044652424633563,-2.6643426677192354,-4.6920843602406315,-3.9362834104551654,-3.6498948671881837,-3.234615425665192,-4.0397061849762075,-5.910050745106034,-4.041098941179476,-2.97291737347267,-4.444505299601529,-5.067694210066171,0.7448753219751474,-2.5128194185390296,2.0383461370104223,-4.693391928891874,-2.868879356337689,-4.960296292907688,-6.343344077916151,0.9066321097221453,-1.454771619610779,-0.892792707821775,-0.8692737165701621,-3.4126738311358613,-3.5778306489216125,-5.533795730423207,-3.8776989950950407,-0.7510315668540151],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"class\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('50b3632b-9e02-419e-846d-fdda8eb64c1c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "เข้ารหัสผลเฉลย แบบ One-Hot Encoding เพื่อที่ว่าเมื่อ Model มีการ Predict ว่าเป็น Class ไหน มันจะให้ค่าความมั่นใจ (Confidence) กลับมาด้วย"
      ],
      "metadata": {
        "id": "jzdSWJuRhT1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "O2VVFjjShUIO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นิยาม Model"
      ],
      "metadata": {
        "id": "-3shDz0_hXGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bfQeYhinhZfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2545683e-0075-4a7b-82ea-b265eedaaeed"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 50)                150       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 303\n",
            "Trainable params: 303\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "o9hf2pOMhbMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "PeQULtIshbcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb2218e-5926-42d4-c3e7-16d83a4d7ff6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "57/57 [==============================] - 1s 6ms/step - loss: 3.6056 - accuracy: 0.3161 - val_loss: 0.9084 - val_accuracy: 0.5517\n",
            "Epoch 2/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.7551 - accuracy: 0.6433 - val_loss: 0.6944 - val_accuracy: 0.6917\n",
            "Epoch 3/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.7039 - val_loss: 0.6661 - val_accuracy: 0.7025\n",
            "Epoch 4/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6422 - accuracy: 0.7061 - val_loss: 0.6446 - val_accuracy: 0.7108\n",
            "Epoch 5/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6231 - accuracy: 0.7217 - val_loss: 0.6263 - val_accuracy: 0.7108\n",
            "Epoch 6/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 0.7178 - val_loss: 0.6112 - val_accuracy: 0.7250\n",
            "Epoch 7/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 0.7333 - val_loss: 0.5978 - val_accuracy: 0.7300\n",
            "Epoch 8/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5835 - accuracy: 0.7389 - val_loss: 0.5891 - val_accuracy: 0.7392\n",
            "Epoch 9/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5714 - accuracy: 0.7450 - val_loss: 0.5786 - val_accuracy: 0.7475\n",
            "Epoch 10/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5626 - accuracy: 0.7511 - val_loss: 0.5678 - val_accuracy: 0.7483\n",
            "Epoch 11/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.7533 - val_loss: 0.5612 - val_accuracy: 0.7667\n",
            "Epoch 12/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7611 - val_loss: 0.5512 - val_accuracy: 0.7658\n",
            "Epoch 13/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.7644 - val_loss: 0.5401 - val_accuracy: 0.7658\n",
            "Epoch 14/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5290 - accuracy: 0.7694 - val_loss: 0.5389 - val_accuracy: 0.7808\n",
            "Epoch 15/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5229 - accuracy: 0.7772 - val_loss: 0.5242 - val_accuracy: 0.7850\n",
            "Epoch 16/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5157 - accuracy: 0.7817 - val_loss: 0.5196 - val_accuracy: 0.7758\n",
            "Epoch 17/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5084 - accuracy: 0.7856 - val_loss: 0.5222 - val_accuracy: 0.7767\n",
            "Epoch 18/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7872 - val_loss: 0.5060 - val_accuracy: 0.7842\n",
            "Epoch 19/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4961 - accuracy: 0.7950 - val_loss: 0.5045 - val_accuracy: 0.7967\n",
            "Epoch 20/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4907 - accuracy: 0.7911 - val_loss: 0.4909 - val_accuracy: 0.7992\n",
            "Epoch 21/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4836 - accuracy: 0.7956 - val_loss: 0.4907 - val_accuracy: 0.7983\n",
            "Epoch 22/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7972 - val_loss: 0.4811 - val_accuracy: 0.8025\n",
            "Epoch 23/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4743 - accuracy: 0.8044 - val_loss: 0.4744 - val_accuracy: 0.8083\n",
            "Epoch 24/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4676 - accuracy: 0.8061 - val_loss: 0.4700 - val_accuracy: 0.8050\n",
            "Epoch 25/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.8078 - val_loss: 0.4658 - val_accuracy: 0.8092\n",
            "Epoch 26/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.8072 - val_loss: 0.4622 - val_accuracy: 0.8108\n",
            "Epoch 27/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.8111 - val_loss: 0.4578 - val_accuracy: 0.8125\n",
            "Epoch 28/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.8144 - val_loss: 0.4567 - val_accuracy: 0.8167\n",
            "Epoch 29/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.8189 - val_loss: 0.4502 - val_accuracy: 0.8158\n",
            "Epoch 30/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.8161 - val_loss: 0.4467 - val_accuracy: 0.8183\n",
            "Epoch 31/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8183 - val_loss: 0.4492 - val_accuracy: 0.8158\n",
            "Epoch 32/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8200 - val_loss: 0.4408 - val_accuracy: 0.8175\n",
            "Epoch 33/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8233 - val_loss: 0.4401 - val_accuracy: 0.8150\n",
            "Epoch 34/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8183 - val_loss: 0.4382 - val_accuracy: 0.8117\n",
            "Epoch 35/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8222 - val_loss: 0.4391 - val_accuracy: 0.8125\n",
            "Epoch 36/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8172 - val_loss: 0.4308 - val_accuracy: 0.8200\n",
            "Epoch 37/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8244 - val_loss: 0.4280 - val_accuracy: 0.8217\n",
            "Epoch 38/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8228 - val_loss: 0.4343 - val_accuracy: 0.8158\n",
            "Epoch 39/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4203 - accuracy: 0.8222 - val_loss: 0.4399 - val_accuracy: 0.8167\n",
            "Epoch 40/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8256 - val_loss: 0.4255 - val_accuracy: 0.8150\n",
            "Epoch 41/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8261 - val_loss: 0.4329 - val_accuracy: 0.8125\n",
            "Epoch 42/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8228 - val_loss: 0.4300 - val_accuracy: 0.8117\n",
            "Epoch 43/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4123 - accuracy: 0.8222 - val_loss: 0.4212 - val_accuracy: 0.8292\n",
            "Epoch 44/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4108 - accuracy: 0.8317 - val_loss: 0.4258 - val_accuracy: 0.8117\n",
            "Epoch 45/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8294 - val_loss: 0.4187 - val_accuracy: 0.8183\n",
            "Epoch 46/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8267 - val_loss: 0.4176 - val_accuracy: 0.8283\n",
            "Epoch 47/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8244 - val_loss: 0.4188 - val_accuracy: 0.8142\n",
            "Epoch 48/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4101 - accuracy: 0.8261 - val_loss: 0.4173 - val_accuracy: 0.8183\n",
            "Epoch 49/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4062 - accuracy: 0.8272 - val_loss: 0.4180 - val_accuracy: 0.8300\n",
            "Epoch 50/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4049 - accuracy: 0.8289 - val_loss: 0.4186 - val_accuracy: 0.8308\n",
            "Epoch 51/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4036 - accuracy: 0.8294 - val_loss: 0.4207 - val_accuracy: 0.8258\n",
            "Epoch 52/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8294 - val_loss: 0.4135 - val_accuracy: 0.8258\n",
            "Epoch 53/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8289 - val_loss: 0.4181 - val_accuracy: 0.8158\n",
            "Epoch 54/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4070 - accuracy: 0.8261 - val_loss: 0.4250 - val_accuracy: 0.8225\n",
            "Epoch 55/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4030 - accuracy: 0.8311 - val_loss: 0.4137 - val_accuracy: 0.8150\n",
            "Epoch 56/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4028 - accuracy: 0.8267 - val_loss: 0.4150 - val_accuracy: 0.8208\n",
            "Epoch 57/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4009 - accuracy: 0.8256 - val_loss: 0.4142 - val_accuracy: 0.8225\n",
            "Epoch 58/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8244 - val_loss: 0.4103 - val_accuracy: 0.8258\n",
            "Epoch 59/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4004 - accuracy: 0.8317 - val_loss: 0.4141 - val_accuracy: 0.8217\n",
            "Epoch 60/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3982 - accuracy: 0.8300 - val_loss: 0.4243 - val_accuracy: 0.8183\n",
            "Epoch 61/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8244 - val_loss: 0.4109 - val_accuracy: 0.8275\n",
            "Epoch 62/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4001 - accuracy: 0.8250 - val_loss: 0.4129 - val_accuracy: 0.8175\n",
            "Epoch 63/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4034 - accuracy: 0.8244 - val_loss: 0.4188 - val_accuracy: 0.8208\n",
            "Epoch 64/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8300 - val_loss: 0.4125 - val_accuracy: 0.8208\n",
            "Epoch 65/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3975 - accuracy: 0.8322 - val_loss: 0.4109 - val_accuracy: 0.8283\n",
            "Epoch 66/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8256 - val_loss: 0.4173 - val_accuracy: 0.8333\n",
            "Epoch 67/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.8278 - val_loss: 0.4183 - val_accuracy: 0.8225\n",
            "Epoch 68/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8256 - val_loss: 0.4101 - val_accuracy: 0.8225\n",
            "Epoch 69/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8267 - val_loss: 0.4151 - val_accuracy: 0.8192\n",
            "Epoch 70/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8294 - val_loss: 0.4119 - val_accuracy: 0.8242\n",
            "Epoch 71/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8283 - val_loss: 0.4140 - val_accuracy: 0.8317\n",
            "Epoch 72/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8272 - val_loss: 0.4085 - val_accuracy: 0.8192\n",
            "Epoch 73/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3960 - accuracy: 0.8272 - val_loss: 0.4099 - val_accuracy: 0.8258\n",
            "Epoch 74/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3983 - accuracy: 0.8317 - val_loss: 0.4072 - val_accuracy: 0.8258\n",
            "Epoch 75/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8339 - val_loss: 0.4103 - val_accuracy: 0.8233\n",
            "Epoch 76/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8250 - val_loss: 0.4185 - val_accuracy: 0.8325\n",
            "Epoch 77/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8278 - val_loss: 0.4079 - val_accuracy: 0.8242\n",
            "Epoch 78/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8300 - val_loss: 0.4122 - val_accuracy: 0.8233\n",
            "Epoch 79/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8272 - val_loss: 0.4072 - val_accuracy: 0.8283\n",
            "Epoch 80/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8261 - val_loss: 0.4190 - val_accuracy: 0.8325\n",
            "Epoch 81/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3948 - accuracy: 0.8283 - val_loss: 0.4107 - val_accuracy: 0.8300\n",
            "Epoch 82/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8267 - val_loss: 0.4150 - val_accuracy: 0.8325\n",
            "Epoch 83/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8317 - val_loss: 0.4113 - val_accuracy: 0.8300\n",
            "Epoch 84/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8256 - val_loss: 0.4083 - val_accuracy: 0.8342\n",
            "Epoch 85/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8272 - val_loss: 0.4190 - val_accuracy: 0.8250\n",
            "Epoch 86/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8250 - val_loss: 0.4100 - val_accuracy: 0.8267\n",
            "Epoch 87/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8244 - val_loss: 0.4141 - val_accuracy: 0.8183\n",
            "Epoch 88/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8300 - val_loss: 0.4062 - val_accuracy: 0.8292\n",
            "Epoch 89/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3929 - accuracy: 0.8272 - val_loss: 0.4070 - val_accuracy: 0.8300\n",
            "Epoch 90/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3923 - accuracy: 0.8294 - val_loss: 0.4162 - val_accuracy: 0.8258\n",
            "Epoch 91/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8233 - val_loss: 0.4119 - val_accuracy: 0.8300\n",
            "Epoch 92/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8322 - val_loss: 0.4189 - val_accuracy: 0.8308\n",
            "Epoch 93/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4020 - accuracy: 0.8256 - val_loss: 0.4170 - val_accuracy: 0.8350\n",
            "Epoch 94/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8311 - val_loss: 0.4225 - val_accuracy: 0.8342\n",
            "Epoch 95/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8328 - val_loss: 0.4084 - val_accuracy: 0.8317\n",
            "Epoch 96/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3967 - accuracy: 0.8278 - val_loss: 0.4268 - val_accuracy: 0.8108\n",
            "Epoch 97/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3993 - accuracy: 0.8294 - val_loss: 0.4134 - val_accuracy: 0.8258\n",
            "Epoch 98/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8250 - val_loss: 0.4088 - val_accuracy: 0.8267\n",
            "Epoch 99/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8322 - val_loss: 0.4158 - val_accuracy: 0.8283\n",
            "Epoch 100/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8317 - val_loss: 0.4196 - val_accuracy: 0.8292\n",
            "Epoch 101/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8278 - val_loss: 0.4081 - val_accuracy: 0.8300\n",
            "Epoch 102/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8328 - val_loss: 0.4078 - val_accuracy: 0.8283\n",
            "Epoch 103/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8278 - val_loss: 0.4125 - val_accuracy: 0.8317\n",
            "Epoch 104/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8294 - val_loss: 0.4070 - val_accuracy: 0.8250\n",
            "Epoch 105/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8300 - val_loss: 0.4094 - val_accuracy: 0.8267\n",
            "Epoch 106/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8267 - val_loss: 0.4110 - val_accuracy: 0.8225\n",
            "Epoch 107/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8311 - val_loss: 0.4116 - val_accuracy: 0.8225\n",
            "Epoch 108/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3940 - accuracy: 0.8300 - val_loss: 0.4112 - val_accuracy: 0.8242\n",
            "Epoch 109/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8267 - val_loss: 0.4100 - val_accuracy: 0.8350\n",
            "Epoch 110/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3943 - accuracy: 0.8306 - val_loss: 0.4087 - val_accuracy: 0.8258\n",
            "Epoch 111/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8306 - val_loss: 0.4101 - val_accuracy: 0.8383\n",
            "Epoch 112/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8300 - val_loss: 0.4103 - val_accuracy: 0.8242\n",
            "Epoch 113/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3919 - accuracy: 0.8328 - val_loss: 0.4090 - val_accuracy: 0.8308\n",
            "Epoch 114/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8278 - val_loss: 0.4083 - val_accuracy: 0.8367\n",
            "Epoch 115/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3915 - accuracy: 0.8311 - val_loss: 0.4097 - val_accuracy: 0.8250\n",
            "Epoch 116/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8289 - val_loss: 0.4144 - val_accuracy: 0.8242\n",
            "Epoch 117/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3938 - accuracy: 0.8272 - val_loss: 0.4087 - val_accuracy: 0.8300\n",
            "Epoch 118/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8272 - val_loss: 0.4118 - val_accuracy: 0.8242\n",
            "Epoch 119/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8322 - val_loss: 0.4103 - val_accuracy: 0.8275\n",
            "Epoch 120/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8300 - val_loss: 0.4141 - val_accuracy: 0.8292\n",
            "Epoch 121/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8289 - val_loss: 0.4088 - val_accuracy: 0.8267\n",
            "Epoch 122/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8233 - val_loss: 0.4117 - val_accuracy: 0.8342\n",
            "Epoch 123/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3922 - accuracy: 0.8317 - val_loss: 0.4102 - val_accuracy: 0.8275\n",
            "Epoch 124/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8317 - val_loss: 0.4131 - val_accuracy: 0.8267\n",
            "Epoch 125/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8306 - val_loss: 0.4109 - val_accuracy: 0.8333\n",
            "Epoch 126/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3905 - accuracy: 0.8333 - val_loss: 0.4074 - val_accuracy: 0.8350\n",
            "Epoch 127/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8228 - val_loss: 0.4124 - val_accuracy: 0.8267\n",
            "Epoch 128/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8261 - val_loss: 0.4091 - val_accuracy: 0.8242\n",
            "Epoch 129/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3907 - accuracy: 0.8317 - val_loss: 0.4128 - val_accuracy: 0.8258\n",
            "Epoch 130/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8306 - val_loss: 0.4091 - val_accuracy: 0.8367\n",
            "Epoch 131/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8317 - val_loss: 0.4129 - val_accuracy: 0.8233\n",
            "Epoch 132/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8339 - val_loss: 0.4104 - val_accuracy: 0.8283\n",
            "Epoch 133/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8344 - val_loss: 0.4145 - val_accuracy: 0.8225\n",
            "Epoch 134/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8283 - val_loss: 0.4079 - val_accuracy: 0.8325\n",
            "Epoch 135/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3920 - accuracy: 0.8306 - val_loss: 0.4079 - val_accuracy: 0.8258\n",
            "Epoch 136/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8278 - val_loss: 0.4119 - val_accuracy: 0.8367\n",
            "Epoch 137/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8283 - val_loss: 0.4106 - val_accuracy: 0.8383\n",
            "Epoch 138/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3932 - accuracy: 0.8267 - val_loss: 0.4086 - val_accuracy: 0.8333\n",
            "Epoch 139/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3912 - accuracy: 0.8278 - val_loss: 0.4115 - val_accuracy: 0.8242\n",
            "Epoch 140/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8256 - val_loss: 0.4142 - val_accuracy: 0.8350\n",
            "Epoch 141/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3916 - accuracy: 0.8300 - val_loss: 0.4221 - val_accuracy: 0.8258\n",
            "Epoch 142/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8311 - val_loss: 0.4125 - val_accuracy: 0.8283\n",
            "Epoch 143/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8250 - val_loss: 0.4146 - val_accuracy: 0.8342\n",
            "Epoch 144/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8272 - val_loss: 0.4184 - val_accuracy: 0.8317\n",
            "Epoch 145/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3934 - accuracy: 0.8317 - val_loss: 0.4103 - val_accuracy: 0.8383\n",
            "Epoch 146/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8267 - val_loss: 0.4108 - val_accuracy: 0.8383\n",
            "Epoch 147/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8317 - val_loss: 0.4091 - val_accuracy: 0.8292\n",
            "Epoch 148/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3910 - accuracy: 0.8278 - val_loss: 0.4111 - val_accuracy: 0.8292\n",
            "Epoch 149/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8283 - val_loss: 0.4117 - val_accuracy: 0.8275\n",
            "Epoch 150/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8267 - val_loss: 0.4126 - val_accuracy: 0.8250\n",
            "Epoch 151/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3916 - accuracy: 0.8333 - val_loss: 0.4113 - val_accuracy: 0.8350\n",
            "Epoch 152/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8333 - val_loss: 0.4083 - val_accuracy: 0.8317\n",
            "Epoch 153/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3907 - accuracy: 0.8294 - val_loss: 0.4294 - val_accuracy: 0.8217\n",
            "Epoch 154/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8311 - val_loss: 0.4110 - val_accuracy: 0.8375\n",
            "Epoch 155/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.8300 - val_loss: 0.4164 - val_accuracy: 0.8317\n",
            "Epoch 156/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8289 - val_loss: 0.4154 - val_accuracy: 0.8217\n",
            "Epoch 157/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3962 - accuracy: 0.8283 - val_loss: 0.4072 - val_accuracy: 0.8308\n",
            "Epoch 158/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8228 - val_loss: 0.4125 - val_accuracy: 0.8358\n",
            "Epoch 159/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 0.8294 - val_loss: 0.4097 - val_accuracy: 0.8350\n",
            "Epoch 160/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8250 - val_loss: 0.4109 - val_accuracy: 0.8350\n",
            "Epoch 161/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8356 - val_loss: 0.4095 - val_accuracy: 0.8300\n",
            "Epoch 162/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8283 - val_loss: 0.4124 - val_accuracy: 0.8275\n",
            "Epoch 163/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8261 - val_loss: 0.4090 - val_accuracy: 0.8308\n",
            "Epoch 164/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8289 - val_loss: 0.4150 - val_accuracy: 0.8292\n",
            "Epoch 165/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3921 - accuracy: 0.8267 - val_loss: 0.4080 - val_accuracy: 0.8283\n",
            "Epoch 166/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3912 - accuracy: 0.8294 - val_loss: 0.4121 - val_accuracy: 0.8308\n",
            "Epoch 167/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.8300 - val_loss: 0.4098 - val_accuracy: 0.8308\n",
            "Epoch 168/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8300 - val_loss: 0.4086 - val_accuracy: 0.8367\n",
            "Epoch 169/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8294 - val_loss: 0.4080 - val_accuracy: 0.8342\n",
            "Epoch 170/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3873 - accuracy: 0.8289 - val_loss: 0.4172 - val_accuracy: 0.8325\n",
            "Epoch 171/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8322 - val_loss: 0.4139 - val_accuracy: 0.8325\n",
            "Epoch 172/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8328 - val_loss: 0.4085 - val_accuracy: 0.8233\n",
            "Epoch 173/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3891 - accuracy: 0.8283 - val_loss: 0.4135 - val_accuracy: 0.8242\n",
            "Epoch 174/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3895 - accuracy: 0.8306 - val_loss: 0.4080 - val_accuracy: 0.8300\n",
            "Epoch 175/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.8278 - val_loss: 0.4112 - val_accuracy: 0.8375\n",
            "Epoch 176/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3885 - accuracy: 0.8272 - val_loss: 0.4120 - val_accuracy: 0.8300\n",
            "Epoch 177/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8283 - val_loss: 0.4145 - val_accuracy: 0.8242\n",
            "Epoch 178/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8294 - val_loss: 0.4113 - val_accuracy: 0.8283\n",
            "Epoch 179/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8278 - val_loss: 0.4088 - val_accuracy: 0.8283\n",
            "Epoch 180/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3907 - accuracy: 0.8294 - val_loss: 0.4146 - val_accuracy: 0.8350\n",
            "Epoch 181/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8322 - val_loss: 0.4101 - val_accuracy: 0.8342\n",
            "Epoch 182/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.8322 - val_loss: 0.4085 - val_accuracy: 0.8350\n",
            "Epoch 183/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3913 - accuracy: 0.8294 - val_loss: 0.4107 - val_accuracy: 0.8258\n",
            "Epoch 184/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.8322 - val_loss: 0.4100 - val_accuracy: 0.8242\n",
            "Epoch 185/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8300 - val_loss: 0.4212 - val_accuracy: 0.8267\n",
            "Epoch 186/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3907 - accuracy: 0.8294 - val_loss: 0.4071 - val_accuracy: 0.8325\n",
            "Epoch 187/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8261 - val_loss: 0.4110 - val_accuracy: 0.8325\n",
            "Epoch 188/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8278 - val_loss: 0.4084 - val_accuracy: 0.8375\n",
            "Epoch 189/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.8250 - val_loss: 0.4102 - val_accuracy: 0.8350\n",
            "Epoch 190/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8289 - val_loss: 0.4138 - val_accuracy: 0.8383\n",
            "Epoch 191/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3889 - accuracy: 0.8278 - val_loss: 0.4118 - val_accuracy: 0.8275\n",
            "Epoch 192/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8267 - val_loss: 0.4091 - val_accuracy: 0.8317\n",
            "Epoch 193/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8317 - val_loss: 0.4099 - val_accuracy: 0.8358\n",
            "Epoch 194/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3883 - accuracy: 0.8306 - val_loss: 0.4091 - val_accuracy: 0.8350\n",
            "Epoch 195/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3902 - accuracy: 0.8317 - val_loss: 0.4098 - val_accuracy: 0.8292\n",
            "Epoch 196/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8322 - val_loss: 0.4136 - val_accuracy: 0.8267\n",
            "Epoch 197/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8283 - val_loss: 0.4123 - val_accuracy: 0.8267\n",
            "Epoch 198/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3881 - accuracy: 0.8361 - val_loss: 0.4094 - val_accuracy: 0.8375\n",
            "Epoch 199/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8250 - val_loss: 0.4140 - val_accuracy: 0.8233\n",
            "Epoch 200/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3905 - accuracy: 0.8267 - val_loss: 0.4211 - val_accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Loss"
      ],
      "metadata": {
        "id": "gI4JvIXahe9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Good Fit')"
      ],
      "metadata": {
        "id": "mw-5vrwHhfKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy"
      ],
      "metadata": {
        "id": "nK4Q7qzRhl3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Good Fit')"
      ],
      "metadata": {
        "id": "ySbUazg7hnCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "จากกราฟ Loss ด้านบน จะเห็นว่าทั้ง Training Loss และ  Validation Loss มีค่าลดลงอย่างต่อเนื่องจนถึงจุดหนึ่งมันจะคงที่ ซึ่งกราฟทั้ง 2 เส้น จะมี Gapระหว่างกันน้อยมาก โดยรูปแบบ Learning Curve ดังกล่าว แสดงว่าเป็น Model แบบ Good Fitting หรือเป็น Model ที่มีการเรียนรู้ที่ดี สามารถนำไป Predict ข้อมูลที่ไม่เคยพบเห็นมาก่อนได้อย่างแม่นยำ"
      ],
      "metadata": {
        "id": "fdUTRmL5hojD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unrepresentative Train Dataset\n",
        "\n",
        "นอกจากเราจะใช้ Learning Curve ในการพิจารณาว่า Model มีประสิทธิภาพหรือไม่แล้ว เรายังสามารถพิจารณาจากรูปแบบของ Learning Curve ได้ว่า Dataset (Train และ Validate) ของเราเป็นตัวแทนของข้อมูลที่ดีหรือไม่\n",
        "\n",
        "โดยจะจำลองสถานการณ์เมื่อ Trainning Dataset ไม่สามารถเป็นตัวแทนของข้อมูลที่ดีได้ ดังขั้นตอนต่อไปนี้"
      ],
      "metadata": {
        "id": "k8bwYOc3hsJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สร้าง Dataset แบบ 3 Class โดยใช้ Function make_blobs ของ Sklearn"
      ],
      "metadata": {
        "id": "NjFFnmtThxW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_blobs(n_samples=100, centers=3, n_features=2, cluster_std=2, random_state=2)"
      ],
      "metadata": {
        "id": "hc5NbUR2ho22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "แบ่งข้อมูลสำหรับ Train และ Validate ด้วยการสุ่มในสัดส่วน 50:50"
      ],
      "metadata": {
        "id": "E1uZRAAKh0Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.5, shuffle= True)\n",
        "\n",
        "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "id": "jImEsuK0h1OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นำ Dataset ส่วนที่ Train มาแปลงเป็น DataFrame โดยเปลี่ยนชนิดข้อมูลใน Column \"class\" เป็น String เพื่อทำให้สามารถแสดงสีแบบไม่ต่อเนื่องได้ แล้วนำไป Plot"
      ],
      "metadata": {
        "id": "xT1jFn5ch3tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.DataFrame(x_train, columns=['x', 'y'])\n",
        "y_train_pd = pd.DataFrame(y_train, columns=['class'])\n",
        "\n",
        "df = pd.concat([x_train_pd, y_train_pd], axis=1)\n",
        "df[\"class\"] = df[\"class\"].astype(str)"
      ],
      "metadata": {
        "id": "7gWrXgowh6R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"class\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0n8iwrY8h7WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เข้ารหัสผลเฉลย แบบ One-Hot Encoding เพื่อที่ว่าเมื่อ Model มีการ Predict ว่าเป็น Class ไหน มันจะให้ค่าความมั่นใจ (Confidence) กลับมาด้วย"
      ],
      "metadata": {
        "id": "JVr7Q_y8h8gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "Ea4ZCHskh9xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นิยาม Model"
      ],
      "metadata": {
        "id": "CB_meh0Lh-74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4qF15CB1iAd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "Wt7n5H34iCKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "PnV8fFIXiCct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Loss"
      ],
      "metadata": {
        "id": "P94sQRYQiFgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Unrepresentative Train Dataset')"
      ],
      "metadata": {
        "id": "ZwJ81XNPiINj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy"
      ],
      "metadata": {
        "id": "8LfRbpeTiJtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Unrepresentative Train Dataset')"
      ],
      "metadata": {
        "id": "PBe_ZVHhiKzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "จากกราฟด้านบนพบว่าเมื่อมีการ Train Model มากขึ้น ค่า Loss จะมีแนวโน้มลดลง และ Accuracy มีแนวโน้มเพิ่มขึ้น แต่จะมี Gap ระหว่าง Training Loss กับ Validation Loss รวมทั้ง Gap ระหว่าง Training Accuracy กับ Validation Accuracy สูง ซึ่งแสดงว่าเรามี Training Dataset น้อยไป ไม่เพียงพอในการ Train Model ครับ"
      ],
      "metadata": {
        "id": "v_29DJhpiMae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unrepresentative Validation Dataset\n",
        "เราจะจำลองสถานการณ์ในกรณีที่ Validation Dataset ที่ไม่สามารถเป็นตัวแทนของข้อมูลที่ดี ดังต่อไปนี้\n",
        "\n",
        "สถานการณ์ที่ 1 Validation Dataset น้อย และไม่สามารถเป็นตัวแทนของ Validation Dataset ได้\n",
        "\n",
        "สถานการณ์ที่ 2 Validation Dataset น้อย และง่ายเกินไป"
      ],
      "metadata": {
        "id": "CeQeAgS_iNtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สถานการณ์ที่ 1 (Validation Dataset น้อย และไม่สามารถเป็นตัวแทนของ Validation Dataset ได้) มีขั้นตอนการทดลองดังต่อไปนี้\n",
        "\n"
      ],
      "metadata": {
        "id": "RGbJf0DkiZvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=10, random_state=2)\n"
      ],
      "metadata": {
        "id": "e5HHBbjGibjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "แบ่งข้อมูลสำหรับ Train และ Validate ด้วยการสุ่มในสัดส่วน 95:5"
      ],
      "metadata": {
        "id": "yHrfWxHqifYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.05, shuffle= True)\n",
        "\n",
        "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "id": "Sruz_F9Gihgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นำ Dataset ส่วนที่ Train มาแปลงเป็น DataFrame โดยเปลี่ยนชนิดข้อมูลใน Column \"class\" เป็น String เพื่อทำให้สามารถแสดงสีแบบไม่ต่อเนื่องได้ แล้วนำไป Plot"
      ],
      "metadata": {
        "id": "x3TI1wPAii0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.DataFrame(x_train, columns=['x', 'y'])\n",
        "y_train_pd = pd.DataFrame(y_train, columns=['class'])\n",
        "\n",
        "df = pd.concat([x_train_pd, y_train_pd], axis=1)\n",
        "df[\"class\"] = df[\"class\"].astype(str)"
      ],
      "metadata": {
        "id": "NLivrSHBikm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"class\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "piW9snQXin4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เข้ารหัสผลเฉลย แบบ One-Hot Encoding เพื่อที่ว่าเมื่อ Model มีการ Predict ว่าเป็น Class ไหน มันจะให้ค่าความมั่นใจ (Confidence) กลับมาด้วย"
      ],
      "metadata": {
        "id": "sci53ZXKiplb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "bj8RbV8EiqmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นิยาม Model"
      ],
      "metadata": {
        "id": "F1XPxHdfirfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-SVh3eBpitXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "-mA22KaRiugj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "4D_XhRhgivz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Loss"
      ],
      "metadata": {
        "id": "Bio9dqsviw_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Unrepresentative Validation Dataset')"
      ],
      "metadata": {
        "id": "l9y19d7liyOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy"
      ],
      "metadata": {
        "id": "fC5mwnvqi0kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['accuracy'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_accuracy'], \n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Unrepresentative Validation Dataset')"
      ],
      "metadata": {
        "id": "tT2H0iLoi13_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในกรณีที่ Validation Dataset น้อย และไม่สามารถเป็นตัวแทนของ Validation Dataset ได้ เราจะเห็นค่า Training Loss ค่อยๆ ลดลงแบบเดียวกับในกรณี Good Fitting แต่ Validation Loss จะแกว่งไปมาเหมือนการสุ่มอยู่รอบๆ กราฟ Training Loss เช่นเดียวกันกับที่เมื่อพิจารณาจากกราฟ Accuracy จะพบว่าค่า Validation Accuracy จะแกว่งไปมาเหมือนการสุ่มอยู่รอบๆ กราฟ Training Accuracy"
      ],
      "metadata": {
        "id": "7ynVH5CRi3k5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สถานการณ์ที่ 2 (Validation Dataset น้อย และง่ายเกินไป) มีขั้นตอนการทดลองดังต่อไปนี้"
      ],
      "metadata": {
        "id": "55XLLxH0i8mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "สร้าง Dataset แบบ 3 Class โดยใช้ Function make_blobs ของ Sklearn"
      ],
      "metadata": {
        "id": "rbXwAEoDi93d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_blobs(n_samples=400, centers=3, n_features=2, cluster_std=2, random_state=2)"
      ],
      "metadata": {
        "id": "TKBHaGp8jAXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "แบ่งข้อมูลสำหรับ Train และ Validate ด้วยการสุ่มในสัดส่วน 97:3"
      ],
      "metadata": {
        "id": "ZNfTdS1tjCF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.03, shuffle= True)\n",
        "\n",
        "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "id": "nEHXyWCAjDSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นำ Dataset ส่วนที่ Train มาแปลงเป็น DataFrame โดยเปลี่ยนชนิดข้อมูลใน Column \"class\" เป็น String เพื่อทำให้สามารถแสดงสีแบบไม่ต่อเนื่องได้ แล้วนำไป Plot"
      ],
      "metadata": {
        "id": "WFIPNHWpjEkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.DataFrame(x_train, columns=['x', 'y'])\n",
        "y_train_pd = pd.DataFrame(y_train, columns=['class'])\n",
        "\n",
        "df = pd.concat([x_train_pd, y_train_pd], axis=1)\n",
        "df[\"class\"] = df[\"class\"].astype(str)"
      ],
      "metadata": {
        "id": "4kP3AXVkjGDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"class\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5GPqScxOjHNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เข้ารหัสผลเฉลย แบบ One-Hot Encoding เพื่อที่ว่าเมื่อ Model มีการ Predict ว่าเป็น Class ไหน มันจะให้ค่าความมั่นใจ (Confidence) กลับมาด้วย"
      ],
      "metadata": {
        "id": "wRnTj7erjK5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "oRyiUU_9jL73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "นิยาม Model"
      ],
      "metadata": {
        "id": "0TLAU63ijNLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ApxsIb_SjOa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "Dm_bvlo2jPWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "5B1W_Oh2jQg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Loss"
      ],
      "metadata": {
        "id": "5YCoymrkjRxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_loss'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Unrepresentative Validation Dataset')"
      ],
      "metadata": {
        "id": "7298YzROjTHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy"
      ],
      "metadata": {
        "id": "n1CXpx1UjU24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = go.Scatter(y=history.history['accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=history.history['val_accuracy'], \n",
        "                    mode=\"lines\",\n",
        "                    line=dict(\n",
        "                        width=2,\n",
        "                        color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='Epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename='Unrepresentative Validation Dataset')"
      ],
      "metadata": {
        "id": "WV3B0rlMjWCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ในกรณีที่ Validation Dataset น้อย และง่ายจนเกินไป เราจะเห็นค่า Training Loss และ Validate Loss ลดลง โดยที่ค่า Validate Loss จะต่ำกว่า Training Lossในทางตรงกันข้าม เมื่อพิจารณาจากกราฟ Accuracy จะพบว่าค่า Training Accuracy และ Validate Accuracy จะเพิ่มขึ้น โดยที่ค่า Validate Accuracy จะสูงกว่า Training Accuracy ครับ"
      ],
      "metadata": {
        "id": "EFaJbYF_jXXj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOrzl2yBjXup"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}